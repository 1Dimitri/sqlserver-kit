<html><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta http-equiv="Content-Language" content="en-gb">
<style type="text/css"><!--
/* Effective stylesheet produced by snapshot save */
small { font-size: 90%; }
h2 { border-top: 2px dashed black; padding-top: 12pt; }
h3 { border: 1px solid rgb(187, 187, 187); background-color: rgb(224, 224, 224); padding-right: 6pt; display: inline; text-align: left; }
li { margin-bottom: 3pt; margin-left: -10pt; }
pre { margin-left: 18pt; }
.theader { background-color: rgb(224, 224, 224); }
.image { margin-left: 3cm; }
.wideimage { margin-left: 1cm; }
.note { font-size: 90%; border: 3px solid rgb(187, 187, 187); display: inline-block; margin-top: 0pt; margin-bottom: 0pt; }
.noteh3 { font-size: 90%; border: 3px solid rgb(187, 187, 187); display: inline-block; margin-top: 0pt; margin-bottom: 12pt; }
.nowrap { white-space: nowrap; }
--></style>
<title>Slow in the Application, Fast in SSMS?</title>
<style type="text/css"><!--
/* Effective stylesheet produced by snapshot save */
#lleo_dialog *::before, #lleo_dialog *::after { content: ""; }
--></style><style id="lleo_css_enjoyContentControls" type="text/css"><!--
/* Effective stylesheet produced by snapshot save */
#lleo_enjoyContentControls, #lleo_enjoyContentControls * { color: rgb(0, 0, 0) ! important; font: 13px/15px Arial,Helvetica ! important; margin: 0px ! important; padding: 0px ! important; background: transparent none repeat scroll 0% 0% ! important; border: 0px none ! important; position: static ! important; vertical-align: baseline ! important; overflow: visible ! important; width: auto ! important; height: auto ! important; max-width: none ! important; max-height: none ! important; float: none ! important; visibility: visible ! important; text-align: left ! important; text-transform: none ! important; border-collapse: separate ! important; border-spacing: 2px ! important; box-sizing: content-box ! important; box-shadow: none ! important; opacity: 1 ! important; text-shadow: none ! important; }
#lleo_enjoyContentControls { background: rgb(247, 200, 117) none repeat scroll 0% 0% ! important; position: fixed ! important; right: 0px ! important; top: -40px ! important; width: 39px ! important; height: 34px ! important; opacity: 0.85 ! important; border-top-left-radius: 4px ! important; border-bottom-left-radius: 4px ! important; box-shadow: 2px 4px 12px rgba(0, 0, 0, 0.3) ! important; z-index: 2147483647 ! important; overflow: hidden ! important; }
#lleo_enjoyContentControls.lleo_show { top: 130px ! important; transition: top 0.8s ease-out 0s ! important; }
#lleo_enjoyContentControls:hover { opacity: 1 ! important; transition: opacity 0.4s linear 0s, width 0.4s linear 1.2s ! important; }
#lleo_enjoyContentControls #lleo_enjoyContentPanel { white-space: nowrap ! important; margin: 9px 44px 0px 10px ! important; opacity: 0 ! important; transition: all 0.4s linear 1.2s ! important; }
#lleo_enjoyContentControls:hover #lleo_enjoyContentPanel { opacity: 1 ! important; }
#lleo_enjoyContentControls #lleo_enjoyContentPanel input { margin-right: 5px ! important; }
#lleo_enjoyContentButton { background: rgb(255, 255, 255) url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQBAMAAADt3eJSAAAAA3NCSVQICAjb4U/gAAAACXBIWXMAAAk6AAAJOgHwZJJKAAAAGXRFWHRTb2Z0d2FyZQB3d3cuaW5rc2NhcGUub3Jnm+48GgAAADBQTFRF////AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAL92gewAAAA90Uk5TAA0PEhUcLS9CxszS29z1M86yOAAAAFVJREFUCFtjYIADduMCMGZgvmsAxggGUFgELMXAwDq7AaI8MnULmGY9pTYRzIj5vzIALHD+/1Gw4pj/fwJA2kECYHNkgAJgK/yAAmDA8QasBQgcIRQA12YesqxFXfcAAAAASUVORK5CYII8850aa0afe766ff5b5bb3697b8de32f7") no-repeat scroll center center / 16px 16px ! important; width: 36px ! important; height: 34px ! important; cursor: pointer ! important; position: absolute ! important; right: 0px ! important; top: 0px ! important; }
@media only screen and (min\2D\2D moz-device-pixel-ratio: 2), not all, not all {
  #lleo_enjoyContentButton { background-image: url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAMAAABEpIrGAAAAA3NCSVQICAjb4U/gAAAACXBIWXMAABJ0AAASdAHeZh94AAAAGXRFWHRTb2Z0d2FyZQB3d3cuaW5rc2NhcGUub3Jnm+48GgAAAD9QTFRF////AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAxQXeHgAAABR0Uk5TAAQFDRkcNz1PcHSIrK6ws7/b8P5ZkzdFAAAAo0lEQVQ4y62RzQ6DMAyDTSkjpfy04Pd/1h12WAtpKk3zzfosJXGAP2l0AODGloc7BJDDtTwgTInS9kAik+W7gd6I7pLFWRK9cuZXgcyT0VjguliJwG1AYPZNvjsAC2OLXzOAYa07LPfj+YLbGJqcPOfd5OTV4WSAVlTNn1Xf+ONZD357t8KrgMbLERovlxSFV2dGhVfy2ebAlG3+SVgc8FHws96B8RE8rjk+bwAAAABJRU5ErkJggga9a50a0b337b82ef5c0bda0f6d6960d3") ! important; }
}
--></style><style type="text/css"><!--
/* Effective stylesheet produced by snapshot save */
#lleo_enjoyContentControls:hover { width: 225px ! important; }
--></style></head>
<body>
<h1 align="center">Slow in the Application, Fast in SSMS?<br><small>Understanding Performance Mysteries</small></h1>
<p><small>An SQL text by <a href="http://www.sommarskog.se/index.html">Erland Sommarskog</a>, SQL Server MVP. <a href="#Revisions">Last revision</a>: 2013-08-30.</small><br>
This article is also available in <a href="http://www.somewheresomehow.ru/fast-in-ssms-slow-in-app-part1/">Russian</a>, translated by Dima Piliugin. </p>

<h2><a name="introduction">Introduction</a></h2>
<p>When I read various forums about <small>SQL</small> Server, I frequently see questions from
deeply mystified
posters. They have identified a slow query or stored procedure
in their application. They cull the <small>SQL</small> batch from the application and run it in <small>SQL</small> Server
Management Studio (<small>SSMS</small>) to analyse it, only to find that the
response is instantaneous. At this point
they are inclined to think that <small>SQL</small> Server is all about
magic. A similar mystery is when a developer has extracted a query in his stored
procedure to run it stand-alone only to find that it runs much faster – or much
slower – than inside the procedure.</p>
<p>No, <small>SQL</small> Server is not about magic. But if you don't have a good understanding
of how<small> SQL</small> Server compiles queries and maintains its plan cache, it may seem so.
Furthermore there are some unfortunate combinations of different defaults in
different environments. In this article, I will try to straighten out why you get this seemingly inconsistent behaviour.
I explain how <small>SQL</small> Server compiles a stored procedure, what <i>parameter sniffing</i>
is and why it is part of the equation in the vast majority of these confusing
situations. I explain how <small>SQL</small> Server uses the cache, and why there may be
multiple entries for a procedure in the cache. Once you have come this far, you
will understand how come the query runs so much faster in <small>SSMS</small>.</p>
<p>To understand how to address that performance problem in your application, you
need to read on. I first make a short break from the theme of parameter sniffing
to discuss a few situations where there are other reasons for the difference in
performance. This is followed by two chapters how to deal with
performance problems where parameter sniffing is involved. The first is about
gathering  information. In the second chapter I discuss some scenarios
– both real-world situations and more generic ones – and possible solutions. In the last chapter I discuss how dynamic <small>SQL</small>
is compiled and interacts with the plan cache and why there are more reasons you
may experience differences in performance between <small>SSMS</small> and the application with
dynamic <small>SQL</small>. At the end there is a
section with links to Microsoft white papers and similar documents in this area.</p>
<h3>Table of Contents</h3>
<p>
<contents>
   <b><a href="#introduction">Introduction</a></b><br>
      <a href="#Presumptions">Presumptions</a><br>
   <b><a href="#compileps">How <small>SQL</small> Server Compiles a Stored Procedure</a></b><br>
      <a href="#whatissp">What is a Stored Procedure?</a><br>
      <a href="#plangenerate">How <small>SQL</small> Server Generates the Query Plan</a><br>
      <a href="#intocache">Putting the Query Plan into the Cache</a><br>
      <a href="#cachekeys">Different Plans for Different Settings</a><br>
      <a href="#defaultsettings">The Default Settings</a><br>
      <a href="#stmtrecompile">The Effects of Statement Recompile</a><br>
      <a href="#storysofar">The Story So Far</a><br>
   <b><a href="#otherreasons">It's Not Always Parameter Sniffing...</a></b><br>
      <a href="#varparamreplace">Replacing Variables and Parameters</a><br>
      <a href="#blocking">Blocking</a><br>
      <a href="#ixviews">Indexed Views and Indexed Computed Columns</a><br>
      <a href="#linkedservers">An Issue with Linked Servers</a><br>
   <b><a href="#sniffinfo">Getting Information to Solve Parameter Sniffing Problems</a></b><br>
      <a href="#factgathering">Getting the Necessary Facts</a><br>
      <a href="#slowstatement">Which is the Slow Statement?</a><br>
      <a href="#ssmsgettingplans">Getting the Query Plans and Parameters with Management Studio</a><br>
      <a href="#dmvgettingplans">Getting the Query Plans and Parameters Directly from the Plan Cache</a><br>
      <a href="#tracegettingplans">Getting Query Plans and Parameters from a Trace</a><br>
      <a href="#tableandindexdefs">Getting Table and Index Definitions</a><br>
      <a href="#statinfo">Finding Information About Statistics</a><br>
   <b><a href="#fixexamples">Examples of How to Fix Parameter-Sniffing Issues</a></b><br>
      <a href="#nonsolution">A Non-Solution</a><br>
      <a href="#inputdepends">Best Index Depends on Input</a><br>
      <a href="#dynsearch">Dynamic Search Conditions</a><br>
      <a href="#reviewindexing">Reviewing Indexing</a><br>
      <a href="#appcache">The Case of the Application Cache</a><br>
      <a href="#badsql">Fixing Bad <small>SQL</small></a><br>
   <b><a href="#compiledynsql">How <small>SQL</small> Server Compiles Dynamic <small>SQL</small></a></b><br>
      <a href="#whatisdynsql">What Is Dynamic <small>SQL</small>?</a><br>
      <a href="#genplandynsql">Generating the Plan for Dynamic <small>SQL</small></a><br>
      <a href="#cachedynsql">Dynamic <small>SQL</small> and the Plan Cache</a><br>
      <a href="#runqueriesssms">Running Application Queries in <small>SSMS</small></a><br>
      <a href="#fixesdynsql">Addressing Parameter-sniffing Problems in Dynamic <small>SQL</small></a><br>
      <a href="#planguides">Plan Guides and Plan Freezing</a><br>
   <b><a href="#furtherreading">Further Reading</a></b><br>
   <b><a href="#Revisions">Revisions</a></b><br>
</contents>
</p>
<h3><a name="Presumptions">Presumptions</a></h3>
<p>The essence of this article applies to all versions of <small>SQL</small> Server, but the focus is on <small>SQL</small> 2005 and later versions.
The article includes several queries to inspect the plan cache; these queries run only on <small>SQL</small> 2005 and later. <small>SQL</small> 2000 and earlier versions had far less instrumentation in this regard.
Beware that to run these queries you need to have the server-level permission
<small>VIEW SERVER STATE</small>.</p>
<p>For the examples in this article I use the Northwind sample database. This
database shipped with <small>SQL</small> 2000. For later versions of <small>SQL</small> Server you can
download it from
   <a href="http://www.microsoft.com/downloads/info.aspx?na=22&amp;p=1&amp;SrcDisplayLang=en&amp;SrcCategoryId=&amp;SrcFamilyId=&amp;u=/downloads/details.aspx?FamilyID=06616212-0356-46a0-8da2-eebc53a68034&amp;DisplayLang=en" target="_blank">
   Microsoft's web site</a>.</p>
<p>This is not a beginner-level article, but I assume that the reader has a
working experience of <small>SQL</small> programming. You don't need to have any prior experience of
performance tuning, but it certainly helps if you helps if you have looked a
little at query plans and if you have some basic knowledge of indexes. I will
not explain the basics in depth, as my focus a little beyond that point. This article will not
teach you everything about performance
tuning, but at least it will be a start. </p>
<p><b>Caveat</b>: In some places I give links to the online version of Books Online. Beware that the <small>URL</small> may lead to Books Online for a
different version of <small>SQL</small>
Server than you are using. On the topic pages, there is a link <i>Other versions</i>, so that you easily can go to
the page that matches the version of <small>SQL</small> Server
you are using. (Or at least that was how Books Online on <small>MSDN</small> was organised when I wrote this article.)</p>
<h2><a name="compileps">How SQL Server Compiles a Stored Procedure</a></h2>
<p>In this chapter we will look at how <small>SQL</small> Server compiles a stored procedure
and uses the plan cache. If your application does not use stored procedures, but
submits <small>SQL</small> statements directly, most of what I say this chapter is still applicable. But there are further
complications with dynamic <small>SQL</small>, and since the facts about stored procedures are confusing enough I have deferred the discussion on dynamic
<small>SQL</small> to the <a href="#furtherreading">last chapter</a>.</p>
<h3><a name="whatissp">What is a Stored Procedure?</a></h3>
<p>That may seem like a silly question, but the question I am getting at is <i>What objects
have query plans on their own?</i> <small>SQL</small>
Server builds query plans for these types of objects:</p>
<ul>
   <li>Stored procedures.</li>
   <li>Scalar user-defined functions.</li>
   <li>Multi-step table-valued functions.</li>
   <li>Triggers.</li>
</ul>
<p>With a more general and stringent terminology I should talk about <i>modules</i>,
but since stored procedures is by far the most widely used type of module, I prefer to
talk about stored procedures to keep it simple. </p>
<p>For other types of objects than the four listed above, <small>SQL</small> Server does not build query plans.
Specifically, <small>SQL</small> Server does not create query plans for views and inline-table
functions. Queries like:</p>
<pre>SELECT abc, def FROM myview
SELECT a, b, c FROM mytablefunc(9)</pre>
<p>are no different from ad-hoc queries that access the tables directly. When
compiling the query, <small>SQL</small> Server expands the view/function into the query, and
the optimizer works with the expanded query text. </p>
<p>There is one more thing we need to understand about what constitutes a stored
procedure. Say that you have two procedures, where the
outer calls the inner one:</p>
<pre>CREATE PROCECURE Outer_sp AS
...
EXEC Inner_sp
...</pre>
<p>I would guess most people think of Inner_sp as being independent from Outer_sp,
and indeed it is. The execution plan for Outer_sp does not include the
query plan for Inner_sp, only the invocation of it. However, there is a very similar situation where
I've noticed that posters on <small>SQL</small> forums often have a
different mental image, to wit dynamic <small>SQL</small>:</p>
<pre>CREATE PROCEDURE Some_sp AS
DECLARE @sql    nvarchar(MAX),
        @params nvarchar(MAX)
SELECT @sql = 'SELECT ...'
...
EXEC sp_executesql @sql, @params, @par1, ...</pre>
<p>It is important to understand that this is no different from nested stored procedures. The generated
<small>SQL</small> string is <b>not</b>
part of Some_sp, nor does it appear anywhere in the query plan for Some_sp, but it has a
query plan and a cache entry of its own. This applies, no matter if the dynamic
<small>SQL</small> is executed through <small><span class="nowrap">EXEC()</span></small> or <b>sp_executesql</b>.</p>
<h3><a name="plangenerate">How SQL Server Generates the Query Plan</a></h3>
<h4>Overview</h4>
<p>When you enter a stored procedure with <small>CREATE PROCEDURE</small> (or <small>CREATE FUNCTION</small>
for a function or <small>CREATE TRIGGER</small> for a trigger), <small>SQL</small> Server verifies that the
code is syntactically correct, and also checks that you do not refer to
non-existing columns. (But if you refer to non-existing tables, it lets get you
away with it, due to a misfeature known as deferred named resolution.) However,
at this point <small>SQL</small> Server does <b>not</b> build any query plan, but merely stores
the query text in the database.</p>
<p>It is not until a user executes the procedure, that <small>SQL</small> Server creates the
plan. For each query, <small>SQL</small> Server looks at the distribution statistics it
has collected about the data in the tables in the query. From
this, it makes an estimate what is best way to execute the query. This
phase is known as <i>optimisation</i>. While the procedure is compiled in one go,
each query is optimised on its own, and there is no attempt to analyse the flow
of execution. This has a very important ramification: the optimizer has no idea
about the run-time values of variables. However, it does know what values
the user specified for the parameters to the procedure.</p>
<h4>Parameters and Variables</h4>
<p>Consider the Orders
table in the Northwind database, and these three procedures:</p>
<pre>CREATE PROCEDURE List_orders_1 AS
   SELECT * FROM Orders WHERE OrderDate &gt; '20000101'
go
CREATE PROCEDURE List_orders_2 @fromdate datetime AS
   SELECT * FROM Orders WHERE OrderDate &gt; @fromdate
go
CREATE PROCEDURE List_orders_3 @fromdate datetime AS
   DECLARE @fromdate_copy datetime
   SELECT @fromdate_copy = @fromdate
   SELECT * FROM Orders WHERE OrderDate &gt; @fromdate_copy
go</pre>
<p class="note"><b>Note</b>: Using <small>SELECT</small> * in production code is bad practice.
I use it in this article to keep the examples concise.</p>
<p>Then we execute the procedures in this way:</p>
<pre>EXEC List_orders_1
EXEC List_orders_2 '20000101'
EXEC List_orders_3 '20000101'</pre>
<p>
Before you run the procedures, enable <i>Include Actual Execution Plan</i> under
the <i>Query</i> menu. (There is also a toolbar button for it, as well as a
keyboard shortcut: Ctrl-M for the "Standard" keyboard, or Ctrl-K if you
are like me and use
the <small>SQL</small> 2000 keyboard.)
If you look at the query plans for the procedures, you will see the
first two procedures have identical plans:</p>
<p class="image">
<img alt="Index seek + Key lookup" src="Slow%20in%20the%20Application,%20Fast%20in%20SSMS_files/list_orders2.jpg" height="167" width="447"></p>
<p>
That is, <small>SQL</small> Server seeks the index on OrderDate, and uses a key lookup to
get the other data. The plan for the third execution is different:</p>
<p class="image"><img alt="Clustered Index Scan" src="Slow%20in%20the%20Application,%20Fast%20in%20SSMS_files/list_orders3.jpg" height="87" width="346"></p>
<p>
In this case, <small>SQL</small> Server  scans the table. (Keep in mind that in a
clustered index the leaf pages contains the data, so a clustered index scan and
a table scan is the same the thing.) Why this difference? To understand why the optimizer makes certain decisions, it is always a good idea to look at what
estimates it is working with. If you hover with the mouse over of the two Seek operators and the Scan operator, you will see the pop-ups below. </p>
<table style="margin-left: 1cm;" cellspacing="2">
<tbody><tr>
<td align="CENTER" valign="TOP">
<img alt="Popup list_orders_1" src="Slow%20in%20the%20Application,%20Fast%20in%20SSMS_files/popup1.jpg" border="0" height="409" width="292">
</td>
<td>   </td>
<td align="CENTER" valign="TOP">
<img alt="Popup list_orders_2" src="Slow%20in%20the%20Application,%20Fast%20in%20SSMS_files/popup2.jpg" border="0" height="397" width="293">
</td>
</tr>
<tr>
<td style="padding-bottom: 12pt;" align="CENTER"><b><small>List_orders_1</small></b></td>
<td>   </td>
<td style="padding-bottom: 12pt;" align="CENTER"><b><small>List_orders_2</small></b></td></tr>
<tr>
<td colspan="3" align="CENTER" valign="TOP">
<img alt="Popup list_orders_3" src="Slow%20in%20the%20Application,%20Fast%20in%20SSMS_files/popup3.jpg" border="0" height="340" width="314">
</td>
</tr>
<tr>
<td colspan="3" align="CENTER"><b><small>List_orders_3</small></b></td>
</tr>
</tbody></table>
<p>The interesting element is <i>Estimated Number of Rows</i>. For the first two procedures, <small>SQL</small> Server estimates that one row will be returned, but for
List_orders_3, the estimate is 249 rows. This difference in estimates explains the different choice of plans. Index Seek + Key Lookup is a good strategy to return a
smaller amount of rows from a table. But as more rows that match the seek
criteria, the cost increases, and there is a increased likelihood that <small>SQL</small>
Server will need to access the same data page more than once. In the extreme case where all rows
are returned, a table scan is much more efficient than seek and lookup. With a
scan,
<small>SQL</small> Server has to read every data page exactly once, whereas with
seek + key lookup, every page will be visited once for each row on the page. The Orders table in Northwind has 830 rows, and when <small>SQL</small> Server estimates that as many
as 249 rows will be returned, it (rightly) concludes that the scan is the best choice. </p>
<h4>Where Do These Estimates Come From?</h4>
<p>Now we know why the optimizer arrives at different execution plans: because the estimates are different. But that only leads to the next question: why
are the estimates different? That is the key topic of this article.</p>
<p>In the first procedure, the date is a constant, which means that the <small>SQL</small> Server only needs to consider exactly this case. It interrogates the statistics for the
Orders table, which indicates that there are no rows with an OrderDate in the
third millennium. (All orders in the Northwind database are from 1996 to 1998.)
Since statistics are statistics, <small>SQL</small> Server cannot be sure that
the query will return no rows at all, why it makes an estimate of one single row.</p>
<p>In case of List_orders_2, the query is against a variable, or more precisely a parameter. When performing the optimisation, <small>SQL</small> Server knows that the
procedure was invoked with the value 2000-01-01. Since it does not any perform flow analysis, it can't say
for sure whether the parameter will have this value when the query is executed.
Nevertheless, it uses the input value to come up
with an estimate, which is the same as for List_orders_1: one single row. This strategy of looking at the values of the input
parameters when optimising a stored procedure is known as <i>parameter sniffing</i>.</p>
<p>
In the last procedure, it's all different. The input value is copied to a
local variable, but when <small>SQL</small> Server builds the plan, it has no understanding of
this and says to itself<i> I don't know what the value this variable will
be</i>.<i>
</i>Because of this, it applies a standard assumption, which for an inequality
operation such as <code>&gt;</code> is a 30 % hit-rate. 30 % of 830 is indeed 249.</p>
<p>Here is a variation of the theme:</p>
<pre>CREATE PROCEDURE List_orders_4 @fromdate datetime = NULL AS
   IF @fromdate IS NULL
      SELECT @fromdate = '19900101'
   SELECT * FROM Orders WHERE OrderDate &gt; @fromdate</pre>
<p>In this procedure, the parameter is optional, and if the user does not fill
in the parameter, all orders are listed. Say that the user invokes the procedure
as:</p>
<pre>EXEC List_orders_4</pre>
<p>The execution plan is identical to the plan for List_orders_1 and List_orders_2. That is, Index Seek + Key Lookup, despite
that all orders are returned. If you
look at the pop-up for the Index Seek operator, you will see that it is identical to the pop-up for List_orders_2 but in one regard, the actual number of rows.
When compiling the procedure, <small>SQL</small> Server
does not know that the value of @fromdate changes, but compiles the procedure
under the assumption that @fromdate has the value <small>NULL</small>. Since all comparisons with <small>NULL</small> yield
<small>UNKNOWN</small>, the query cannot return any rows at all, if @fromdate still
has this value at run-time. If <small>SQL</small> Server would take the input value as the final
truth, it could construct a plan with only a Constant Scan that does not access the table at all (run the query <code>SELECT * FROM Orders WHERE OrderDate &gt;
NULL</code> to see an example of this). But <small>SQL</small> Server must generate a plan which
returns the correct result no matter what value @fromdate has at run-time. On
the other hand, there is no obligation to build a plan which is the best for all
values. Thus, since the assumption is that
no rows will be returned, <small>SQL</small> Server settles for the Index Seek. (The estimate is still that one row will be returned. This is
because <small>SQL</small> Server never uses an estimate of 0 rows.)</p>
<p>This is an example of when parameter sniffing backfires, and in this
particular case it may be better to write the procedure in this way:</p>
<pre>CREATE PROCEDURE List_orders_5 @fromdate datetime = NULL AS
   DECLARE @fromdate_copy datetime
   SELECT @fromdate_copy  = coalesce(@fromdate, '19900101')
   SELECT * FROM Orders WHERE OrderDate &gt; @fromdate_copy</pre>
<p>With List_orders_5 you always get a Clustered Index
Scan.</p>
<h4>Key Points</h4>
<p>In this section, we have learned three very important things:</p>
<ul>
  <li>A constant is a constant, and when a query includes a constant, <small>SQL</small>
  Server can use the value of the constant with full trust, and even take such
  shortcuts to not access a table at all, if it can infer from constraints that
  no rows will be returned.</li>
  <li>For a parameter, <small>SQL</small> Server does not know the run-time value, but it "sniffs" the input value when compiling the query. </li>
  <li>For a local variable, <small>SQL</small> Server has no idea at all of the run-time value, and applies standard assumptions. (Which the assumptions are depends on the
  operator and what can be deduced from the presence of unique indexes.)</li>
</ul>
<p>And there is a corollary of this: if you take out a query from a stored
procedure and replace variables and parameters with constants,
you now have quite a different query. More about this later.</p>
<h3><a name="intocache">Putting the Query Plan into the Cache</a></h3>
<p>If <small>SQL</small> Server would compile a stored procedure – that is optimise and build a query plan – every time the procedure is executed, there is a big risk that <small>SQL</small> Server would
crumble from all the <small>CPU</small> resources it would take. I immediately need to qualify
this, because it is not true for all systems. In a big data warehouse where a
handful of business analysts runs complicated queries that take a minute on
average to
execute, there would be no damage if there was compilation every time – rather it
could be beneficial. But in an <small>OLTP</small> database where plenty of users run stored
procedures with short and simple queries, this concern is very much for real.</p>
<p>For this reason, <small>SQL</small> Server caches the query plan for a stored procedure, so
when the next user runs the procedure, the compilation phase can be skipped, and
execution can commence directly. The plan will stay in the cache, until some
event forces the plan out of the cache. Examples of such events are:</p>
<ul>
   <li><small>SQL</small> Server's buffer cache is fully utilised, and <small>SQL</small> Server needs to age
   out buffers that have not been used for some time from the cache. The buffer
   cache includes table data as well as query plans.</li>
   <li>Someone runs <small>ALTER PROCEDURE</small> on the procedure.</li>
   <li>Someone runs <b>sp_recompile</b> on the procedure.</li>
   <li>Someone runs the command <small>DBCC FREEPROCCACHE</small> which clears the entire plan cache.</li>
   <li><small>SQL</small> Server is restarted. Since the cache is memory-only,
   the cache is not preserved over restarts.</li>
   <li>Changing of certain configuration parameters (with <b>sp_configure</b> or
   through the Server Properties pages in <small>SSMS</small>) evict
   the entire plan cache.</li>
</ul>
<p>If such an event occurs, a new query plan will be created the next time the
procedure is executed. <small>SQL</small> Server will anew "sniff" the input parameters, and
if the parameter values are different this time, the query plan may be
different from the previous plan.</p>
<p>There are other events that do not cause the entire procedure plan to be
evicted from the cache, but which trigger recompilation of one or more individual
statements in the procedure. The recompilation occurs the next time the
statement is executed. This applies even if the event occurred after the procedure started executing.
Here are examples of such events:</p>
<ul>
   <li>Changing the definition of a table that appears in the statement.</li>
   <li>Dropping or adding an index for a table appearing in the statement. This includes rebuilding an index with <small>ALTER INDEX</small> or <small>DBCC DBREINDEX</small>.</li>
   <li>New or updated statistics for a table in the statement.
   Statistics can be created and updated by <small>SQL</small> Server automatically. The <small>DBA</small>
   can also create and update statistics with the commands <small>CREATE STATISTICS</small> and
   <small>UPDATE STATISTICS</small>. </li>
   <li>Someone runs <b>sp_recompile</b> on a table referred to in the statement.</li>
</ul>
<p class="note"><b>Note</b>: In <small>SQL</small> Server 2000, there is no statement recompilation, but the
entire procedure is always recompiled.</p>
<p>These lists are by no means exhaustive, but you should observe one thing which is <b>
not</b> there: executing the procedure with different values for the input
parameters from the original execution. That is, if the second invocation of
List_orders_2 is:</p>
<pre>EXEC List_orders_2 '19900101'</pre>
<p>The execution will still use the index on OrderDate, despite the query now
retrieves all orders. This leads to a very important observation: the parameter
values of the first execution of the procedure have a huge impact for
subsequent executions. If this first set of values for some reason is atypical,
the cached plan may not be optimal for future executions. This is why parameter sniffing is such a big deal.</p>
<p class="noteh3"><b>Note</b>: for a complete list of what can cause plans to be
flushed or statements to be recompiled, see the white paper on Plan Caching listed in
the <i><a href="#furtherreading">Further Reading</a></i> section.</p>
<h3><a name="cachekeys">Different Plans for Different Settings</a></h3>
<p>There is a plan for the procedure in the cache. That means that everyone can use it, or? No, in this section we will learn that there can be multiple
plans for the same procedure in the cache. To understand this, let's consider this contrived example:</p>
<pre>CREATE PROCEDURE List_orders_6 AS
   SELECT *
   FROM   Orders
   WHERE  OrderDate &gt; '12/01/1998'
go
SET DATEFORMAT dmy
go
EXEC List_orders_6
go
SET DATEFORMAT mdy
go
EXEC List_orders_6
go</pre>
<p>If you run this, you will notice that the first execution returns many
orders, whereas the second execution returns no orders. And if you look at the execution
plans, you will see that they are different as well. For the first execution, the plan is a
Clustered Index Scan (which is the best choice with so many rows returned), whereas the second execution plan uses Index Seek with Key Lookup (which is the
best when no rows are returned). </p>
<p>How could this happen? Did <small>SET
DATEFORMAT</small> cause recompilation? No, that would not be smart. In this example, the executions come one after each other, but they could just as well be submitted
in parallel by different users with different settings for the date format. Keep in mind that the entry
for a stored procedure in the plan cache is not tied to a certain session or
user, but it is global to all connected users.</p>
<p>Instead the answer is that <small>SQL</small> Server creates a second cache entry for the second
execution of the procedure.
We can see this if we peek into the plan cache with this query:</p>
<pre>SELECT qs.plan_handle, a.attrlist
FROM   sys.dm_exec_query_stats qs
CROSS  APPLY sys.dm_exec_sql_text(qs.sql_handle) est
CROSS  APPLY (SELECT epa.attribute + '=' + convert(nvarchar(127), epa.value) + '   '
              FROM   sys.dm_exec_plan_attributes(qs.plan_handle) epa
              WHERE  epa.is_cache_key = 1
              ORDER  BY epa.attribute
              FOR    XML PATH('')) AS a(attrlist)
WHERE  est.objectid = object_id ('dbo.List_orders_6')
  AND  est.dbid     = db_id('Northwind') </pre>
<p class="noteh3"><b>Note</b>: You need the
server-level permission <small>VIEW SERVER STATE</small> to run queries
against the plan cache.</p>
<p class="note"><b>Note</b>: The queries against the plan cache
in this article do not run on <small>SQL</small> 2000.</p>
<p>The <small>DMV</small> (Dynamic Management View) <b>
sys.dm_exec_query_stats</b> has one entry for each query currently in the plan
cache. If a procedure has multiple statements, there is one row per statement.
Of interest here is <b>sql_handle</b> and <b>plan_handle</b>. I use <b>
sql_handle</b> to determine which procedure the cache entry relates to (later we
will see examples where we also retrieve the query text) so that we can filter
out
all other entries in the cache. Most often you use <b>plan_handle</b> to
retrieve the query plan itself, and we will see an example of this later, but in this query I access a <small>DMV</small> that returns
the attributes of the query plan. More specifically, I return the attributes that
are <i>cache keys</i>. When there is more than entry in the cache for the same procedure, the entries have at least one difference in the cache keys. A cache key is a run-time setting, which for one reason
or another calls for a different query plan. Most of these settings are controlled
with a <small>SET</small> command, but not all.</p>
<p>The query above returns two rows, indicating that there are two entries for the
procedure in the cache. The output may look like this:</p>
<pre>plan_handle                                          attrlist
---------------------------------------------------- --------------------------------------------------------
0x0500070064EFCA5DB8A0A905000000000000000000000000   compat_level=100   date_first=7   date_format=1
                                                     set_options=4347   user_id=1
0x0500070064EFCA5DB8A0A805000000000000000000000000   compat_level=100   date_first=7   date_format=2
                                                     set_options=4347   user_id=1</pre>
<p>To save space, I have deleted many of the values in the <b>attrlist</b> column
and I have also folded the column into two lines. If you run the query yourself, you can see the complete list
of cache keys, and they are quite a few. If you look up the topic for
<a href="http://msdn.microsoft.com/en-us/library/ms189472.aspx" target="_blank"> <b>
sys.dm_exec_plan_attributes</b></a> in Books Online, you will see description
for many of the plan attributes, but you will also note that far from all cache
keys are documented. In this article I will not dive into all cache keys, not
even the documented ones, but focus on the most important ones.</p>
<p>As I said, the example is contrived, but it gives a good illustration to
why the query plans must be different: different date formats may yield
different results. A somewhat more normal example is this:</p>
<pre>EXEC sp_recompile List_orders_2
go
SET DATEFORMAT dmy
go
EXEC List_orders_2 '12/01/1998'
go
SET DATEFORMAT mdy
go
EXEC List_orders_2 '12/01/1998'
go</pre>
<p>(The initial <b>sp_recompile</b> is to make sure that the plan from the
previous example is flushed.) This example yields the same results and the same
plans as with List_orders_6 above. That is, the two query plans uses the actual parameter value
when the respective plan is built. The first query uses 12 Jan 1998, and the
second 1 Dec 1998.</p>
<p>A very important cache key is <b>set_options</b>. This is a bit mask that gives the setting of a number of <small>SET</small> options that can be <small>ON</small> or <small>OFF</small>. If
you look further in the topic of <b><a href="http://msdn.microsoft.com/en-us/library/ms189472.aspx" target="_blank">sys.dm_exec_plan_attributes</a></b>, you find a
listing that details which <small>SET</small> option each bit describes. (You
will also see that there are a few more items that are not controlled by the <small>SET</small>
command.) Thus, if two
connections have any of these options set differently, the connections will
use different cache entries for the same procedure – and therefore they could be using different query plans, with possibly big difference in performance.</p>
<p>One way to translate the <b>set_options</b> attribute is to run this query:</p>
<pre>SELECT convert(binary(4), 4347)</pre>
<p>This tells us that the hex value for 4347 is 0x10FB. Then we can look in Books
Online and follow the table to find out that the following <small>SET</small> options are in
force: <small>ANSI_PADDING</small>, Parallel Plan, <small>CONCAT_NULL_YIELDS_NULL, ANSI_WARNINGS, ANSI_NULLS,
QUOTED_IDENTIFIER, ANSI_NULL_DFLT_ON</small> and <small>ARITHABORT</small>. </p>
<p>You can also use this <a href="http://www.sommarskog.se/query-plan-mysteries/setoptions.sql" target="_blank">table-valued function</a> that I have written and run:</p>
<pre>SELECT Set_option FROM setoptions (4347) ORDER BY Set_option</pre>
<p class="note"><b>Note</b>: You may be wondering what <i>Parallel Plan</i> is doing here, not the least since the plan in the example is not parallel. When <small>SQL</small>
Server builds a parallel plan for a query, it may later also build a non-parallel plan if the <small>CPU</small> load in the server is such that it is not defensible to run a
parallel plan. It seems that for a plan that is always serial that the bit for parallel plan is nevertheless
set in <b>set_options</b>.</p>
<p>To simplify the discussion, we can say that each of these <small>SET</small> options – <small>ANSI_PADDING, ANSI_NULLS</small> etc –
is a cache key on its own. The fact that they are
added together in a singular numeric value is just a matter of packaging.</p>
<h3><a name="defaultsettings">The Default Settings</a></h3>
<p>About all of the <small>SET ON/OFF</small> options that are cache keys exist because of legacy reasons. Originally, in the
dim and distant past, <small>SQL</small> Server included a number of behaviours that violated
the <small>ANSI</small> standard for <small>SQL</small>. With <small>SQL</small> Server 6.5, Microsoft introduced all these <small>SET</small> options
(save for <small>ARITHABORT</small>, which was in the product already in 4.x), to permit users to use
<small>SQL</small> Server in an <small>ANSI</small>-compliant way. In <small>SQL</small> 6.5, you had to use the
<small>SET</small> options explicitly to get <small>ANSI</small>-compliant behaviour, but with <small>SQL</small> 7, Microsoft changed
the defaults for clients that used the new versions of the <small>ODBC</small> and <small>OLE DB API</small>s.
The <small>SET</small> options still remained to provide backwards compatibility for older
clients.</p>
<p class="note"><b>Note:</b> In case what you are curious what is the impact of these <small>SET</small> options, I refer
you to Books Online. Some of them are fairly straight-forward to explain,
whereas others are just too confusing. To understand this article, you only
need to understand that they exist, and what impact they have on the plan cache.</p>
<p>Alas, Microsoft did not change the defaults with full consistency, and
even today the defaults depend on how you connect, as detailed in the table
below.</p>
<table border="1" cellpadding="2" cellspacing="0">
  <tbody><tr class="theader" valign="bottom">
    <th></th><th>Applications using<br><small>ADO</small> .Net, <small>ODBC</small> or <small>OLE DB</small></th><th><small>SSMS</small>,
  <br>Query Analyzer</th><th><small>SQLCMD</small>,<br>
  <small>OSQL, BCP,<br> SQL</small> Server Agent
  </th><th><small>ISQL</small>, <br>DB-Library</th></tr>
<tr><td class="theader"><b><small>ANSI_NULL_DFLT_ON</small></b></td><td align="center"><small>ON</small></td><td align="center"><small>ON</small></td>
  <td align="center"><small>ON</small></td><td align="center"><b><small>OFF</small></b></td></tr>
<tr><td class="theader"><b><small>ANSI_NULLS</small></b></td><td align="center"><small>ON</small></td><td align="center"><small>ON</small></td>
  <td align="center"><small>ON</small></td><td align="center"><b><small>OFF</small></b></td></tr>
<tr><td class="theader"><b><small>ANSI_PADDING</small></b></td><td align="center"><small>ON</small></td><td align="center"><small>ON</small></td>
  <td align="center"><small>ON</small></td><td align="center"><b><small>OFF</small></b></td></tr>
<tr><td class="theader"><b><small>ANSI_WARNINGS</small></b></td><td align="center"><small>ON</small></td><td align="center"><small>ON</small></td>
  <td align="center"><small>ON</small></td><td align="center"><b><small>OFF</small></b></td></tr>
<tr><td class="theader"><b><small>CONACT_NULLS_YIELD_NULL</small></b></td><td align="center"><small>ON</small></td><td align="center"><small>ON</small></td>
  <td align="center"><small>ON</small></td><td align="center"><b><small>OFF</small></b></td></tr>
<tr><td class="theader"><b><small>QUOTED_IDENTIFIER</small></b></td><td align="center"><small>ON</small></td><td align="center"><small>ON</small></td><td align="center">
  <b><small>OFF</small></b></td><td align="center"><b><small>OFF</small></b></td></tr>
<tr><td class="theader"><b><small>ARITHABORT</small></b></td><td align="center"><b><small>OFF</small></b></td><td align="center"><small>ON</small></td>
  <td align="center"><b><small>OFF</small></b></td><td align="center"><b><small>OFF</small></b></td></tr>
</tbody></table>
<p>You might see where this is getting at. Your application connects with
<small>ARITHABORT OFF</small>, but when you run the query in <small>SSMS, ARITHABORT</small> is <small>ON</small> and thus
you will not reuse the cache entry that the application uses, but <small>SQL</small> Server
will compile the procedure anew, sniffing your current parameter values, and you may get a different plan than from the application.
So there you have a likely answer to the initial question of this article. There
are a few more possibilities that we will look into in the next chapter, but the
by far most common reason for <i>slow in the application, fast in <small>SSMS</small></i> in
<small>SQL</small> 2005 and later is parameter sniffing and the different defaults for
<small>ARITHABORT</small>. (If that was all you wanted to know, you can stop reading. If you
want to fix your performance problem – hang on!) </p>
<p>Beside the <small>SET</small> command and the defaults above, <small>ALTER DATABASE</small> permits you to
  say that a certain <small>SET</small> option always should be <small>ON</small> in a database and thus
  override the default set by the <small>API</small>. However, while the syntax may indicate so,
  you cannot specify than an option should be <small>OFF</small> this way. Also, beware that if
  you test these options from Management Studio, they may not seem to work, since
  <small>SSMS</small> submits explicit <small>SET</small> commands. There is also a
  server-level setting for the same purpose, the configuration option <b>user
    options</b> which is a bit mask. You can set the individual bits in the
  mask from the <b>Connection</b> pages of the <b>Server
    Properties</b> in Management Studio. Overall, I recommend against controlling
  the defaults this way, as in my opinion they mainly serve to increase the
confusion.</p>
<p>It is not always the run-time setting of an option that applies. When you create a procedure, view, table etc, the settings
for <small>ANSI_NULLS</small>
and <small>QUOTED_IDENTIFIER</small>, are saved with the object. That is, if you run this:</p>
<pre>SET ANSI_NULLS, QUOTED_IDENTIFIER OFF
go
CREATE PROCEDURE stupid @x int AS
IF @x = NULL PRINT "@x is NULL"
go
SET ANSI_NULLS, QUOTED_IDENTIFIER ON
go
EXEC stupid NULL</pre>
<p>It will print </p>
<pre>@x is NULL</pre>
<p>(When <small>QUOTED_IDENTIFIER</small> is <small>OFF</small>, double quote(<code>"</code>) is a string delimiter on equal basis with single quote(<code>'</code>). When the
setting is <small>ON</small>, double quotes delimit identifiers in the same way that square
brackets (<code>[]</code>) do and the <small>PRINT</small> statement would yield a compilation
error.)</p>
<p>In addition, the setting for <small>ANSI_PADDING</small> is saved per table column
where it is applicable (The data types <b>varchar</b>, <b>nvarchar</b> and <b>varbinary</b>).</p>
<p>All these options and different defaults are certainly confusing, but here
are some pieces of advice. First, remember that the first six of these seven
options exist only to
supply backwards compatibility, so there is little reason why you should ever
have any of them <small>OFF</small>. Yes, there are situations when some of them may seem to buy a little more
convenience if they are <small>OFF</small>, but don't fall for that temptation. One complication here, though,
is that the <small>SQL</small> Server tools spew out <small>SET</small> commands for some of these options
when you script objects. Thankfully, they mainly produce <small>SET ON</small> commands that
are harmless. (But I think that when you script a table, scripts may still in
some situations have a <small>SET ANSI_PADDING OFF</small> at the end.)</p>
<p>Next, when it comes to <small>ARITHABORT</small>, you should know that in <small>SQL</small> 2005 and later
versions, this setting has <b>zero</b> impact as long as <small>ANSI_WARNINGS</small> is <small>ON</small>.
Thus, there is no reason to turn it on for the sake of the matter. And when it comes to <small>SQL</small> Server
Management Studio, you might want do yourself a favour, and open this dialog and
uncheck <small>SET ARITHABORT</small> as highlighted:</p>
<p>
<img alt="Advanced Query Execution Options" src="Slow%20in%20the%20Application,%20Fast%20in%20SSMS_files/SSMS.jpg" witdh="747" height="428"></p>
<p>This will change your default setting for <small>ARITHABORT</small> when you connect with
<small>SSMS</small>. It will not help you to make your application to run faster, but you will not
at least have to be perplexed by getting different performance in <small>SQL</small> Server
Management Studio. </p>
<p>For reference, below is how the <small>ANSI</small> page should look
like. A very strong recommendation: never change anything on this page!</p>
<p>
<img alt="ANSI Query Execution Options" src="Slow%20in%20the%20Application,%20Fast%20in%20SSMS_files/SSMS2.jpg" witdh="747" height="428"></p>
<p>When it comes to <small>SQLCMD</small> and <small>OSQL</small>, make the habit to always use the <code>
<span class="nowrap">-I</span></code> option, which causes these tools to run with <small>QUOTED_IDENTIFIER
ON</small>. The corresponding option for <small>BCP</small> is <code><span class="nowrap">-q</span></code>. It's a little more difficult in Agent, since there is no way to change the default for Agent – at least I have not found any. Then again, if you only run stored procedures from your job steps, this is not an issue, since the saved setting for stored procedures takes precedence. But if you would run loose batchs of <small>SQL</small> from Agent jobs, you could face the problem with different query plans in the job and <small>SSMS</small> because of the different defaults for <small>QUOTED_IDENTIFER</small>. For such jobs, you should always include the the command <small>SET QUOTED_IDENTIFIER ON</small> as the first command in the job step.</p>
<p>We have already looked at <small>SET DATEFORMAT</small>, and there are two more options in
  that group: <small>LANGUAGE</small> and <small>DATEFIRST</small>. The default language is configured per user, and there is a
  server-wide configuration option which controls what is the default language for
  new users. The default language controls the default for the other two. Since
  they are cache keys, this means that two users with different default languages
will have different cache entries, and may thus have different query plans. </p>
<p>My
recommendation is that you should try avoid to be dependent on language and date settings in <small>SQL</small> Server
altogether. For instance, in as far as you use date literals at all, use a format that
is always interpreted the same, such as <small>YYYYMMDD</small>. (For more details about
date formats, see the article <i>
<a href="http://www.karaszi.com/SQLServer/info_datetime.asp">The ultimate guide
to the datetime datatypes</a></i> by <small>SQL</small> Server <small>MVP</small> Tibor Karaszi.) If you want
to produce localised output from a stored procedure depending on the user's
preferred language, it may be better to roll your own than rely on the language
setting in <small>SQL</small> Server.</p>
<h3><a name="stmtrecompile">The Effects of Statement Recompile</a></h3>
<p>To get a complete picture how <small>SQL</small> Server builds the query plan, we need to
study what happens when
<span style="font-size:12.0pt;font-family:&quot;Times New Roman&quot;,&quot;serif&quot;;
mso-fareast-font-family:&quot;Times New Roman&quot;;mso-fareast-theme-font:minor-fareast;
mso-ansi-language:EN-GB;mso-fareast-language:EN-GB;mso-bidi-language:AR-SA">
individual </span>statements are recompiled. Above, I mentioned
a few situations where it can happen, but at that point I did not go into
details.</p>
<p>The procedure below is certainly contrived, but it serves well to demonstrate
what happens.</p>
<pre>CREATE PROCEDURE List_orders_7 @fromdate datetime,
                               @ix       bit AS
   SELECT @fromdate = dateadd(YEAR, 2, @fromdate)
   SELECT * FROM  Orders WHERE OrderDate &gt; @fromdate
   IF @ix = 1 CREATE INDEX test ON Orders(ShipVia)
   SELECT * FROM  Orders WHERE OrderDate &gt; @fromdate
go
EXEC List_orders_7 '19980101', 1</pre>
<p>When you run this and look at the actual execution plan, you will see that
the plan for the first <small>SELECT</small> is a Clustered Index Scan, which agrees with what we have learnt this far. <small>SQL</small> Server sniffs the value 1998-01-01 and estimates
that the query will return 267 rows which is too many to read with seek + key lookup. What <small>SQL</small> Server does not know is
that the value of @fromdate changes before the
queries are executed. Nevertheless, the plan for the second, identical, query is precisely seek + key lookup and the estimate is that one row is returned. This is because the <small>CREATE INDEX</small>
statement sets a mark that the schema of the Orders table has changed, which
triggers a recompile of the second <small>SELECT</small> statement. When the recompiling the
statement, <small>SQL</small> Server sniffs the value of the parameter which is current at this
point, and thus finds the better plan.</p>
<p>Run the procedure again, but with different parameters (note that the date is two years earlier in time):</p>
<pre>EXEC List_orders_7 '19960101', 0</pre>
<p>The plans are the same as in the first execution, which is a little more exciting than it may seem at first glance. On this second execution, the first query is
recompiled because of the added index, but this time
the scan is the "correct" plan, since we retrieve about on third of
the orders. However, since the second query is not recompiled now, the second query runs
with the Index Seek from the previous execution, although it is not an efficient
plan this time.</p>
<p>Before you continue, clean up:</p>
<pre>DROP INDEX test ON Orders
DROP PROCEDURE List_orders_7</pre>
<p>As I said, this example is contrived. I made it that way, because I wanted a
compact example that is simple to run. In a real-life situation, you may have a procedure
that uses the same parameter in two queries against different tables. The <small>DBA</small>
creates a new index on one of the tables, which causes the query against that
table to be recompiled, whereas the other query is not. The key takeaway here is
that the plans for two statements in a
procedure may have been compiled for different "sniffed" parameter values.</p>
<p>When we have seen this, it seems logical that this could be extended to local
variables as well. But this is not the case:</p>
<pre>CREATE PROCEDURE List_orders_8 AS
   DECLARE @fromdate datetime
   SELECT @fromdate = '20000101'
   SELECT * FROM  Orders WHERE OrderDate &gt; @fromdate
   CREATE INDEX test ON Orders(ShipVia)
   SELECT * FROM  Orders WHERE OrderDate &gt; @fromdate
   DROP INDEX test ON Orders
go
EXEC List_orders_8
go
DROP PROCEDURE List_orders_8</pre>
<p>In this example, we get a Clustered Index Scan for both <small>SELECT</small> statements,
despite that the second <small>SELECT</small> is recompiled during execution and the value of
@fromdate is known at this point.</p>
<p>However, there is one exception, and that is table variables. Normally <small>SQL</small>
Server estimates that a table variable has a single row, but when there are
recompiles in sway, the estimate may be different:</p>
<pre>CREATE PROCEDURE List_orders_9 AS
   DECLARE @ids TABLE (a int NOT NULL PRIMARY KEY)
   INSERT @ids (a)
      SELECT OrderID FROM   Orders
   SELECT COUNT(*)
   FROM   Orders O
   WHERE  EXISTS (SELECT *
                  FROM   @ids i
                  WHERE  O.OrderID = i.a)
   CREATE INDEX test ON Orders(ShipVia)
   SELECT COUNT(*)
   FROM   Orders O
   WHERE  EXISTS (SELECT *
                  FROM   @ids i
                  WHERE  O.OrderID = i.a)
DROP INDEX test ON Orders
go
EXEC List_orders_9
go
DROP PROCEDURE List_orders_9</pre>
<p>When you run this you will get in total four execution plans. The two of
interest are the second and fourth plans that come from the two identical <small>SELECT COUNT</small>(*) queries. I have
included the interesting parts of the plans here, together with the pop-up for the Clustered Index Scan operator over the table variable.</p>
<p class="wideimage"><img alt="Nested-loops join" src="Slow%20in%20the%20Application,%20Fast%20in%20SSMS_files/nestedloops.jpg" height="167" width="717"></p>
<p class="wideimage"><img alt="Merge join" src="Slow%20in%20the%20Application,%20Fast%20in%20SSMS_files/mergejoin.jpg" height="163" width="721"></p>
<p>In the first plan there is a Nested Loops Join operator together with an
Clustered Index Seek on the Orders table, which is congruent with the estimate
of the number of rows in the table variable: one single row, the standard assumption. In the second query, the
join is carried out with a Merge Join together with a table scan of Orders. As you can see, the estimate for the table variable is now 830 rows, because when recompiling a query, <small>SQL</small> Server
"sniffs" the cardinality of the table variable, even if it is not a parameter. </p>
<p>And with some amount of bad luck this can cause issues similar to those with
parameter sniffing. I once ran into a situation where a key procedure in our
system suddenly was slow, and I tracked it down to a statement with a
table variable where the estimated number of rows was 41. Unfortunately, when this
procedure runs in the daily processing it's normally called on one-by-one basis,
so one row was a much better estimate in that case.</p>
<p>Speaking of table variables, you may be curious about table-valued
<i>parameters</i>, introduced in <small>SQL</small> 2008. They are handled very similar to table
variables, but since the parameter is populated before the procedure is invoked,
it is now a common situation that <small>SQL</small> Server will use an estimate of more than one row. Here is an example:</p>
<pre>CREATE TYPE temptype AS TABLE (a int NOT NULL PRIMARY KEY)
go
CREATE PROCEDURE List_orders_10 @ids temptype READONLY AS
   SELECT COUNT(*)
   FROM   Orders O
   WHERE  EXISTS (SELECT *
                  FROM   @ids i
                  WHERE  O.OrderID = i.a)
go
DECLARE @ids temptype
INSERT @ids (a)
   SELECT OrderID FROM   Orders
EXEC List_orders_10 @ids
go
DROP PROCEDURE List_orders_10
DROP TYPE temptype</pre>
<p>The query plan for this procedure is the same as for the second <small>SELECT</small> query in List_orders_9, that is Merge Join + Clustered Index
Scan of Orders, since <small>SQL</small> Server sees the 830 rows in @ids when the query is
compiled.</p>
<h3><a name="storysofar">The Story So Far</a></h3>
<p>In this chapter we have looked at how <small>SQL</small> Server compiles a stored procedure and what significance the actual parameter values have
for compilation. We have
seen that <small>SQL</small> Server puts the plan for the procedure into cache, so that the plan can be reused later. We have also seen that there can be more than one entry
for the same stored procedure in the cache. We have seen that there is a large number of different cache keys, so potentially there can be very many plans for a single
stored procedure. But we have also learnt that many of the <small>SET</small> options that are cache keys are legacy options
that you should never change. </p>
<p>In practice, the most important <small>SET</small> option is <small>ARITHABORT</small>, because the default for this option is different in an application and in <small>SQL</small> Server Management
Studio. This explains why you can spot a slow query in your application, and then run it at good speed in <small>SSMS</small>. The application uses a plan which was compiled
for
a different set of sniffed parameter values than the actual values, whereas when you run the query in <small>SSMS</small>, it is likely that there is no plan for <small>ARITHABORT
ON</small>
in the cache, so <small>SQL</small> Server will build a plan that fits with your current parameter values. </p>
<p>You have also understood that you can verify that this is the case by running
this command in your query window:</p>
<pre>SET ARITHABORT OFF</pre>
<p>and with great likelihood, you will now get the slow behaviour of the application
also in <small>SSMS</small>. If this happens, you know that you have a performance problem related to
parameter sniffing. What you may not know yet is how to address this performance problem, and
in the following chapters I will discussion possible solutions,
before I return to the theme of compilation, this time for ad-hoc queries, a.k.a. dynamic <small>SQL</small>.</p>
<p class="note"><b>Note</b>: There are always these funny variations. The application I mainly work with actually issues <small>SET ARITHABORT ON</small> when it connects, so we should never see this confusing behaviour in <small>SSMS</small>. Except that we do. Some parts of the application also issues the command <small>SET NO_BROWSETABLE ON</small> on connection. I have never been able to understand the impact of this undocumented <small>SET</small> command, but I seem to recall that it is related to early versions of "classic" <small>ADO</small>. And, yes, this setting is a cache key.</p>
<h2><a name="otherreasons">It's Not Always Parameter Sniffing...</a></h2>
<p>Before we delve into how address performance problems related to parameter
sniffing, which is quite a broad topic, I first like to give some coverage to a
couple of cases where parameter sniffing is not involved, but where you
nevertheless may experience different performance in the application and <small>SSMS</small>.</p>
<h3><a name="varparamreplace">Replacing Variables and Parameters</a></h3>
<p>I have already touched at this, but it is worth expanding on a bit. </p>
<p>Occasionally, I see people in the forums or the newsgroups that tell me that
their stored procedure is slow, but when they run the same query outside the
procedure it's fast. After a few posts in the thread, the truth is revealed: the
query they are struggling with refer to variables, be that local variables or
parameters. To troubleshoot the query on its own, they have replaced the
variables with constants. But as we have seen, the
resulting stand-alone query is quite different,
and <small>SQL</small> Server can make more accurate estimates with constants instead of
variables, and therefore arrive at a better plan. Furthermore, <small>SQL</small> Server does not have to consider that the constant may
have a different value next time the query is run.</p>
<p>A similar mistake is to make the parameters into variables. Say that you have:</p>
<pre>CREATE PROCEDURE some_sp @par1 int AS
   ...
   -- Some query that refers to @par1</pre>
<p>You want to troubleshoot this query on its own, so you do:</p>
<pre>DECLARE @par1 int
SELECT @par1 = 4711
-- query goes here</pre>
<p>From what you have learnt here, you know that this is very different from when @par1 really is a parameter. <small>SQL</small> Server has no idea about the value for
@par1 when you declare it as a local variable and will make standard assumptions.</p>
<p>But if you have a 1000-line stored procedure, and one query is slow, how do
you run it stand-alone with great fidelity, so
that you have the same presumptions as in the stored procedure?</p>
<p>One very simple option, but which also is limited, is to add <small>OPTION (RECOMPILE)</small> at the end of the query. This only works on <small>SQL</small> 2005, where this hint causes
the query to be recompiled with the values of all variables – parameters or locally declared
– sniffed and the plan will be compiled under the assumption
that the values could be different next time. That is, the method is only good if the query refers
solely to parameters, and
not to local variables in the procedure. (Because the values of local variables are normally not
sniffed.) Furthermore, the method does not work on <small>SQL</small> 2008 and later,
where the implementation of <small>OPTION (RECOMPILE)</small> is more resaonble: <small>SQL</small> Server compiles the query as if
all variable
values are constants. (Confusingly, this does not apply to all builds
of <small>SQL</small> 2008; for a while Microsoft had to revert back to the old
behaviour because of a critical bug.)</p>
<p>What always works is to embed the query in <b>sp_executesql</b>:</p>
<pre>EXEC sp_executesql N'-- Some query that refers to @par1', N'@par1 int', 4711</pre>
<p>You will need to double any single quotes in the query to be able to put it in a character literal. If the query refers to local variables, you should assign
them in the block of dynamic <small>SQL</small>.</p>
<p>Yet an option is of course to create dummy procedure with the problematic
statement; this saves from doubling any quotes. To avoid litter in the database, you could create a temporary stored
procedure:</p>
<pre>CREATE PROCEDURE #test @par1 int AS
   -- query goes here.</pre>
<p>As with
dynamic <small>SQL</small>, make sure that local variables are locally declared also in your
dummy. I will need to add the caveat I have not investigated whether <small>SQL</small> Server have special tweaks
or limitations when optimising temporary stored procedures. Not that I see why there should be any, but I have been burnt before...</p>
<h3><a name="blocking">Blocking</a></h3>
<p>You should not forget that one possible reason that the procedure ran slow in the application was simply a matter of blocking. When you tested the query three hours
later in <small>SSMS</small>, the blocker had completed its work. If you find that no matter how you run the procedure in <small>SSMS</small>, with or without <small>ARITHABORT</small>, the procedure is
always fast, blocking is starting to seem a likely explanation. Next time you are alarmed that the procedure is slow, you should start your investigation with some
blocking analysis. That is a topic which is completely outside the scope for this article, but for a good tool to investigate locking, see my
<a href="http://www.sommarskog.se/sqlutil/beta_lockinfo.html">beta_lockinfo</a>.</p>
<h3><a name="ixviews">Indexed Views and Indexed Computed Columns</a></h3>
<p>This is a problem which is far more common on <small>SQL</small> 2000 than on later versions.</p>
<p>For <small>SQL</small> Server 2005 or later to consider indexed views and indexes on computed columns (as well as filtered
indexes added in <small>SQL</small> 2008) when compiling a query, these settings must be <small>ON: QUOTED_IDENTIFIER, ANSI_NULLS, ANSI_WARNINGS, ANSI_PADDING, CONCAT_NULL_YIELDS_NULL</small>. Furthermore,
<small>NUMERIC_ROUNDABORT</small> must be <small>OFF</small>. But on <small>SQL</small> 2000, there is one more option that must be <small>ON</small>, and, yes, you guessed it: <small>ARITHABORT</small>. To be precise, this also
applies on <small>SQL</small> 2005 and <small>SQL</small> 2008 if the database compatibility level is 80. (The reason for this is that on <small>SQL</small> 2000 there is one type of error
– domain errors, e.g. <code><span class="nowrap">
sqrt(-1)</span></code> – that is covered by <small>ARITHABORT</small>, but not by <small>ANSI_WARNINGS</small>.)</p>
<p>Thus, on <small>SQL</small> 2000, an application using the default <small>SET</small> options will not be
able to take benefit of an index on a computed column or an indexed view, even
if that would be the optimal query plan. But when you run the query in <small>SSMS</small> or Query
Analyzer, performance will be a lot better, even if no parameter sniffing is
involved, because they have <small>ARITHABORT</small> <small>ON</small> by default. </p>
<p>If you are stuck on <small>SQL</small> 2000, you could change the client to always submit
<small>SET ARITHABORT ON</small> (this is not possible through the connection string; it has to
be submitted separately). You can also use <small>ALTER DATABASE</small> to turn on <small>ARITHABORT</small>
on database level. (But as I noted above, I find this setting more confusing
than helpful.)</p>
<p>There is a second phenomenon you can run into with indexed computed columns and indexed views. It's not unique to <small>SQL</small> 2000, but it happens a lot more there.
You have a stored procedure that is slow. You take out the statement and run it on its own, and now it's lightning fast, even if you package it nice and cleanly
in a temporary stored procedure as above. Why? Run this statement:</p>
<pre>SELECT objectproperty(object_id('your_sp'), 'IsQuotedIdentOn'),
       objectproperty(object_id('your_sp'), 'IsAnsiNullsOn')</pre>
<p>Most likely it will return 0 it at least one of the two columns. As I noted
<a href="#defaultsettings">previously</a>, these two settings, <small>QUOTED_IDENTIFIER</small> and <small>ANSI_NULLS</small> are saved with the procedure, and thus
the saved settings apply when the procedure runs, not the settings of the connection.</p>
<p>But why would these options be <small>OFF</small>? In <small>SQL</small> 2000, you can create stored procedures from Enterprise Manager, and Enterprise Manager always submits <small>SET
QUOTED_IDENTIFIER OFF</small> and <small>SET ANSI_NULLS OFF</small> prior to creating the object. It does not tell you, and there is no option to turn this madness off.
A much better deal is to use Query Analyzer: not only does it have the appropriate
defaults for the <small>SET</small> options, it also offers a far better editing experience. Yet, during the hey days of <small>SQL</small> 2000, it was apparent on the newsgroups that many developers were unaware of Query Analyzer, and used Enterprise Manager
exclusively.</p>
<p>For the most of us <small>SQL</small> 2000 is behind us, but you can still run into this issue for two reasons:</p>
<ol>
  <li>The database was upgraded from <small>SQL</small> 2000.</li>
  <li>The procedure was created in a script that was executed with <small>SQLCMD</small> and <small>OSQL</small>, both which run with <small>QUOTED_IDENTIFIER OFF</small> by default. (Always run these
  tools with the <code><span class="nowrap">-I</span></code> option to override.)</li>
</ol>
<p>To find all bad modules in a database on <small>SQL</small> 2005 or later, you can use this <small>SELECT</small>:</p>
<pre>SELECT o.name
FROM   sys.sql_modules m
JOIN   sys.objects o ON m.object_id = o.object_id
WHERE  (m.uses_quoted_identifier = 0 or
        m.uses_ansi_nulls = 0)
  AND  o.type NOT IN ('R', 'D') </pre>
<h3><a name="linkedservers"></a>An Issue With Linked Servers</h3>
<p>This section concerns an issue with linked servers when the <i>remote</i> server is earlier than <small>SQL</small> 2012 SP1. Consider this query:</p>
<pre>SELECT C.*
FROM   SOME_SERVER.Northwind.dbo.Orders O
JOIN   Customers C ON O.CustomerID = C.CustomerID
WHERE  O.OrderID &gt; 20000</pre>
<p>I ran this query twice, logged in as two different users. The first user is sysadmin on both servers, whereas the second user is a plain user with only <small>SELECT</small> permissions. To ensure that I would get different cache entries, I used different settings for <small>ARITHABORT</small>.</p>
<p>When I ran the query as sysadmin I got this plan:</p>
<p><img src="Slow%20in%20the%20Application,%20Fast%20in%20SSMS_files/linked-query1.jpg" alt="Linked-query1" class="image" height="187" width="591"></p>
<p>When I ran the query as the plain user, the plan was different:</p>
<p><img src="Slow%20in%20the%20Application,%20Fast%20in%20SSMS_files/linked-query2.jpg" alt="Linked query 2" class="image" height="178" width="599"></p>
<p>How come the plans are different? It's certainly not parameter sniffing because there are no parameters. As always when a query plan has an unexpected shape or operator, it is a good idea to look at the estimates. Here are the pop-ups for the Remote Query operators in the two plans, with the pop-up for the first plan to the left:</p>
<table style="margin-left: 1cm;" border="0">
  <tbody><tr>
    <td><img src="Slow%20in%20the%20Application,%20Fast%20in%20SSMS_files/linked-popup1.jpg" alt="Estimate is 1 row" height="171" width="308"></td>
    <td>   </td>
    <td><img src="Slow%20in%20the%20Application,%20Fast%20in%20SSMS_files/linked-popup2.jpg" alt="Estimate is 249 rows" height="182" width="292"></td>
  </tr>
</tbody></table>
<p>You can see that the estimates are different. When I ran as sysadmin, the estimate was 1 row, which is a correct number, since there are no Orders in Northwind where the order ID exceeds 20000. But when I ran as a plain user, the estimate was 249 rows. We recognize this particular number as 30 % of 830 orders, or the  estimate for an inequality operation when the optimizer has no information. Previously, this was due to an unknown variable value, but in this case there is no variable that can be unknown. No, it is the statistics themselves that are missing. </p>
<p>As long as a query accesses only tables in the local server, the optimizer can always access the statistics for all tables in the query; there are no extra permission checks. But this is different with tables on a linked server. When <small>SQL</small> Server accesses a linked server, there is no secret protocol that is only used for inter-server communication. No, instead <small>SQL</small> Server uses the standard <small>OLE DB</small> interface for linked servers, be that other <small>SQL</small> Server instances, Oracle, text files or your home-brewed data source, and connects just like any other user. Exactly how statistics is retrieved depends on the data source and the <small>OLE DB</small> provider in question. In this case, the provider is <small>SQL</small> Server Native Client which retrieves the statistics in two steps. (You can see this by running Profiler against the remote server). First the provider runs the procedure <b>sp_table_statistics2_rowset</b> which returns information about which column statistics there are, as well as their cardinality and their density information. In the second step, the provider runs <small>DBCC SHOW_STATISTICS</small>, a command that returns the full distribution statistics. (We will look closer at this command later in this article.) Here is the catch: up to the <small>RTM</small> version of <small>SQL</small> Server 2012, to have permission to run <small>DBCC SHOW_STATISTICS</small>, you had to be a member of the server role <b>sysadmin</b> or any of the database roles <b>db_owner</b> or <b>db_ddladmin</b>. </p>
<p>And this is why I got different results. When running as sysadmin I got  the full distribution statistics which indicated that there are no rows with order ID &gt; 20000, and the estimate was one row. (Recall that the optimizer never assumes zero rows from statistics.) But when running as the plain user, <small>DBCC SHOW_STATISTICS</small> failed with a permission error. This error was not propagated, but instead the optimizer accepted that there were no statistics and used default assumptions. Since it did get cardinality information, it learnt that the remote table has 830 rows, whence the estimate of 249 rows.</p>
<p>Whenever you encounter a performance problem where a query that includes access to a linked server is slow in the application, but it runs fast when you test it from <small>SSMS</small>, you should always investigate if insufficient permissions on the remote database could be the cause. (Keep in mind that the access to the linked server may not be overt in the query, but could be hidden in a view.) If you determine that permissions on the remote database is the problem, what actions could you take? </p>
<ul>
  <li>You can add the users to the role <b>db_ddladmin</b> on the remote database, but since this gives them right to add and drop tables, this is not recommendable.</li>
  <li>By default, when a users connect to a remote server they connect as themselves, but you can set up a login mapping with <b>sp_addlinkedsrvlogin</b>, so that  users map to a proxy account  that has membership in <b>db_ddladmin</b>. Note that this proxy account must be an <small>SQL</small> login, so this is not an option if the remote server does not have <small>SQL</small> authentication enabled. This solution too is somewhat dubious from a security perspective, although it's better the previous suggestion.</li>
  <li>In some cases you can rewrite the query with <small>OPENQUERY</small> to force evaluation on the remote server. This can be particularly useful, if the query includes several remote tables. (But it can also backfire, because the optimizer now gets even less statistics information from the remote server.)</li>
  <li>You could of course use the full battery of hints and plan guides to get the plan you want.</li>
  <li>Finally, you should ask yourself whether that linked-server access is needed. Maybe the databases could be on the same server? Could data be replicated? Some other solution?</li>
</ul>
<p>To repeat: what matters is the permissions on the remote server, not the local server where the query runs. Furthermore, the issue exists only when the <small>SQL</small> Server version of the remote server is the <small>RTM</small> version of <small>SQL</small> Server 2012 or any earlier versions of <small>SQL</small> Server. Starting with service pack 1 of <small>SQL</small> 2012, the permission requirement for <small>DBCC SHOW_STATISTICS</small> has been relaxed, so that it is sufficient to have <small>SELECT</small> permission on the table, making this gotcha null and void. (To be precise, the change came with one of the cumulative updates to the <small>RTM</small> version of <small>SQL</small> 2012, exactly which I don't recall.)</p>
<p>Note also that this section has no relevance to what may happen with other remote data sources such as Oracle, My<small>SQL</small> or Access as they have a different permission system. However, if you would run a query from, say, an Oracle server against a remote <small>SQL</small> Server 2008 instance, it is likely that this issue could affect your performance.</p>
<h2><a name="sniffinfo">Getting Information to Solve Parameter Sniffing Problems</a></h2>
<p>We have learnt how it may come that you have a stored
procedure that runs slow in the application, and the very same call runs fast
when you try it in <small>SQL</small> Server Management Studio: Because of different settings of
<small>ARITHABORT</small> you get different cache entries, and since <small>SQL</small> Server employs
parameter sniffing, you may get different execution plans.</p>
<p>While the secret behind the mystery now has been unveiled, the main problem
still remains: how do you address the performance problem? From what you read
this far, you already know of a quick
fix. If you have never seen the problem before and/or the situation is urgent, you
can always do:</p>
<pre>EXEC sp_recompile problem_sp</pre>
<p>As we have seen, this will flush the procedure from the plan cache, and next
time it is invoked, there will be a new query plan. And if the problems never
comes back, consider case closed.</p>
<p>But if the problem keeps reoccurring – and unfortunately, this is the more
likely outcome – you need to perform a deeper analysis,
and in this situation you should <b>not</b> use sp_recompile, or in some
other way
alter the procedure. You should keep that slow plan around, so that you can
examine it, and not the least find what parameter values the bad
plan was built for. This is the topic for this
chapter.</p>
<p>Before I go on, a small note: above I recommended that you should change
your preferences in <small>SSMS</small>, so that you by default connect with <small>ARITHABORT</small>
<small>OFF</small> to
avoid this kind of confusion. But there is actually a small disadvantage with
having the same settings as the application: you may not observe that the
performance problem is related to parameter sniffing. But if you make it a habit
when investigating performance issue to run your problem procedure with <small>ARITHABORT</small> both <small>ON</small> and
<small>OFF</small>, you can easily conclude whether parameter sniffing is involved.</p>
<h3><a name="factgathering">Getting the Necessary Facts</a></h3>
<p>All performance troubleshooting requires facts. If you don't have facts you will be in
the situation that Bryan Ferry describes so well in the song <i>Sea Breezes</i>
from the first Roxy Music album:</p>
<blockquote>
  <i>We've been running round in our present state<br>Hoping help will come from
  above<br>But even angels there make the same mistakes </i>
</blockquote>
<p>If you don't have facts, not even the angels will be able to help you. The
base facts you need to troubleshoot performance issues related to parameter
sniffing are:</p>
<ol>
  <li>Which is the slow statement?</li>
  <li>What are the different query plans?</li>
  <li>What parameter values did <small>SQL</small> Server sniff?</li>
  <li>What are the table and index definitions?</li>
  <li>How does the distribution statistics look like? Is it up to date?</li>
</ol>
<p>Almost all of these points apply to about any query-tuning effort. Only
the third point is unique to parameter-sniffing issues. That, and the plural in
the second point: you want to look at two plans, the good plan and the bad plan.
In the following sections, we will look at these points one by one.</p>
<h3><a name="slowstatement">Which is the Slow Statement?</a></h3>
<p>First on the list is to find the slow statement – in most cases, the problem
lies with a single statement. If the procedure has only one statement, this is
trivial. Else, you can use Profiler to find out; the Duration column will tell
you. Either just trace the procedure
from the application, or run the procedure from Management Studio (with
<small>ARITHABORT OFF</small>!) and filter for your own spid. </p>
<p>Yet an option is to use the stored procedure <b> <a href="http://www.sommarskog.se/sqlutil/sqltrace.html">
sqltrace</a></b>, written by Lee Tudor and which I am glad to host on my web site.
<b>sqltrace</b> takes an <small>SQL</small> batch as parameter, starts a server-side trace,
runs the batch, stops the trace and then summarises the result. There are a
number of input parameters to control the procedure, for instance how to sort
the output.</p>
<h3><a name="ssmsgettingplans">Getting the Query Plans and Parameters with Management Studio</a></h3>
<p>In many cases you can easily find the query plans by running the procedure in
Management Studio, after first enabling Include Actual Execution Plan (you find
it under the Query menu). This works well, as long as the procedure does not
include a multitude of queries, in which case the Execution Plan tab gets too
littered to work with. We will look at alternative strategies in the next
section.</p>
<p>Typically you would run the procedure like this:</p>
<pre>SET ARITHABORT ON
go
EXEC that_very_sp 4711, 123, 1
go
SET ARITHABORT OFF
go
EXEC that_very_sp 4711, 123, 1</pre>
<p>The assumption here is that the application runs with the default options, in
which case the first call will give the good plan – because the plan is sniffed
for the parameters you provide – and the second call will run with the bad plan
which already is in the plan cache.
To determine the cache keys in sway, you can use the query in the section <i>
<a href="#cachekeys">Different Plans for Different Settings</a> </i>to see the cache-key values for the plan(s) in the cache. (If you already have tried the
procedure in Management Studio, you may have two entries. The column <b>
execution_count</b> in <b>sys.dm_exec_query_stats</b> can help you to discern the entries
from each other; the one with the low count is probably your attempt from <small>SSMS</small>.)</p>
<p>Once you have the plans, you can easily find the sniffed parameter values.
Right-click the left-most operator in the plan – the one that reads <small>SELECT,
INSERT</small>, etc – select Properties, which will open a pane to the right. (That is
the default position; the pane is detachable.) Here is an example how it can
look like:</p>
<p class="image"><img alt="Query-plan Properties" src="Slow%20in%20the%20Application,%20Fast%20in%20SSMS_files/paramvalues.jpg" height="550" width="317"></p>
<p>The first <i>Parameter Compiled Value</i> is the sniffed value which is
causing you trouble one way or another. If you know your application and the
pattern how it is used, you may get an immediate revelation when you see the
value. Maybe you do not, but at least you know now that there is a situation where the application calls the procedure with this possibly odd
value.</p>
<p>Note also that you can see the
settings of some of the <small>SET</small> options that are cache keys. However, beware that
there is a
<a href="https://connect.microsoft.com/SQLServer/feedback/details/335971/statistics-xml-always-report-the-same-statementsetoptions" target="_blank">
bug</a> in <small>SQL</small> 2005, so that that the <small>SET</small> options will be always be reported as
False. This is an engine bug, and thus your version of <small>SSMS</small> does not matter.</p>
<p>While Management Studio provides a very good interface to examine query
plans, I still want to put in a plug for a more versatile alternative, to wit
<a href="http://www.sqlsentry.net/plan-explorer/sql-server-query-view.asp"><small>SQL</small>
Sentry Plan Explorer</a>, a free tool from <small>SQL</small> Sentry, a tool vendor in the <small>SQL</small>
Server space. Their Plan Explorer permits you view the plan in different ways, and it is
also easier to navigate among the queries if your batch has a lot of them.</p>
<p>When you look at a query plan, it is far from always apparent what part of the plan that is really costly. But the
thickness of the arrows is a good lead. The thicker the arrow, the more rows are passed to the next operator. And if you are looking at an actual execution
plan, the thickness is based on the actual number of rows. On the other hand, I usually don't give any attention to the percentages at all. They are always
estimates, and they can be way off, particular it there is a gross misestimate somewhere in the plan.</p>
<h3><a name="dmvgettingplans">Getting the Query Plans and Parameters Directly from the Plan Cache</a></h3>
<p>It is not always feasible to use <small>SSMS</small> to get the query plans and the sniffed parameter values. The bad query maybe runs for more minutes than your patience
can accept, or the procedure includes too many statements that you only get a mess in <small>SSMS</small>. Not the least this can be an issue if the procedure includes a loop
that is executed many times, in which case not even <small>SQL</small> Sentry Plan Explorer may be workable.</p>
<p>One option to get hold of the query plan and the sniffed parameters is to retrieve it directly from the plan cache. This is quite convenient
with help of the query below, but there is an obvious limitation with this method: you only get the estimates.
The actual number of rows and actual number of executions, two values that are
very important to understand why a plan is bad, are missing.</p>
<h4>The Query</h4>
<p>This query will return the statements, the sniffed parameter values and the
query plans for a stored procedure:</p>
<pre>DECLARE @dbname    nvarchar(256),
        @procname  nvarchar(256)
SELECT @dbname = 'Northwind',
       @procname = 'dbo.List_orders_11'

; WITH basedata AS (
   SELECT qs.statement_start_offset/2 AS stmt_start,
          qs.statement_end_offset/2 AS stmt_end,
          est.encrypted AS isencrypted, est.text AS sqltext,
          epa.value AS set_options, qp.query_plan,
          charindex('&lt;ParameterList&gt;', qp.query_plan) + len('&lt;ParameterList&gt;')
             AS paramstart,
          charindex('&lt;/ParameterList&gt;', qp.query_plan) AS paramend
   FROM   sys.dm_exec_query_stats qs
   CROSS  APPLY sys.dm_exec_sql_text(qs.sql_handle) est
   CROSS  APPLY sys.dm_exec_text_query_plan(qs.plan_handle,
                                            qs.statement_start_offset,
                                            qs.statement_end_offset) qp
   CROSS  APPLY sys.dm_exec_plan_attributes(qs.plan_handle) epa
   WHERE  est.objectid  = object_id (@procname)
     AND  est.dbid      = db_id(@dbname)
     AND  epa.attribute = 'set_options'
), next_level AS (
   SELECT stmt_start, set_options, query_plan,
          CASE WHEN isencrypted = 1 THEN '-- ENCRYPTED'
               WHEN stmt_start &gt;= 0
               THEN substring(sqltext, stmt_start + 1,
                              CASE stmt_end
                                   WHEN 0 THEN datalength(sqltext)
                                   ELSE stmt_end - stmt_start + 1
                              END)
          END AS Statement,
          CASE WHEN paramend &gt; paramstart
               THEN CAST (substring(query_plan, paramstart,
                                   paramend - paramstart) AS xml)
          END AS params
   FROM   basedata
)
SELECT set_options AS [SET], n.stmt_start AS Pos, n.Statement,
       CR.c.value('@Column', 'nvarchar(128)') AS Parameter,
       CR.c.value('@ParameterCompiledValue', 'nvarchar(128)') AS [Sniffed Value],
       CAST (query_plan AS xml) AS [Query plan]
FROM   next_level n
CROSS  APPLY   n.params.nodes('ColumnReference') AS CR(c)
ORDER  BY n.set_options, n.stmt_start, Parameter</pre>
<p>If you have never worked with these DMVs before, I appreciate if this is
mainly mumbo-jumbo to you. To
keep the focus on the main subject of this article, I will
defer to a later section to explain this query. The only thing I like to give
attention to here and now is that you specify the database and the procedure
you want to work with in the beginning. You may think this would better be a
stored procedure, but it is quite likely that you want to add or remove columns, depending on
what you are looking for. </p>
<h4>The Output</h4>
<p>To see the query in this action, you can use this test batch (and, yes, the
examples get more and more contrived as we move on):</p>
<pre>CREATE PROCEDURE List_orders_11 @fromdate datetime,
                                @custid   nchar(5) AS
SELECT @fromdate = dateadd(YEAR, 2, @fromdate)
SELECT *
FROM   Orders
WHERE  OrderDate &gt; @fromdate
  AND  CustomerID = @custid
IF @custid = 'ALFKI' CREATE INDEX test ON Orders(ShipVia)
SELECT *
FROM   Orders
WHERE  CustomerID = @custid
  AND  OrderDate &gt; @fromdate
IF @custid = 'ALFKI' DROP INDEX test ON Orders
go
SET ARITHABORT ON
EXEC List_orders_11 '19980101', 'ALFKI'
go
SET ARITHABORT OFF
EXEC List_orders_11 '19970101', 'BERGS'</pre>
<p>When you have executed this batch, you can run the query above. When I do
this I see this result in <small>SSMS</small>:</p>
<p>
<img alt="Results of plan-cache query" src="Slow%20in%20the%20Application,%20Fast%20in%20SSMS_files/planinfo.jpg" height="172" width="1065"></p>
<p>These are the columns:</p>
<p>
<b><small>SET</small></b> – The <b>set_options</b> attribute for the plan. As I discussed earlier, this is a bit mask.
In this picture, you see the two most likely values. 251 is
the default settings and 4347 is the default settings + <small>ARITHABORT ON</small>. If you see other values, you can use the function
<a href="http://www.sommarskog.se/query-plan-mysteries/setoptions.sql" target="_blank">setoptions</a> to translate the bit mask.</p>
<p>
<b>Pos</b> – This is the position for the query in the procedure, counted in
characters from the start of the batch that created the procedure,
including any comments preceding <small>CREATE PROCEDURE</small>. Not terribly useful in
itself, but serves to sort the statements in the order they appear in the
procedure.</p>
<p>
<b>Statement</b> – The <small>SQL</small> statement. Note that the statements are repeated once for
each parameter in the query.</p>
<p>
<b>Parameter</b> – The name of the parameter. Only parameters that
appear in this statement are listed. As a consequence of this, statements that do not refer to any parameters are not included in the output
at all. </p>
<p>
<b>Sniffed Value</b> – The compile-time value for the parameter, that is, the value
that the optimizer sniffed when it built the plan. In difference to the Properties pane for the
plan, you don't see any actual parameter value here. As I discussed previously, the sniffed value for a parameter can be different for different
statements in the procedure, and you see an example of this in the picture above. </p>
<p>
<b>Query Plan</b> – The query plan. If you have <small>SQL</small> Server Management
Studio 2008 or later, you can click on the <small>XML</small> document, and you will see the graphical
plan directly. If you have <small>SSMS</small> 2005, you will only see the <small>XML</small> document. You
can save it with the extension .sqlplan, and then re-open it to see the graphics. As I noted
above, this is only the estimated plan. You cannot get any actual values from the cache.</p>
<h4>The Query Explained</h4>
<p>This query refers to some DMVs which not all readers may be acquainted with. It also uses some query techniques that you
may not be very familiar with, for instance
XQuery. It would take up too much space and distract you from the main topic to
dive into the query in full, so I will explain it only briefly. If the query and the explanation goes over
your head, don't feel too bad about it. As long you understand the output, you
can still have use for the query.</p>
<p>The query uses two CTEs (Common Table Expression). The first <small>CTE</small>,
<b>basedata</b>, includes all access to DMVs.
We have already seen of all of
them but <b>sys.dm_exec_text_query_plan</b>. There are two more columns we
retrieve from <b>sys.dm_exec_query_stats</b>, to wit <b>statement_start_offset</b>
and <b>statement_end_offset</b>. They delimit the statement for this row, and we
pass them to <b>sys.dm_exec_text_query_plan</b> to get the plan for this
statement only. (Recall that the procedure is a single cache entry with a
single <b>plan_handle</b>.) <b>sys.dm_exec_text_query_plan</b> returns the column <b>query_plan</b>
which contrary to what you may expect is <b>nvarchar(<small>MAX</small>)</b>.
The reason for this is that the <small>XML</small> for a query plan may be so deeply
nested, that it cannot be represented with <small>SQL</small> Server's built-in <b>xml</b>
data type. The <small>CTE</small> returns the query plan as such, but it also extracts the
positions for the part in the document where the parameter values appear.</p>
<p>In the next <small>CTE</small>, <b>next_level</b>, I go on and use the values
obtained in <b>basedata</b>. The <small>CASE</small>
expression extracts the statement from the text returned by <b>sys.dm_exec_sql_text</b>.
The way to do this is fairly clunky, not the least with those long column names.
Since there is little reason to modify that part of the query, I say no more but
refer you to Books Online. Or just believe me when I say it works. :-) The next
column in the <small>CTE</small>, <b>params</b>, performs the actual extraction of the parameter
values from the query-plan document and converts that element to the <b>xml</b>
data type.</p>
<p>In the final <small>SELECT</small>, I shred the <b>params</b> document, so that we get one row
per parameter. It can certainly be argued that it is better to have all
parameters on a single row, since in this case each statement will only appear
once, and you could easily modify the final <small>SELECT</small> to achieve that. In the final
<small>SELECT</small>, I also convert the query-plan column to <small>XML</small>, but as noted above, this
could fail because of limitations with the <b>xml</b> data type. If you get such
an error, just comment out that column.</p>
<p>Beside the alterations I have already mentioned, there are several ways you
could modify the query to get information you find interesting. For instance,
you could add more columns
from <b>sys.dm_exec_query_stats</b> or more plan
attributes. I opted to include the <b>set_options</b> attribute only, since this
is the cache key which is most likely to vary. If you would like to include all statements in the procedure, including those that do not refer to any of the
input parameters, just change <small>CROSS APPLY</small> on the next-to-last line to <small>OUTER APPLY</small>.</p>
<h3><a name="tracegettingplans">Getting Query Plans and Parameters from a Trace</a></h3>
<p>Yet an alternative to get hold of the query plans is to run a trace against the application or against your connection in <small>SSMS</small>. There are several Showplan
events you can include in a trace. The most versatile is Showplan <small>XML</small> Statistics Profile which gives you the same information as you see in <small>SSMS</small> when you enable
Include Actual Execution Plan.</p>
<p>However, for several reasons a trace is rarely a very good alternative. To start with, enabling query-plan information in a trace induces quite some load on the
server. And observe that this applies even if you narrow your filter to a single spid. The way the trace engine works, all processes still have to
generate the event. I know. By mistake I once left a trace with Showplan <small>XML</small> Statistics Profile, filtered for my own spid,
running on a production
server. It wasn't fun.</p>
<p>Next, if you run the trace in Profiler, you are likely to find it very difficult to set up a good filter that captures what you want to see but hides all
the noise. One possibility, once the trace has completed, is to save the trace
to a table in the database, which permits you to find the interesting
information through queries. (But don't ask Profiler to save to a table
while the trace is running. The overhead for that is awful.) The plan is in the
TextData column. Cast it to <b>xml</b> and then you can view it as I described
in the previous section.</p>
<p>In <small>SQL</small> 2008 you can also use Extended Events to get hold of the query plan, but this does not appear to be any simple work. I have not worked with
Extended Events myself, so I cannot give any examples.</p>

<h3><a name="tableandindexdefs">Getting Table and Index Definitions</a></h3>
<p>I assume that you are already acquainted with ways to find out how a table is
defined, either with <b>sp_help</b> or through scripting, so I jump directly to the topic of indexes. They too can be scripted or you can use <b>sp_helpindex</b>.
But scripting is bulky in my opinion, and sp_helpindex does not support features
added in <small>SQL</small> 2005 or later. This query can be helpful:</p>
<pre>DECLARE @tbl nvarchar(265)
SELECT @tbl = 'Orders'

SELECT o.name, i.index_id, i.name, i.type_desc,
       substring(ikey.cols, 3, len(ikey.cols)) AS key_cols,
       substring(inc.cols, 3, len(inc.cols)) AS included_cols,
       stats_date(o.object_id, i.index_id) AS stats_date,
       i.filter_definition
FROM   sys.objects o
JOIN   sys.indexes i ON i.object_id = o.object_id
CROSS  APPLY (SELECT ', ' + c.name +
                     CASE ic.is_descending_key
                          WHEN 1 THEN ' DESC'
                          ELSE ''
                     END
              FROM   sys.index_columns ic
              JOIN   sys.columns c ON ic.object_id = c.object_id
                                  AND ic.column_id = c.column_id
              WHERE  ic.object_id = i.object_id
                AND  ic.index_id  = i.index_id
                AND  ic.is_included_column = 0
              ORDER  BY ic.key_ordinal
              FOR XML PATH('')) AS ikey(cols)
OUTER  APPLY (SELECT ', ' + c.name
              FROM   sys.index_columns ic
              JOIN   sys.columns c ON ic.object_id = c.object_id
                                  AND ic.column_id = c.column_id
              WHERE  ic.object_id = i.object_id
                AND  ic.index_id  = i.index_id
                AND  ic.is_included_column = 1
              ORDER  BY ic.index_column_id
              FOR XML PATH('')) AS inc(cols)
WHERE  o.name = @tbl
  AND  i.type IN (1, 2)
ORDER  BY o.name, i.index_id</pre>
<p>As listed, the query will not run on <small>SQL</small> 2005, but just remove the final
column, <b>filter_definition</b>, from the result set. This column applies to
filtered indexes, a feature added in <small>SQL</small> 2008. As for the column <b>stats_date</b>,
see the next section.</p>
<p>The query only lists regular relational indexes, not <small>XML</small> indexes or spatial indexes. Problems related to searches in <small>XML</small> documents or spatial columns are
beyond the scope for this article anyway.</p>
<h3><a name="statinfo">Finding Information About Statistics</a></h3>
<p>To view all statistics for a table, you can use this query:</p>
<pre>DECLARE @tbl nvarchar(265)
SELECT @tbl = 'Orders'

SELECT o.name, s.stats_id, s.name, s.auto_created, s.user_created,
       substring(scols.cols, 3, len(scols.cols)) AS stat_cols,
       stats_date(o.object_id, s.stats_id) AS stats_date,
       s.filter_definition
FROM   sys.objects o
JOIN   sys.stats s ON s.object_id = o.object_id
CROSS  APPLY (SELECT ', ' + c.name
              FROM   sys.stats_columns sc
              JOIN   sys.columns c ON sc.object_id = c.object_id
                                  AND sc.column_id = c.column_id
              WHERE  sc.object_id = s.object_id
                AND  sc.stats_id  = s.stats_id
              ORDER  BY sc.stats_column_id
              FOR XML PATH('')) AS scols(cols)
WHERE  o.name = @tbl
ORDER  BY o.name, s.stats_id</pre>
<p>As with the query for indexes, the query does not run for <small>SQL</small> 2005 as listed, but just remove <b>filter_definition</b> from the result set.<b> auto_created</b>
refers to statistics that <small>SQL</small> Server creates automatically when it gets the occasion, while <b>user_created</b> refers to indexes created explicitly with <small>CREATE
STATISTICS</small>. If both are 0, the statistics exists because of an index. </p>
<p>The column <b>stats_date</b> returns when the statistics most recently was
updated. If the date is way back in the past, the statistics may be out of date.
The root cause to parameter-sniffing-related problems is usually something else
than outdated statistics, but it is always a good idea to look out for this. One
thing to keep in mind is that statistics for columns with monotonically
increasing data – e.g. id and date columns – quickly go out of date, because
queries are often for the most recently inserted data, which is always beyond
the last slot in the histogram (more about histograms later).</p>
<p>If you believe statistics are out of date, you can use this command:</p>
<pre>UPDATE STATISTICS tbl WITH FULLSCAN, INDEX</pre>
<p>This updates all statistics related to indexes on the table by scanning them
in full. <small>FULLSCAN</small> is not necessary, but I have been burned too often by
inaccurate statistics to be really comfortable with sampled statistics (which is
the default). Restricting the statistics update to indexes only reduces execution
time considerably, because <small>SQL</small> Server scans the table once for each non-index
statistics.</p>
<p>You can also update statistics for a single index. The syntax for this is not
what you may expect:</p>
<pre>UPDATE STATISTICS tbl indexname WITH FULLSCAN</pre>
<p class="note"><b>Note</b>: there is no period between the table name and the index name, just space.</p>
<p>Note that after updating statistics, you may see an immediate performance
improvement in the application. This does necessarily prove that outdated
statistics was the problem. Since updated statistics causes recompilation, the
parameters may be re-sniffed and you get a better plan.</p>
<p>To see the distribution statistics for an index use <small>DBCC SHOW_STATISTICS</small>.
This command takes two parameters. The first is the table name, whereas the
second can be the name of an index or statistics, but you can also specify a
column name. For instance:</p>
<pre>DBCC SHOW_STATISTICS (Orders, OrderDate)</pre>
<p>This displays three result sets. I will not cover all of them here, but only say that the
last result set is the actual histogram for the statistics. The histogram
reflects the distribution that <small>SQL</small> Server has recorded about the data in the
table. Here is how the first few lines look in the example:</p>
<p class="wideimage">
<img alt="Histogram" longdesc="http://www.sommarskog.se/DBCC%20SHOW_STATISTICS%20(Order,%20OrderDate)" src="Slow%20in%20the%20Application,%20Fast%20in%20SSMS_files/showstats.jpg" height="119" width="668"></p>
<p>This tells us that according to the statistics there is exactly one row with
OrderDate = <span class="nowrap">1996-07-04</span>. Then there is one row in the range from
1996-07-05 to 1996-07-07 and two rows with OrderDate = 1996-07-08. (Because
<small>RANGE_ROWS</small> is 1 and <small>EQ_ROWS</small> is 2.) The histogram then continues, and there is in
total 188 steps for this particular statistics. There are never more than 200
steps in a histogram. For full details on the output, please see the topic for <a href="http://msdn.microsoft.com/en-us/library/ms174384.aspx" target="_blank"><small>DBCC
SHOW_STATISTICS</small></a> in Books
Online. One of the white papers listed in the section <i>
<a href="#furtherreading">Further Reading</a></i> has more valuable information
about statistics and the histogram.</p>
<p class="note"><b>Note</b>: In <small>SQL</small> 2005 there is a bug, so if there is a step for
<small>NULL</small> values, <small>DBCC SHOW_STATISTICS</small> fails to show
this row. This is a display error, and the value is still there in the
histogram.</p>
<p>You do typically not run <small>DBCC SHOW_STATISTICS</small> for all statistics to have the information
just in case, but only when you think that the information may be useful to you.
We will look such an example in the next chapter.</p>
<h2><a name="fixexamples">Examples of How to Fix Parameter-Sniffing Issues</a></h2>
<p>It is important to understand that parameter sniffing in itself is not a problem; au contraire, it is a feature, since without it <small>SQL</small> Server would have to
rely on blind assumptions, which in most cases would lead to less optimal query plans. But sometimes parameter sniffing works against you.
We can identify three typical situations:</p>
<ol>
  <li>The query usage is such that parameter sniffing is entirely inappropriate.
  That is, a plan which is good for a certain execution may be
  inappropriate for the next.</li>
  <li>There is a specific pattern in the application where one group of calls is
  very different from the main bulk. Often this is a call the
  application performs on start-up or at the beginning of a new day. </li>
  <li>The index structure for one or more tables is such that there is no perfect
  index for the query, but there are several half-good indexes, and it is
  haphazard which index the optimizer chooses.</li>
</ol>
<p>It can be difficult to say beforehand which applies to your situation, and
that is why you need to make a careful analysis. In the previous section I
discussed what information you need, although I did not always make it clear
what for. In addition, there is one more thing I did not list but which be
immensely helpful: intimate knowledge about how the application works and its
usage pattern.</p>
<p>Since there are several possible reasons why parameter sniffing gives you a
headache, this means that there is no single solution that you can apply. Rather
there is a host of them, depending on where the root cause lies. In the following I will give some examples of parameter-sniffing related
issues and how to address them. Some are real-life examples, others are more
generic in nature. Some of the examples focus more on the analysis, others take
a straight look at the solution.</p>
<h3><a name="nonsolution">A Non-Solution</a></h3>
<p>Before I go into the real solutions, let me first point out that adding <small>SET
ARITHABORT ON</small> to your procedure is <b>not</b> a solution. It will seem to work
when you try it. But that is
only because you recreated the procedure which forced a new compilation and then the next invocation sniffed the current set of parameters. <small>SET ARITHABORT ON</small> is
only a placebo, and not even a good one. The problem will most likely come back. It will not even help you avoid the confusion with
different performance in the application and <small>SSMS</small>, because the overall cache
entry will still have <small>ARITHABORT OFF</small> as its plan attribute.</p>
<p>So, don't put <small>SET ARITHABORT ON</small> in your stored procedures. Overall, I
strongly discourage from you using any of the <small>SET</small> commands that are cache keys in
your code.</p>
<h3><a name="inputdepends">Best Index Depends on Input</a></h3>
<p>Consider this procedure:</p>
<pre>CREATE PROCEDURE List_orders_12 @custid   nchar(5),
                                @fromdate datetime,
                                @todate   datetime AS
   SELECT *
   FROM   Orders
   WHERE  CustomerID = @custid
     AND  OrderDate BETWEEN @fromdate AND @todate</pre>
<p>There is a non-clustered index on CustomerID and another one on OrderDate.
Assume that the order activity among customers varies vividly. Many
customers make just a handful a orders per year. But some customers are more
active, and some real big guys may place several orders per day.</p>
<p>In the Northwind database, the most active customer is <small>SAVEA</small> with 31 orders,
whereas <small>CENTC</small> has only one order. Run the below:</p>
<pre>EXEC List_orders_12 'SAVEA', '19970811', '19970811'
go
sp_recompile List_orders_12
go
EXEC List_orders_12 'CENTC', '19960101', '19961231'</pre>
<p>That is, for <small>SAVEA</small> we only look at the orders for a single day, but for <small>CENTC</small>
we look at the orders for an entire year. As you may sense, these two
invocations are best served by different indexes. Here is the plan for the first invocation:</p>
<p class="image"><img alt="Plan for List_orders_11 'SAVEA'" src="Slow%20in%20the%20Application,%20Fast%20in%20SSMS_files/list_orders_11a.jpg" height="168" width="506"></p>
<p><small>SQL</small> Server here uses the index for OrderDate which is the most selective. The plan for the second invocation is different:</p>
<p class="image"><img alt="Plan for list_orders_11 'CENTC'" src="Slow%20in%20the%20Application,%20Fast%20in%20SSMS_files/list_orders_11b.jpg" height="174" width="512"></p>
<p>Here CustomerID is the most selective column, and <small>SQL</small> Server uses the index on CustomerID.</p>
<p>One solution to address this is to force recompilation every time with the
<small>RECOMPILE</small> query hint:</p>
<pre>CREATE PROCEDURE List_orders_12 @custid   nchar(5),
                                @fromdate datetime,
                                @todate   datetime AS
SELECT *
FROM   Orders
WHERE  CustomerID = @custid
  AND  OrderDate BETWEEN @fromdate AND @todate
<b>OPTION (RECOMPILE)</b></pre>
<p>With this hint, <small>SQL</small> Server will compile the query every time. Rather than
using the query hint, you can tell <small>SQL</small> Server to recompile the entire procedure
on each invocation:</p>
<pre>CREATE PROCEDURE List_orders_12 @custid   nchar(5),
                                @fromdate datetime,
                                @todate   datetime <b>WITH RECOMPILE </b>AS</pre>
<p>For the example at hand, it has no importance which you use, since the
procedure consists of a single statement. But for a long procedure with many statements
of which only one is problematic, <small>WITH RECOMPILE</small>
is clearly an inferior choice, due to the increase in compilation overhead. An
interesting effect of <small>WITH RECOMPILE</small>, though, is that the plan is never put into
the cache at all, whereas this happens when you use <small>OPTION (RECOMPILE)</small>. </p>
<p>In many cases, forcing recompilation every time is quite alright, but there are a few situations where it
is not:</p>
<ol>
  <li>The procedure is called with a very high frequency, and the compilation overhead
  hurts the system.</li>
  <li>The query is very complex and the compilation time has a noticeable
  negative impact on the response time.</li>
</ol>
<p>More to the point, while forcing recompilation is a solution that is
almost always feasible, it is not always the best solution. As a matter of fact,
the example we have looked at in this section is probably not very typical for
the situation "fast in <small>SSMS</small>, slow in the application". Because, if you have
varying usage pattern, you will be alerted of varying performance within the
application itself. So there is all reason to read on, and see if the situation
you are facing fits with the other examples I present.</p>
<h3><a name="dynsearch">Dynamic Search Conditions</a></h3>
<p>It is very common to have forms where users can select from a number of
search conditions. For instance they can select to see orders on a certain date,
by a certain customer, for a certain product etc, including combinations of the
parameters. Such procedures are sometimes implemented with a <small>WHERE</small> clauses that
goes:</p>
<pre>WHERE (CustomerID = @custid OR @custid IS NULL)
  AND (OrderDate = @orderdate OR @orderdate IS NULL)
  ...</pre>
<p>As you may imagine, parameter sniffing is not beneficial for such procedures.
I am not going to take up much space on this problem here, for two
reasons: 1) As I've already said, problem with such procedures usually manifests
itself with varying performance within the application. 2) I have a
separate article devoted to this topic, entitled <i><a href="http://www.sommarskog.se/dyn-search.html">Dynamic
Search Conditions in <small>T‑SQL</small></a></i>. </p>
<h3><a name="reviewindexing">Reviewing Indexing</a></h3>
<p>Some time ago, one of my clients contacted me because one of their customers
experienced a severe performance problem with a function in their system. My
client said that the same code ran well at other sites, and there had been no
change recently to the application. (But you know, clients always say that, it seems.) They had been able to isolate the
problematic procedure, which included a query which looked something like this:</p>
<pre>SELECT DISTINCT c.*
FROM   Table_C c
JOIN   Table_B b ON c.Col1 = b.Col2
JOIN   Table_A a ON a.Col4 = b.Col1
WHERE  a.Col1 = @p1
  AND  a.Col2 = @p2
  AND  a.Col3 = @p3  </pre>
<p>When executed from the application, the query took 10-15 minutes. When they
ran the procedure from <small>SSMS</small>, they found that response time was instant. That was then they called me.</p>
<p>An account was set up for me to make it possible to log in to the site in question. I found
that all three tables were of some size, at least a million rows in each. I looked
at the indexes for Table_A, and I found that it had some 7-8 indexes. Of interest
for this query was:</p>
<ul>
  <li>One non-clustered, non-unique index <b>Combo_ix</b> on (Col1, Col2, Col5, Col4) and
  maybe some more columns.</li>
  <li>One non-clustered, non-unique index <b>Col2_ix</b> on (Col2).</li>
  <li>One non-clustered, non-unique index <b>Col3_ix</b> on (Col3).</li>
</ul>
<p>Thus, where was no index that covered all conditions in the <small>WHERE</small> clause.</p>
<p>When I ran the procedure in <small>SSMS</small> with default settings, the optimizer choose
the first index to get data from Table_A. When I changed the setting of <small>ARITHABORT</small>
to <small>OFF</small> to match the application, I saw a plan that used the index on Col3. </p>
<p>At this point I ran</p>
<pre>DBCC SHOW_STATISTICS (Table_A, Col3_ix)</pre>
<p>The output looked like something like this:</p>

<table border="1" cellspacing="0">
<colgroup span="5"><col align="LEFT"><col align="RIGHT">
<col align="RIGHT"><col align="RIGHT"><col align="RIGHT">
</colgroup>
  <tbody><tr>
    <th><small>RANGE_HI_KEY</small></th>
    <th><small>RANGE_ROWS</small></th>
    <th><small>EQ_ROWS</small></th>
    <th><small>DISTINCT_RANGE_ROWS</small></th>
    <th><small>AVG_RANGE_ROWS</small></th>
  </tr>
  <tr>
    <td><small>APRICOT</small></td>
    <td>0</td>
    <td>17</td>
    <td>0</td>
    <td>1</td>
  </tr>
  <tr>
    <td><small>BANANA</small></td>
    <td>0</td>
    <td>123462</td>
    <td>0</td>
    <td>1</td>
  </tr>
  <tr>
    <td><small>BLUEBERRY</small></td>
    <td>0</td>
    <td>46</td>
    <td>0</td>
    <td>1</td>
  </tr>
  <tr>
    <td><small>CHERRY</small></td>
    <td>0</td>
    <td>92541</td>
    <td>0</td>
    <td>1</td>
  </tr>
  <tr>
    <td><small>FIG</small></td>
    <td>0</td>
    <td>1351</td>
    <td>0</td>
    <td>1</td>
  </tr>
  <tr>
    <td><small>KIWI</small></td>
    <td>0</td>
    <td>421121</td>
    <td>0</td>
    <td>1</td>
  </tr>
  <tr>
    <td><small>LEMON</small></td>
    <td>0</td>
    <td>6543</td>
    <td>0</td>
    <td>1</td>
  </tr>
  <tr>
    <td><small>LIME</small></td>
    <td>0</td>
    <td>122131</td>
    <td>0</td>
    <td>1</td>
  </tr>
  <tr>
    <td><small>MANGO</small></td>
    <td>0</td>
    <td>95824</td>
    <td>0</td>
    <td>1</td>
  </tr>
  <tr>
    <td><small>ORANGE</small></td>
    <td>0</td>
    <td>10410</td>
    <td>0</td>
    <td>1</td>
  </tr>
  <tr>
    <td><small>PEAR</small></td>
    <td>0</td>
    <td>46512</td>
    <td>0</td>
    <td>1</td>
  </tr>
  <tr>
    <td><small>PINEAPPLE</small></td>
    <td>0</td>
    <td>21102</td>
    <td>0</td>
    <td>1</td>
  </tr>
  <tr>
    <td><small>PLUM</small></td>
    <td>0</td>
    <td>13</td>
    <td>0</td>
    <td>1</td>
  </tr>
  <tr>
    <td><small>RASPBERRY</small></td>
    <td>0</td>
    <td>95</td>
    <td>0</td>
    <td>1</td>
  </tr>
  <tr>
    <td><small>RHUBARB</small></td>
    <td>0</td>
    <td>7416</td>
    <td>0</td>
    <td>1</td>
  </tr>
  <tr>
    <td><small>STRAWBERRY</small></td>
    <td>0</td>
    <td>24611</td>
    <td>0</td>
    <td>1</td>
  </tr>
</tbody></table>

<p>That is, there were only 17 distinct values for this column and with a very
uneven distribution among these values. I also confirmed this fact by
running the query:</p>
<pre>SELECT Col3, COUNT(*) FROM Table_A GROUP BY Col3 ORDER BY Col3</pre>
<p>I proceeded to look at the bad query plan to see what value that the optimizer
had sniffed
for @p3. I found that it was – <small>APPLE</small>, a value not present
in the table at all! That is, the first time the procedure was executed, <small>SQL</small>
Server had estimated that the query would return a single row (recall that it
never estimates zero rows), and that the index on Col3 would be the most
efficient index to find that single row.</p>
<p>Now, you may ask how it could be that unfortunate that the procedure was
first executed for <small>APPLE</small>? Was that just bad luck? Since I don't know this
system, I can't tell for sure, but apparently this had happened more than
once. I got the impression that this procedure was
executed many times as part of some bigger operation. Say that the operation would always start with <small>APPLE. </small>Remember that reindexing a table triggers recompilation, and it is
very common to run maintenance jobs at night to keep fragmentation in check.
That is, the first call in the morning can be very decisive for your
performance. (Why would the operation start with a value that is not in the
database? Who knows, but maybe <small>APPLE</small> is an unusual condition that needs to be
handled first if it exists. Or maybe it is just the alphabet.) </p>
<p>For this particular query, there is a whole slew of possible measures to
address the performance issue.</p>
<ol>
  <li><small>OPTION (RECOMPILE) / WITH RECOMPILE</small></li>
  <li>Add the "optimal" index on (Col1, Col2, Col3) <small>INCLUDE</small> (Col4).</li>
  <li>Make the index on Col3 filtered or drop it entirely.</li>
  <li>Use an index hint to force use of any of the other indexes.</li>
  <li>The query hint <small>OPTIMIZE FOR</small>. </li>
  <li>Copy @p3 to a local variable.</li>
  <li>Change the application behaviour.</li>
</ol>
<p>We have already looked at forcing recompilation, and while it most probably would have solved the problem, it is not likely that it would have been
the best solution. Below, I will look into the other options in more detail.</p>
<h4>Add a New Index</h4>
<p>My primary recommendation to the customer was the second on the list: add an index that
matches the query perfectly. That is, an index where all columns in the <small>WHERE</small>
clauses are index keys, with the least selective column Col3 last among the
keys. On top of that, add Col4 as an included column, so that the query can be
resolved from the new index alone, without any need to access the data pages.</p>
<p>However, adding a new index is not always a good solution. If you have a table
which is accessed in a multitude of ways, it may not be feasible to add indexes
that matches all <small>WHERE</small> and <small>JOIN</small> conditions, even less to add covering indexes
for each and every query. Particularly, this applies to tables with a heavy
update rate, like an Orders table. The more indexes you add to a table, the
higher the cost to insert, update and delete rows in the table. </p>
<h4>Change / Drop the Index on Col3</h4>
<p>How useful is the index on Col3 really? Since I don't know this system
it is difficult to tell. But generally, indexes on columns with only a small
set of values are not very selective and thus not that useful. So an option could be to drop
the
index on Col3 altogether and thereby save the optimiser from this trap. Maybe
the index was added at some point in time
by mistake, or it was added without understanding of how the database would look like after a
couple of years. (Having worked a little more with this client, it
seems that they add indexes on all FK columns a matter of routine. Which may or
may not be a good idea.)</p>
<p>It cannot be denied, that you have to be a very brave person to drop
an index entirely. You can consult <b>sys.dm_dbindex_usage_stats</b>
to see how much the index is used. Just keep in mind that this <small>DMV</small> is cleared
when <small>SQL</small> Server is restarted or the database is taken offline.</p>
<p>Since the distribution in Col3 is so uneven, it is not unlikely that there are queries that look for these rare values specifically. Maybe <small>FIG</small> is "New
unprocessed rows". Maybe <small>RASPBERRY</small> means "errors". In this case, it could be
beneficial to make Col3_ix a <i>filtered index</i>, a new feature added in <small>SQL</small> 2008. For instance, the index definition could read:</p>
<pre>CREATE INDEX col3_ix ON Table_A(col3) WHERE col3 IN ('FIG', 'RASPBERRY', 'APPLE', 'APRICOT')</pre>
<p>This has two benefits:</p>
<ol>
  <li>The size of the index is reduced with more than 99%.</li>
  <li>The index is no longer eligible for the problem query. Recall that <small>SQL</small>
  Server must select a plan which is correct for all input values, so even if
  the sniffed parameter value is <small>APPLE, SQL</small> Server cannot use the index, because
  the plan would yield incorrect result for <small>KIWI</small>.</li>
</ol>
<h4>Force a Different Index</h4>
<p>If you know that the index on Col1, Col2 will always be the best index, but
you don't want to add or drop any index, you can force the index with:</p>
<pre>SELECT c.*
FROM   Table_C c
JOIN   Table_B b ON c.Col1 = b.Ccol2
JOIN   Table_A a <b>WITH (INDEX = Combo_ix)</b> ON a.Col4 = b.Col1
WHERE  a.Col1 = @p1
  AND  a.Col2 = @p2
  AND  a.Col3 = @p3  </pre>
<p>You can even say:</p>
<pre>WITH (INDEX (combo_ix, col2_ix))</pre>
<p>to give the optimizer the choice between the two "good" indexes.</p>
<p>Index hints are very popular, too popular one might say. You should think
twice before you add an index hint, because tomorrow your data distribution may
be different, and another index would serve the query better. A second problem
is that if you later decide to rearrange the indexes, the query will fail with
an error because of the missing index.</p>
<h4>OPTIMIZE FOR</h4>
<p><small>OPTIMIZE FOR</small> is a query hint that permits you to control parameter sniffing.
For the example query, it could look like this:</p>
<pre>SELECT c.*
FROM   Table_C c
JOIN   Table_B b ON c.col1 = b.col2
JOIN   Table_A a ON a.col4 = b.col1
WHERE  a.col1 = @p1
  AND  a.col2 = @p2
  AND  a.col3 = @p3
<b>OPTION (OPTIMIZE FOR (@p3 = 'KIWI'))</b>  </pre>
<p>The hint tells <small>SQL</small> Server to ignore the input value, but instead compile the
query as if the input value of @p3 is <small>KIWI</small>, the most common value in
Col3. This will surely dissuade <small>SQL</small> Server from using the index.</p>
<p>A second option, available only in <small>SQL</small> 2008 and later, is:</p>
<pre>OPTION (OPTIMIZE FOR (@p3 UNKNOWN))</pre>
<p>Rather than hard-coding any particular value, we can tell <small>SQL</small> Server to make
a blind assumption to completely kill parameter sniffing for @p3. </p>
<p>It is worth adding that you can use <small>OPTIMIZE FOR</small> also with locally declared
variables, and not only with parameters.</p>
<h4>Copy @p3 to a Local Variable</h4>
<p>Rather than using @p3 directly in the query, you can copy it to a local
variable, and then use the variable in the query. This has the same effect as
<small>OPTIMIZE FOR UNKNOWN</small>. And it works on any version of <small>SQL</small> Server.</p>
<h4>Change the Application</h4>
<p>Yet an option is to change the processing in the application so that it starts
with the one of the more common values. It's not really a solution I recommend,
because it creates an extra dependency between the application and the database.
There is risk that the application two years later is rewritten to
accommodate new requirements that <small>PEACH</small> rows must be handled, and this handling
is added first... </p>
<p>Then again, there may be a general flaw in the process. Is the application
asking for values for one fruit at a time, when it should be getting values for
all fruits at once? I helped this client with another query. We had a
query-tuning session in a conference room, and I was never able to get really
good performance from the query we were working with. But towards the end of the day, the responsible
developer said that he knew how to continue, and his solution was to rearchitect
the entire process the problematic query was part of.</p>
<h4>Summing it Up</h4>
<p>As you have seen in this example, there are a lot of options to choose from – too
many, you may think. But performance tuning often requires you to have a
well-stuffed bag of tricks, because different problems call for different
solutions.</p>
<h3><a name="appcache">The Case of the Application Cache</a></h3>
<p>In
the system I work with most of my time, there is a kind of application cache that keeps data in a main-memory database, let's call it MemDb. It is used for
several purposes, but the main purpose is to serve as a cache from which the
customer's web server can retrieve data rather than querying the database.
Typically, in the morning there is a total refresh of the data. When data in a
database table is updated, there is a signalling mechanism which triggers MemDb to run a
stored procedure to get the delta. In order to
find what has changed, MemDb relies on timestamp columns. (A timestamp column
holds an 8-byte value that is unique to the database and which grows
monotonically. It is
automatically updated when the row is inserted or updated.) A typical stored procedure
to retrieve changes looks like this:</p>
<pre> CREATE PROCEDURE memdb_get_updated_customers @tstamp timestamp AS
   SELECT CustomerID, CustomerName, Address, ..., tstamp
   FROM   Customers
   WHERE  tstamp &gt; @tstamp</pre>
<p>When MemDb calls these procedures, it passes the highest value in the tstamp
column for the table in question from the previous call. When the procedure is invoked for a refresh, the main-memory database passes 0x as the parameter to get
all rows.</p>
<p class="note"><b>Side note</b>: The actual scheme is more complicated than the
above; I have simplified it to focus on what is important for this article. If you are
considering something similar,
be warned that as shown, this example has lots of issues with concurrent updates,
not the least if you use any form of snapshot isolation. <small>SQL</small> 2008 introduced Change Tracking which is a more solid
solution, particularly designed for this purpose. I also like to add that MemDb was not my idea, nor was I involved in the design of it.</p>
<p>At one occasion when I monitored the performance for a customer that just
had went live with our system, I noticed that the MemDb procedures had quite
long execution times. Since they run very often to read deltas, they have to
be very quick. After all, one idea with MemDb is to take load off from the
database – not add to it.</p>
<p>For a query like the above to be fast there has to be an index on tstamp,
but will this index be used? From what I said above, the first thing in the
morning, MemDb would run:</p>
<pre>EXEC memdb_get_updated_customers 0x</pre>
<p>Then a little later, it would run something like:</p>
<pre>EXEC memdb_get_updated_customers 0x000000000003E806</pre>
<p>It is not uncommon that during the night that query plans fall out of
the cache because of nightly batches that consume a lot of memory. Or there is a
maintenance job to rebuild indexes which triggers recompiles. So typically,
when the morning refresh runs, there is not any plan in the cache, and the
value sniffed is 0x. Given this value, will the optimizer use the index on
tstamp? Yes, if it is the clustered index. But since a timestamp column is
updated every time a row is updated, it is not a very good choice for the
clustered index, and all our indexes on timestamp columns are non-clustered. (And
for tables with a high update frequency, also a non-clustered index on a
timestamp column may be questionable.) Thus, since the optimizer sees that the
parameter indicates that all rows in the table will be retrieved, it settles for
a table scan. This plan is put into cache, and subsequent calls also scan the table,
even if they are only looking for the most recent rows. And it was this poor
performance that I saw.</p>
<p>When you have a situation like this, there are, just like in the previous
example, several ways to skin the cat. But before we look at the possibilities,
we need to make one important observation: there is no query plan that is good
for both cases. We want to use the index to read the deltas, but when making the
refresh we want the scan. Thus, only solutions that can yield different plans
for the two cases need to apply.</p>
<h4>OPTION (RECOMPILE)</h4>
<p>While this solution meets the requirement, it is not a good solution. This
means that every time we retrieve a delta, there is compilation. And while the
above procedure is simple, some of the MemDb procedures are decently complex with
a non-trivial cost for compilation.</p>
<h4>EXECUTE WITH RECOMPILE</h4>
<p>Rather than requesting recompilation inside the procedure, it is better to do
it in the special case: the refresh. The refresh only happens once a day normally. And furthermore, the refresh is fairly costly in itself, so the cost of
recompilation is marginal in this case. That is, when MemDb wants to make a refresh, it should call the procedure in this way:</p>
<pre>EXECUTE memdb_get_updated_customers WITH RECOMPILE</pre>
<p>As I noted previously, it is likely that there is no plan in the cache early
in the morning, so you may ask what's the point? The point is that when you use <small>WITH
RECOMPILE</small> with <small>EXECUTE</small>, the plan is <i>not</i> put into cache. Thus, the refresh can
run with the scan, but the plan in the cache will be built from the first delta
retrieval. (And if the plan for reading the delta still is in the cache, that plan will remain in the cache.)</p>
<p>For the particular problem in our system, this was the solution I mandated. An extra advantage for us was that there was a single code path in MemDb that
had to be changed.</p>
<p>There is however a slight problem with this solution. Normally, when you call
a stored procedure from a client, you use a special command type where you
specify only the name of the procedure. (For instance, <b>
CommandType.StoredProcedure</b> in <small>ADO .NET</small>.) The client <small>API</small> then makes an <small>RPC</small>
(remote procedure call) to <small>SQL</small> Server, and there is never an <small>EXECUTE</small> statement as such.
Very few client APIs seem to expose any method to specify
<small>WITH RECOMPILE</small> when you call a procedure through <small>RPC</small>. The workaround is to send
the full <small>EXECUTE</small> command, using <b>CommandType.Text</b> or similar, for
instance:</p>
<pre>cmd.CommandType = CommandType.Text;
cmd.Text = "EXECUTE memdb_get_updated_customers @tstamp WITH RECOMPILE";</pre>
<p class="note"><b>Note:</b> You should pass the parameters through the Parameters collection or
corresponding mechanism in your <small>API</small>; don't inline the values in the string with
the <small>EXEC</small> command, as this could open for
<a href="http://www.sommarskog.se/dynamic_sql.html#SQL_injection"><small>SQL</small> injection</a>.</p>
<h4>Using a Wrapper Procedure</h4>
<p>If you have a situation where you realise that <small>EXECUTE WITH RECOMPILE</small> is the
best solution, but it is not feasible to change the client, you can introduce a
wrapper procedure. In our example the original procedure would be renamed to
memdb_get_updated_customers_inner, and then you would write a wrapper that
goes:</p>
<pre>CREATE PROCEDURE memdb_get_updated_customers @tstamp timestamp AS
   IF @tstamp = 0x
      EXECUTE memdb_get_updated_customers_inner @tstamp WITH RECOMPILE
   ELSE
      EXECUTE memdb_get_updated_customers_inner @tstamp</pre>
<p>In many cases this can be a plain and simple solution, particularly if you have a small number of such procedures. (We have many.)</p>
<h4>Different Code Paths</h4>
<p>Another approach would be to have different code paths for the two cases:</p>
<pre>CREATE PROCEDURE memdb_get_updated_customers @tstamp timestamp AS
   IF @tstamp = 0x
   BEGIN
      SELECT CustomerID, CustomerName, Address, ..., tstamp
      FROM   Customers
   END
   ELSE
   BEGIN
      SELECT CustomerID, CustomerName, Address, ..., tstamp
      FROM   Customers WITH (INDEX = timestamp_ix)
      WHERE  tstamp &gt; @tstamp
   END</pre>
<p>Note that it is important to force the index in the <small>ELSE</small> branch, or else this branch will scan the table if the first call to the procedure is for @tstamp =
0x because of parameter sniffing. If your procedure is more complex and includes joins to other tables, it is
not likely that this strategy will work out, even if you force the index on
tstamp. The estimates for the joins would be grossly incorrect and the query plans would still be optimized to get all data, and not the delta.</p>
<h4>Different Procedures</h4>
<p>In the situation we had, there was one procedure that was particularly
difficult. It retrieved account transactions (this is a system for stock
trading). The refresh would not retrieve all transactions in the database, but
only from the N most recent days, where N was a system parameter read from the
database. In this database N was 20. The procedure did not read from a single
table, but there was umpteen other tables in the query. Rather than having a
timestamp parameter, the parameter was an id. This works since transactions are never
updated, so MemDb only needs to look for new transactions. </p>
<p>I found that in this case <small>EXECUTE WITH RECOMPILE</small> alone would not save the
show, because there were several other problems in the procedure. I found no
choice but to have two procedures, one for the refresh and one for the delta. I replaced the
original procedure with a wrapper:</p>
<pre>CREATE PROCEDURE memdb_get_transactions @transid int AS
   IF coalesce(@transid, 0) = 0
      EXECUTE memdb_get_transactions_refresh
   ELSE
   BEGIN
      DECLARE @maxtransid int
      SELECT @maxtransid = MAX(transid) FROM transactions
      EXECUTE memdb_get_transactions_delta @transid, @maxtransid
   END</pre>
<p>I had to add @maxtransid as a parameter to the delta procedure, because with an open condition like <code>WHERE
transid &gt; @transid</code>, <small>SQL</small> Server would tend to
miss-estimate the number of rows it had to read. Making the interval
closed addressed that issue, and by passing @maxtransid as a parameter, <small>SQL</small>
Server could sniff the value.</p>
<p>From a maintainability perspective, this is by means not a pleasant step to
take, not the least when the logic is as complicated as in this case. If you have
to do this, it is
important that you litter the code with comments to tell people that if they
change one procedure, they need to change the other one as well.</p>
<p class="noteh3"><b>Note</b>: a few years later, a colleague rewrote
memdb_get_transactions_refresh to become a second wrapper. As I mentioned, it
reads some system parameters to determine which transactions to read. To get
the refresh to perform decently, he found that he had to pass the values of these system
parameters, as well as the interval of transactions ids to read as parameters to
an inner procedure to get full benefit of parameter sniffing. (We were still on <small>SQL</small> 2000 at the time. In <small>SQL</small> 2005 or later <small>OPTION (RECOMPILE)</small> would have
achieved the same thing.)</p>
<h3><a name="badsql">Fixing Bad SQL</a></h3>
<p>There may also be situations where the root cause to the problem is simply
poorly written <small>SQL</small>. As just one example, consider this query:</p>
<pre>SELECT ...
FROM   Orders
WHERE  (CustomerID = @custid OR @custid IS NULL)
  AND  (EmployeeID = @empid  OR @empid IS NULL)
  AND  convert(varchar, OrderDate, 101) = convert(varchar, @orderdate, 101)</pre>
<p>The idea is that users here are able to search orders for a given date, and
optionally they can also specify other search parameters. The programmer here
consdiders
that OrderDate may include a time portion, and factors it out by using a format
code to <b>convert</b>() that produces a string without time, and to be really sure he
performs the same operation on the input parameter. (In Northwind, OrderDate is
always without a time portion, but for the sake of the example, I'm assuming
that this is not the case.)</p>
<p>For this query, the index on OrderDate would be the obvious choice, but the
way the query is written, <small>SQL</small> Server cannot seek that index, because OrderDate is
entangled in an expression. This is sometimes referred to that the expression is
not <i>sargable</i>, where <i>sarg</i> is short for <i>search argument</i>, that is,
something that can be used as a seek predicate in a query. (Personally, I don't think
the term "sargable" is a very useful one, but since you see people drop it from
time to time, I figured that I should mention this piece of jargon.)</p>
<p>Since the index on OrderDate has been disqualified, the optimizer may settle
for any of the other indexes, depending on the input for the first parameter,
causing poor performance for other types of searches. The remedy is to rewrite
the query, for instance like:</p>
<pre>SELECT ...
FROM   Orders
WHERE  (CustomerID = @custid OR @custid IS NULL)
  AND  (EmployeeID = @empid  OR @empid IS NULL)
  AND  OrderDate &gt;= @orderdate
  AND  OrderDate &lt; dateadd(DAY, 1, @orderdate)</pre>
<p>(It seems reasonable to assume that an input parameter which is supposed to be a date
does not have any time portion, so there is little reason to litter the code
with any extra conversion.)</p>
<p>Poorly written <small>SQL</small>, often manifests itself in general performance problems –
that is, the query always runs slow – and maybe not so often in situations where
parameter sniffing matters. Nevertheless, when you battle your parameter-sniffing problem, there is all reason to investigate whether the query could be
written in a way that avoids the dependency of the input parameter for a good
plan. There are many ways to write bad <small>SQL</small>, and this is not the place the list
them all. Hiding columns in an expression is just one example, albeit a common one. Sometimes the expression is hidden because it is due to implicit
conversion. Examine the plan, and when an index is not used as you would expect, go back and look at the query again.</p>
<h2><a name="compiledynsql">How SQL Server Compiles Dynamic SQL</a></h2>
<p>We leave the topic of parameter sniffing and to go back to the base topic for
this article: why a query may run slow in the application, yet fast in <small>SQL</small>
Server Management Studio. Hitherto we have only looked at stored procedures, and
with stored procedures the reason is most often due to different settings for
<small>SET ARITHABORT</small>. If you have an application that does not use stored procedures,
but generates the queries in the client or the middle layer, there are a few more
reasons why you may get a new cache entry when you run the query in <small>SSMS</small> and
maybe also a different plan than in the application.</p>
<h3><a name="whatisdynsql">What Is Dynamic SQL?</a></h3>
<p>Dynamic <small>SQL</small> is any <small>SQL</small> which not part of a stored procedure (or any other
type of module). This includes:</p>
<ul>
  <li><small>SQL</small> statements executed with <small><span class="nowrap">EXEC()</span></small> and <b>sp_executesql</b>.</li>
  <li><small>SQL</small> statements sent directly from the client.</li>
  <li><small>SQL</small> statements submitted from modules written in the <small>SQLCLR</small>.</li>
</ul>
<p>Dynamic <small>SQL</small> comes in two flavours, unparameterised and parameterised. In
unparameterised <small>SQL</small>, the programmer composes the <small>SQL</small> string by concatenating the
language elements with the parameter values. For instance, in <small>T‑SQL</small>:</p>
<pre>SELECT @sql = 'SELECT mycol FROM tbl WHERE keycol = ' + convert(varchar, @value)
EXEC(@sql)</pre>
<p>Or in C#:</p>
<pre>cmd.CommandText = "SELECT mycol FROM tbl WHERE keycol = " + value.ToString();</pre>
<p>Unparameterised <small>SQL</small> is <b>bad</b> for several reasons, please see my article
<i><a href="http://www.sommarskog.se/dynamic_sql.html">The Curse and Blessings of Dynamic <small>SQL</small></a></i> for
a discussion on why.</p>
<p>In parameterised <small>SQL</small>, you pass parameters much like in a stored procedure. In
<small>T‑SQL</small>:</p>
<pre>EXEC sp_executesql N'SELECT mycol FROM dbo.tbl WHERE keycol = @value',
                   N'@value int', @value = @value</pre>
<p>Or in C#:</p>
<pre>cmd.CommandText = "SELECT mycol FROM dbo.tbl WHERE keycol = @value";
cmd.Parameters.Add("@value", SqlDbType.Int);
cmd.Parameters["@value"].Value = value;</pre>
<p>The C# code results in a call to <b>sp_executesql</b> which looks exactly
like the <small>T‑SQL</small> example above. </p>
<p>For more details on <b>sp_executesql</b>, pleases see my article <i>
<a href="http://www.sommarskog.se/dynamic_sql.html">The Curse and Blessings of Dynamic <small>SQL</small></a></i>. </p>
<h3><a name="genplandynsql">Generating the Plan for Dynamic SQL</a></h3>
<p><small>SQL</small> Server compiles dynamic <small>SQL</small> very much in the same way as stored
procedures. That is, if the batch consists of more than one query, the entire batch
is nevertheless compiled as a whole, and <small>SQL</small> Server has no knowledge of the values
of local variables in the batch. As with stored procedures, <small>SQL</small>
Server sniffs the values of parameters, and uses these values when
building the query plan.</p>
<p>There is however one initial step in compilation which is unique dynamic <small>SQL</small>,
to wit parameterisation. That is, <small>SQL</small> Server may replace a constant in the query
with a parameter. For example, if you have:</p>
<pre>SELECT * FROM Orders WHERE OrderID = 11000</pre>
<p><small>SQL</small> Server will compile this as you had submitted</p>
<pre>EXEC sp_executesql N'SELECT * FROM Orders WHERE OrderID = @p1', N'@p1 int', @p1 = 11000</pre>
<p>There are two modes of parameterisation, simple and forced. With simple
parameterisation, <small>SQL</small> Server only parameterises a fairly narrow class of simple
queries. With forced parameterisation, <small>SQL</small> Server replaces about all constants
with parameters, with some exceptions, as detailed in
<a href="http://msdn.microsoft.com/en-us/library/ms175037.aspx" target="_blank">this topic</a>
in Books Online. The default mode is simple parameterisation, and you set the
mode per database with <small>ALTER DATABASE</small>.</p>
<p>Forced parameterisation can be a big performance saver for an application
that does not used parameterised statements, but there is little reason to use
it with well-written applications.</p>
<h3><a name="cachedynsql">Dynamic SQL and the Plan Cache</a></h3>
<p>Query plans for dynamic <small>SQL</small> are put into the plan cache, just like plans for
stored procedures. (If you hear someone telling you something else, that person
is either just confused or relying on very old information. Up to <small>SQL</small> Server 6.5, <small>SQL</small> Server did
not cache plans for dynamic <small>SQL</small>.) As with stored procedures, plans
for dynamic <small>SQL</small> may be flushed from the
cache for a number of reasons, and individual statements may be recompiled.
Furthermore, there may be more than one plan for the same query text
because of difference in <small>SET</small> options.</p>
<p>However, there are two complications with dynamic <small>SQL</small> which do not apply to
stored procedures.</p>
<h4>The Query Text as Hash Key</h4>
<p>When <small>SQL</small> Server looks up a stored procedure in the cache, it uses the name of
the procedure. But that is not possible with dynamic <small>SQL</small>, as there is no
name. Instead, <small>SQL</small> Server computes a hash from the query text – including any
parameter list – and uses this hash as a key in the plan cache. And here is
something very important: this hash value is computed without any normalisation
whatsoever of the batch text. Comments are not stripped. White space is not
trimmed or collapsed. Case is not forced to upper or lowercase, even if
the database has a case-insensitive collation. The hash is computed from the
text exactly as submitted, and any small difference will yield a different hash
and a different cache entry.</p>
<p>Run this with Include Actual Execution Plan enabled:</p>
<pre>EXEC sp_executesql N'SELECT * FROM Orders WHERE OrderDate &gt; @orderdate',
                   N'@orderdate datetime', '20000101'
EXEC sp_executesql N'SELECT * FROM Orders WHERE OrderDate &gt; @orderdate',
                   N'@orderdate datetime', '19980101'
EXEC sp_executesql N'select * from Orders where OrderDate &gt; @orderdate',
                   N'@orderdate datetime', '19980101'</pre>
<p>You will find that the first two calls use the same plan, Index Seek + Key
Lookup, whereas the third query uses a Clustered Index Scan. That is, the second
call reuses the plan created for the first call. But in the third call, the <small>SQL</small>
keywords are in lowercase, why there is no cache hit, and a new plan is created. Just to
enforce this fact, here is a second example with the same result. </p>
<pre>DBCC FREEPROCCACHE
go
EXEC sp_executesql N'SELECT * FROM Orders WHERE OrderDate &gt; @orderdate',
                   N'@orderdate datetime', '20000101'
EXEC sp_executesql N'SELECT * FROM Orders WHERE OrderDate &gt; @orderdate',
                   N'@orderdate datetime', '19980101'
EXEC sp_executesql N'SELECT * FROM Orders WHERE OrderDate &gt; @orderdate ',
                   N'@orderdate datetime', '19980101'</pre>
<p>The difference is that there is a single trailing space in the third statement.</p>
<h4>The Significance of the Default Schema</h4>
<p>The other difference to stored procedures is less obvious, and is best shown
with an example. Run this and look at the execution plans:</p>
<pre>DBCC FREEPROCCACHE
go
CREATE SCHEMA Schema2
go
CREATE USER User1 WITHOUT LOGIN WITH DEFAULT_SCHEMA = dbo
CREATE USER User2 WITHOUT LOGIN WITH DEFAULT_SCHEMA = Schema2
GRANT SELECT ON Orders TO User1, User2
GRANT SHOWPLAN TO User1, User2
go
EXEC sp_executesql N'SELECT * FROM Orders WHERE OrderDate &gt; @orderdate',
                   N'@orderdate datetime', '20000101'
go
EXECUTE AS USER = 'User1'
EXEC sp_executesql N'SELECT * FROM Orders WHERE OrderDate &gt; @orderdate',
                   N'@orderdate datetime', '19980101'
REVERT
go
EXECUTE AS USER = 'User2'
EXEC sp_executesql N'SELECT * FROM Orders WHERE OrderDate &gt; @orderdate',
                   N'@orderdate datetime', '19980101'
REVERT
go
DROP USER User1
DROP USER User2
DROP SCHEMA Schema2
go</pre>
<p>The first two executions use Index Seek + Key Lookup, whereas the third uses
Clustered Index Scan, despite the query and the parameter being identical to the
second query. What is going on?</p>
<p>First something about the setup. The script creates two database users and
grants them the permissions to run the queries. We run the query three times.
First as ourselves (presumably we are <b>dbo</b>), and then we impersonate the two
newly created users. (If you are not acquainted with impersonation, look up the topic for
<a href="http://msdn.microsoft.com/en-us/library/ms181362.aspx" target="_blank"><small>EXECUTE AS</small></a>
in Books Online; I also cover it in my article <i><a href="http://www.sommarskog.se/grantperm.html">
Granting Permissions through Stored Procedures</a></i>.) The users are
login-less, but that is only because we don't need any logins for this
example. What is important is
that they have different default schemas. User1 has dbo as its default schema,
but not User2. Why does this matter?</p>
<p>Keep in mind that when <small>SQL</small> Server looks up an object, it first looks in the
default schema of the user, and if the object is not found, it looks in
the dbo schema. For dbo and User1, the query is unambiguous, since dbo
is their default schema and this is the schema for the Orders table. But
for User2 this is different. Currently there is only dbo.Orders, but what if Schema2.Orders is added later?
Per the rules, User2 should now get data from that table and not from dbo.Orders. But
if User2 would use the same cache entry as dbo
and User1, that would not happen. Therefore, User2
needs a cache entry of its own. If Schema2.Orders is added, that cache entry can
be invalidated without affecting other users.</p>
<p>We can see this is the plan attributes. Here is a variation of the query we
ran for stored procedures:</p>
<pre>SELECT qs.plan_handle, a.attrlist
FROM   sys.dm_exec_query_stats qs
CROSS  APPLY sys.dm_exec_sql_text(qs.sql_handle) est
CROSS  APPLY (SELECT epa.attribute + '=' + convert(nvarchar(127), epa.value) + '   '
              FROM   sys.dm_exec_plan_attributes(qs.plan_handle) epa
              WHERE  epa.is_cache_key = 1
              ORDER  BY epa.attribute
              FOR    XML PATH('')) AS a(attrlist)
WHERE  est.text LIKE '%WHERE OrderDate &gt; @orderdate%'
  AND  est.text NOT LIKE '%sys.dm_exec_plan_attributes%'</pre>
<p>There are three differences to the query for stored procedures:</p>
<ol>
  <li>There is no condition on <b>db_id</b>(), since this column is only populated for
  sql_handles from stored procedures.</li>
  <li>As there is no procedure name to match on, we have to use part of the
  query text.</li>
  <li>We need an extra condition to filter out the query against <b>
  sys.dm_exec_plan_attributes</b> itself.</li>
</ol>
<p>When I ran this query, I saw this (partial) list of attributes:</p>
<pre>date_first=7   date_format=1   dbid=6   objectid=158662399   set_options=251   user_id=5
date_first=7   date_format=1   dbid=6   objectid=158662399   set_options=251   user_id=1   </pre>
<p>First look at <b>objectid</b>. As you see, this value is identical for the
two entries. This object id is the hash value that I described above. Next, look at the attribute which is
distinctive: <b>user_id</b>. The name as such is a misnomer; the value is the
default schema for the users using this plan. The dbo schema always has schema_id = 1. In my
Northwind database, Schema2 got schema_id = 5 when I ran the query, but you may
see a different value.</p>
<p>Now run this query:</p>
<pre>EXEC sp_executesql N'SELECT * FROM <b>dbo.</b>Orders WHERE OrderDate &gt; @orderdate',
                   N'@orderdate datetime', '20000101'</pre>
<p>And then run the query against <b>sys.dm_exec_plan_attributes</b> again.
A third row in the output appears:</p>
<pre>date_first=7   date_format=1   dbid=6   objectid=549443125   set_options=251   user_id=-2   </pre>
<p><b>objectid</b> is different from above, since the query text is different.
<b>user_id</b> is now
-2. What does this mean? If you look closer at the query, you see now that we
now specify the schema explicitly when we access the Orders table. That is, the query is
now unambiguous, and all users can use this cache entry. And that is
exactly what <b>user_id</b> = -2 means: there are no ambiguous object references in the
query. The corollary of this is that it is very much best practice to always use
two-part notation in dynamic <small>SQL</small>, no matter whether you create the dynamic <small>SQL</small>
in a client program or in a stored procedure.</p>
<p>You may think that "we don't use schemas in our app, so this is not an issue
for us", but not so fast! When you use <small>CREATE USER</small>, the default schema for the
new user is indeed <b>dbo</b>, unless you specify something else. However, if the <small>DBA</small>
is of the old school, he may create users with any of the legacy procedures <b>
sp_adduser</b> and <b>sp_grantdbaccess</b>, and they work differently. They not
only create a user, but they also create a schema with the same name as the
user and set this schema as the default schema for the newly created user and
make this use the owner of the schema. Does it sound corny? Yes, but up to <small>SQL</small> 2000, schema and users
were unified in <small>SQL</small> Server. Since you may not have control over how users are
created, you should not rely on that users have dbo as their default schema.</p>
<p>Finally, you may wonder why this issue does not apply to the caching of plans for
stored procedures. The answer is that in a stored procedure, the name resolution
is always performed from the procedure owner, not the current user. That is, in
a procedure owned by dbo, <i>Orders</i> can only refer to dbo.Orders never to
any Orders table in some other schema. (But keep in mind that this applies only
to the direct query text of the stored procedure. It does not apply to dynamic
<small>SQL</small> invoked with <small><span class="nowrap">EXEC()</span></small> or sp_executesql inside the procedure.)</p>
<h3><a name="runqueriesssms">Running Application Queries in SSMS</a></h3>
<p>As you understand from the previous section, there are a few more pitfalls
when you want to troubleshoot an application query from <small>SSMS</small> which may cause you
to get a different cache entry, and potentially a different query plan.</p>
<p>As with stored procedures, you need to keep <small>ARITHABORT</small> and other <small>SET</small> options
in mind. But you also need to make sure that you have exactly the same query
text, and that your default schema agrees with the user running the application.</p>
<p>The latter is the easiest to deal with. In most cases this will do:</p>
<pre>EXECUTE AS USER = 'appuser'
go
-- Run SQL here
go
REVERT</pre>
<p><i>appuser</i> is here the database user that the application uses – either a private user for the application, or the user name for the actual person using
the database. However, this fails if the query access resources outside the current database. In this case you can use <small>EXECUTE AS LOGIN</small> instead.
Note that this requires a server-level permission.</p>
<p>Retrieving the exact <small>SQL</small> text can be more difficult. The best is use a trace to capture the <small>SQL</small>; you can run the trace either in Profiler or
as a
server-side trace. If the <small>SQL</small> statement is unparameterised, you need to be careful that you copy the full text, and then select exactly that text in <small>SSMS</small>. That
is, do not drop or add any leading or trailing blanks. Don't add line breaks to
make the query more readable, and don't delete any comments. Keep it exactly as
the application runs it. You can use the query against <b>sys.dm_exec_plan_attributes</b> in this article to verify that you did
not add a second entry to the cache. </p>
<p>Another alternative is to get the text from <b>sys.dm_exec_query_stats</b>
and <b>sys.dm_exec_sql_text</b>. Here is a query you can use:</p>
<pre>SELECT '&lt;' + est.text + '&gt;'
FROM   sys.dm_exec_query_stats qs
CROSS  APPLY sys.dm_exec_sql_text(qs.sql_handle) est
WHERE  est.text LIKE '%some significant SQL text here%'</pre>
<p>It is important that you run the query in <i>text</i> mode. In grid mode, <small>SSMS</small>
will replace line breaks with space. The angle brackets in the output is there
as delimiters, so that you can select the exact query text including any leading
or trailing blanks.</p>
<p>Parameterised <small>SQL</small> is easier to deal with, since the <small>SQL</small> statement is packaged in a character literal. That is, if you see
this in Profiler</p>
<pre>EXEC sp_executesql N'SELECT * FROM Orders WHERE OrderDate &gt; @orderdate',
                   N'@orderdate datetime', '20000101'</pre>
<p>It's no issue if you happen to change it to, say:</p>
<pre>EXEC sp_executesql
N'SELECT * FROM Orders WHERE OrderDate &gt; @orderdate',
N'@orderdate datetime', '20000101'</pre>
<p>What is important is that you must not change anything of what is between the quotes of the two first parameters to sp_executesql, since this is what
the hash is computed from.</p>
<p>If you don't have any server-level permission – <small>ALTER TRACE</small> to run a trace, or <small>VIEW SERVER STATE</small> to query <b>sys.dm_exec_query_stats</b> and <b>
sys.dm_exec_sql_text</b> – it's starting to get difficult. If the dynamic <small>SQL</small> is produced in a stored procedure which you can edit, you can add a <small>PRINT</small> statement to get the text. (Actually, stored procedures
that produce dynamic <small>SQL</small> should
always have a @debug parameter and include the line: <code>IF @debug = 1 PRINT @sql</code>.)
You still have to be careful to get the exact text, and not add or drop any spaces. If there is a parameter list, you need to make sure
that you
copy the parameter
list exactly as well. If the <small>SQL</small> is produced by the application or by a .Net stored procedure, getting the <small>SQL</small> statement may be possible if you can run the application in a
debugger, but the exact text of the parameter list may be difficult to get at. The best bet may be try application against a database on a <small>SQL</small>
Server instance where you have all the required permissions, for instance on
your local workstation and get the query text this way.</p>
<h3><a name="fixesdynsql">Addressing Parameter-sniffing Problems in Dynamic SQL</a></h3>
<p>The methods we looked at previously are applicable to dynamic <small>SQL</small> as well. There are few more things to make a note of, though.</p>
<h4>The Effects of Auto-parameterisation</h4>
<p>You can tell that a predicate has been parameterised by looking at the
operator properties. For instance, for this query</p>
<pre>SELECT * FROM Orders WHERE OrderID = 11000</pre>
<p>The Seek Predicate is:</p>
<pre>Seek Keys[1]: Prefix: [Northwind].[dbo].[Orders].OrderID =
Scalar Operator(CONVERT_IMPLICIT(int,[@1],0))</pre>
<p><code>[@1]</code> reveals that the query has been auto-parameterised.</p>
<p>Occasionally, the fact that <small>SQL</small> Server auto-parameterises a query, works against you.
Say that you have a query that goes:</p>
<pre>SELECT ... FROM dbo.Orders WHERE Status = 'Delayed'</pre>
<p>There is a filtered index on Status, which includes the value <i>Delayed</i>,
because Orders are generally not delayed. (There is no Status column in
Northwind; I had to make one up for the example.) But if <small>SQL</small> Server decides to
parameterise the query, the index cannot be used, because the plan must
accommodate for all possible values of the parameter.</p>
<p>There is not really any foolproof way to turn off any of the modes of
parameterisation, but there are some hacks. If the database is in simple
parameterisation, parameterisation only happens with very simple queries, for
instance only for one-table queries. The full list of conditions is not included
in Books Online, but one of the white papers listed in the section<i>
<a href="#furtherreading">Further Reading</a></i>
includes a list. According to that list, it seems to sufficient to add a
condition like:</p>
<pre>AND 1 = 1</pre>
<p>to stop simple parameterisation from happening. But the anxious person asks
how we can trust that this does not change in a future release.</p>
<p>If the database is in forced parameterisation, there are two alternatives.
There is a list in Books Online in the
<a href="http://msdn.microsoft.com/en-us/library/ms175037.aspx" target="_blank">topic for forced
parameterisation</a> which specifies when forced parameterisation is not
employed. One solution according to the list is to use <small>OPTION (RECOMPILE)</small>, which
is OK as long as you can take the cost of compilation every time. Another
solution is to add a variable to the query:</p>
<pre>DECLARE @x int
SELECT ... FROM dbo.Orders WHERE Status = 'Delayed' AND @x IS NULL</pre>
<p>Again, you may ask what could happen in a future version of <small>SQL</small> Server. It's
certainly an ugly hack.</p>
<p class="noteh3"><b>Note</b>: If you read the Query Hints topic in Books
Online, you will find that there is a hint to control parameterisation, but this
hint is only permitted in plan guides, which is the topic for the next section.</p>
<h3><a name="planguides">Plan Guides and Plan Freezing</a></h3>
<p>Sometimes you may find that you want to modify a query to resolve a
performance issue by adding a hint of some sort. For a stored procedure, it is
not unlikely that you can edit the procedure and deploy the change fairly
immediately. But if the query is generated inside an application, it may be more
difficult. The entire executable has to be built and maybe be deployed to
all user machines. It is also likely that there is a mandatory QA step involved.
And if the application is a third-party application, changing it is out of the
question.</p>
<p>However, <small>SQL</small> Server provides a solution for these situations, to wit <i>plan guides</i>. There are two ways to set up plan guides, a general way, and a shortcut which is also known as <i>plan freezing</i>. General plan guides were introduced in <small>SQL</small> 2005, whereas plan freezing was added in <small>SQL</small> 2008.</p>
<p class="note"><b>Note</b>: this feature is not available on the low-end editions of <small>SQL</small> Server –
  Express, Web and Workgroup Edition. </p>
<p>Here is an example of a setting up plan guide. This particular example runs on <small>SQL</small> 2008 only:</p>
<pre>DBCC FREEPROCCACHE
go
EXEC sp_executesql N'SELECT * FROM dbo.Orders WHERE OrderDate &gt; @orderdate',
                   N'@orderdate datetime', @orderdate = '19960101'
go
EXEC sp_create_plan_guide
     @name = N'MyGuide',
     @stmt = N'SELECT * FROM dbo.Orders WHERE OrderDate &gt; @orderdate',
     @type = N'SQL',
     @module_or_batch = NULL,
     @params = N'@orderdate datetime',
     @hints =  N'OPTION (TABLE HINT (dbo.Orders , INDEX (OrderDate)))'
go
EXEC sp_executesql N'SELECT * FROM dbo.Orders WHERE OrderDate &gt; @orderdate',
                   N'@orderdate datetime', @orderdate = '19980101'
go
EXEC sp_control_plan_guide N'DROP', N'MyGuide'</pre>
<p>In this example, I create a plan to ensure that a query against OrderDate always uses an Index Seek (maybe because I expect all queries to be for the last
few days or so). I specify a name for the guide. Next, I specify the exact statement for which the guide applies. As when you run a query in <small>SSMS</small> to investigate
it, you need to make sure that you do not add or lose any leading or trailing space,
nor must you make any other alteration. The parameter <b>@type</b> specifies that the guide is for
dynamic <small>SQL</small> and not for a stored procedure. Had the <small>SELECT</small> statement been part of a larger batch, I would have to specify the text of that batch
in the parameter <b>@module_or_batch</b>, again exactly as submitted from the
application. When I specify <small>NULL</small> for <b>@module_or_batch</b>, <b>@stmt</b> is assumed to be
the entire batch text. <b>@params</b> is the parameter list for the batch, and again
there must be an exact match character-by-character with what the application
submits. </p>
<p>Finally, <b>@hints</b> is where the fun is. In this example I specify that the query should always use the index on OrderDate, no matter the sniffed value for
@orderdate. This particular query hint, <small>OPTION (TABLE HINT)</small> is not available in
<small>SQL</small> 2005, and that is why the example does not run on that version.</p>
<p>In the script, the initial <small>DBCC FREEPROCCACHE</small> is only there to give us a clean slate. Also, for the purpose of the demo, I run the query with a parameter value that gives the "bad" plan, the Clustered Index Scan. Once the plan has been entered, it takes effect directly. That is, any current entry for the query is evicted from the cache.</p>
<p>On <small>SQL</small> 2008, you can specify the parameters to <b>sp_create_plan_guide</b> in any order as long as you name them, and you can leave out the N before the string literals. However, <small>SQL</small> 2005
is far less forgiving. The parameters must be entered in the given order, even
if you name them, and you must specify the N before all the string literals.</p>
<p>In this example I used a plan guide to force an index, but you can use other hints as well, including the <small>USE PLAN</small> hint that permits you to specify the
complete query plan to use. Certainly not for the faint of heart!</p>
<p>...although that is exactly the idea with plan freezing. Say that you have a query that sways between two plans, one good and one bad, due to parameter sniffing, and there is not really any civilised way to get the bad plan out of the equation. Rather than battling with the complex parameters of <b>sp_create_plan_guide</b>, you can extract plan handle from the cache and feed it to the stored procedure <b>sp_create_plan_guide_from_handle</b> to force the plan you know is good. Here is a demo and example.</p>
<pre>DBCC FREEPROCCACHE
SET ARITHABORT ON
go
EXEC sp_executesql N'SELECT * FROM dbo.Orders WHERE OrderDate &gt; @orderdate',
                   N'@orderdate datetime', @orderdate = '19990101'
go
DECLARE @plan_handle varbinary(64),
        @rowc int

SELECT @plan_handle = plan_handle
FROM   sys.dm_exec_query_stats qs
CROSS  APPLY sys.dm_exec_sql_text(qs.sql_handle) est
WHERE  est.text LIKE '%Orders WHERE OrderDate%'
  AND  est.text NOT LIKE '%dm_exec_query_stats%'
SELECT @rowc = @@rowcount

IF @rowc = 1
   EXEC sp_create_plan_guide_from_handle 'MyFrozenPlan', @plan_handle
ELSE
  RAISERROR('%d plans found in plan cache. Canno create plan guide', 16, 1, @rowc)
go
-- Test it out!
SET ARITHABORT OFF
go
EXEC sp_executesql N'SELECT * FROM dbo.Orders WHERE OrderDate &gt; @orderdate',
                   N'@orderdate datetime', @orderdate = '19960101'
go
SET ARITHABORT ON
EXEC sp_control_plan_guide 'DROP', 'MyFrozenPlan'
</pre>
<p>For the purpose of the demo, I first clear the plan cache and set <small>ARITHABORT</small> to a known state. Then I run my query with a parameter that I know will give me the good plan. The next batch demonstrates how to use <b>sp_create_plan_guide_from_handle</b>. I first run a query against <b>sys.dm_exec_query_stats</b> and <b>sys.dm_exec_sql_text</b> to find the entry for my batch. Next I capture <b>@@rowcount</b> into a local variable (since <b>@@rowcount</b> is set after each statement, I prefer to copy it to a local variable in a <small>SELECT</small> that is directly attached to the query to avoid accidents). This is a safety precaution in case I get multiple matches or no matches in the cache. If I get exactly one match, I call <b>sp_create_plan_guide_from_handle</b> which takes two parameters: the name of the plan guide and the plan handle. And that's it!</p>
<p>The next part tests the guide. To make sure that I don't use the same cache entry, I use a different setting of <small>ARITHABORT</small>. If you run the demo with display of execution plan enabled, you will see that the second execution of the query uses the same plan with Index Seek + Key Lookup as the first, alhough the normal plan for the given parameter would be a Clustered Index Scan. That is, the plan guide is not dependent on certain <small>SET</small> options.</p>
<p>When you use this for real, you would  run the query you want the plan guide for, only if the desired plan is not in the cache already. The query against the cache will require some craftmanship so that you get exactly one hit and the correct hit. An alternative may be to look at the matches in the query output in <small>SSMS</small> and copy and paste the plan handle.</p>
<p>A cool thing is that you don't have to set this up on the production server, but you can experiement on a lab server. The guide is stored in <b><a href="http://msdn.microsoft.com/en-us/library/ms178010%28v=SQL.105%29.aspx" target="_blank">sys.plan_guides</a></b>, so once you have the guide right, you can use the content there to craft a call to <b>sp_create_plan_guide</b> that you run the production server. You can also script the plan guide through Object Explorer in <small>SSMS</small>.</p>
<p>If you have a multi-statement batch or a stored procedure, you may not want to set up a guide for the entire batch, but only for a statement. For this reason <b>sp_create_plan_guide_from_handle</b> accepts a third parameter <b>@statement_start_offset</b>, a value you also can get from <b>sys.dm_exec_query_stats</b>. </p>
<p>This was just an introduction to plan guides and plan freezing. Plan guides is certainly an advanced feature,
and rather than making this article even longer I refer you to the
  <a href="http://msdn.microsoft.com/en-us/library/ms187032.aspx" target="_blank">sections on plan guides</a> in Books Online, or
to the white paper
listed in the section <i><a href="#furtherreading">Further Reading</a></i> below.</p>
<p>Overall, I see plan guides as a last resort. As I have said, they are fairly complex and there is a bit of a learning curve. Furthermore, a hint within a
  query is easy to see and will stand out like a sore thumb the day it has become obsolete. But a plan guide may lie around, and one day
  when reality has changed, the guide induces a bad query plan. But since you have
forgotten about the guide, you cannot understand why you get this bad plan. Plan freezing is easier to use, but keep in mind that with the smallest change in the query, the plan guide will no longer be matched by <small>SQL</small> Server.</p>
<h2><a name="Conclusion">Concluding Remarks</a></h2>
<p>You have now learnt why a query may perform differently in the application
and in <small>SSMS</small>. You have also seen several ways to address issues with parameter
sniffing.</p>
<p>Did I include all reasons why there could be a performance difference between
the application and <small>SSMS</small>? Not all, there are certainly a few more reasons. For
instance, if the application runs on a remote machine, and you run <small>SSMS</small> directly
on the server, a slow network can make quite a difference. But I believe I have
captured the most likely reasons that are due to circumstances within <small>SQL</small>
Server.</p>
<p>Did I include all possible reasons why parameter sniffing may give you a bad
plan, and how you should address it? Probably not. There is room for more
variation there, and particularly I can't know what your applicaton is doing.
But hopefully some of the methods I have presented you can help you to find a
solution.</p>
<p>If you think that you have stumbled on something which is not
covered in the article, but you think I should have included, please drop me a
line at <a href="mailto:esquel@sommarskog.se">esquel@sommarskog.se</a>. And the
same applies if you see any errors, not the least spelling or grammar errors. On the other
hand, if you have a question how to solve a particular problem, I recommend you
to post a question to an <small>SQL</small> Server forum, because a lot more people will see
your question. If you absolutely want me to see the question, post to Microsoft's
<a href="http://social.msdn.microsoft.com/Forums/en-US/transactsql">Transact-<small>SQL</small>
forum</a>, and mail me the link (because there is too much traffic for me to read it
all).</p>
<h3><a name="furtherreading">Further Reading</a></h3>
<p>If you want learn more about query compilation, statistics etc, here are a few articles that I like to recommend:</p>
<p><i><a href="http://technet.microsoft.com/en-us/library/dd535534%28SQL.100%29.aspx">Statistics Used by the Query Optimizer in Microsoft <small>SQL</small> Server 2008</a></i>
– A white paper written by Eric Hanson and Yavor Angelov from the <small>SQL</small> Server team. There is also a
<a href="http://technet.microsoft.com/en-us/library/cc966419.aspx">version for <small>SQL</small> 2005</a>.</p>
<p><i><a href="http://technet.microsoft.com/en-us/library/ee343986%28SQL.100%29.aspx">Plan Caching in <small>SQL</small> Server 2008</a></i> – A white paper written by <small>SQL</small>
Server <small>MVP</small> Greg Low. Appendix A in this paper details the rules for simple
parameterisation. There is an older
<a href="http://technet.microsoft.com/en-us/library/cc966425.aspx">
version for <small>SQL</small> 2005</a>, written by Arum Marathe.</p>
<p><i>
<a href="http://technet.microsoft.com/en-us/library/dd672789%28SQL.100%29.aspx">Troubleshooting Performance Problems in <small>SQL</small> Server 2008</a></i> – An
extensive document, which looks at performance from several angles, not only query tuning. Written by a number of developers and <small>CSS</small> people from
Microsoft. There is also a <a href="http://technet.microsoft.com/en-us/library/cc966540.aspx">version for <small>SQL</small> 2005</a>.</p>
<p> <i>
<a href="http://technet.microsoft.com/en-us/library/cc917694.aspx">Forcing Query Plans</a></i>
– A white paper about plan guides. This version is for <small>SQL</small> 2005; I have not seen
any version for <small>SQL</small> 2008.</p>
<h2><a name="Revisions">Revisions</a></h2>
<p><b>2013-08-30</b> – Corrected an error in the function <a href="http://www.sommarskog.se/query-plan-mysteries/setoptions.sql">setoptions</a> that Alex Valen was kind to point out.</p>
<p><b>2013-04-14</b> – Updated the section <a href="#linkedservers"><i>An Issue with Linked Servers</i></a> to reflect that this gotcha has been removed with <small>SQL</small> 2012 SP1. Also added a note on <small>SET NO_BROWSETABLE ON</small> to the section <a href="#storysofar"><i>The Story so Far</i></a>.</p>
<p><b>2011-12-31</b> – Added some text about <a href="#defaultsettings"><small>SQL</small> Server Agent</a> which runs with <small>QUOTED_IDENTIFER OFF</small> by default. Extended the section on <a href="#planguides">Plan Guides</a> to also gover plan freezing.</p>
<p><b>2011-11-27</b> – Several of the links in the section Further Reading were broken. This has been fixed.</p>
<p><b>2011-07-02</b> – There is now a <a href="http://www.sql.ru/blogs/somewheresomehow/999">Russian</a> translation available. Kudos to Dima Piliugin for this work!</p>
<p><b>2011-06-25</b> – Added a section <a href="#linkedservers"><i>An Issue with Linked Servers</i></a> to discuss a special problem that may cause the situation Slow in the application, fast in <small>SSMS</small>.</p>
<p><b>2011-02-20</b> – First version.</p>


<div class=" lleo_show" id="lleo_enjoyContentControls">
    <div id="lleo_enjoyContentPanel">
        <label id="lleo_enjoyContentLabel"><input id="lleo_enjoyContentCheckbox" checked="checked" type="checkbox">Show this icon if possible</label>
    </div>
    <div id="lleo_enjoyContentButton" title="Enjoy Content!"></div>
</div></body></html>