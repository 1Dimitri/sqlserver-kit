<html><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta http-equiv="Content-Language" content="en-gb">
<style type="text/css"><!--
/* Effective stylesheet produced by snapshot save */
small { font-size: 90%; }
h2 { border-top: 2px dashed black; padding-top: 12pt; }
h3 { border: 1px solid rgb(187, 187, 187); background-color: rgb(224, 224, 224); padding-right: 6pt; display: inline; text-align: left; }
li { margin-bottom: 3pt; margin-left: -10pt; }
pre { margin-left: 18pt; }
.errmsg { color: rgb(255, 0, 0); }
.nowrap { white-space: nowrap; }
--></style>

<title>Arrays and Lists in SQL Server 2008</title>
<style type="text/css"><!--
/* Effective stylesheet produced by snapshot save */
#lleo_dialog *::before, #lleo_dialog *::after { content: ""; }
--></style><style id="lleo_css_enjoyContentControls" type="text/css"><!--
/* Effective stylesheet produced by snapshot save */
#lleo_enjoyContentControls, #lleo_enjoyContentControls * { color: rgb(0, 0, 0) ! important; font: 13px/15px Arial,Helvetica ! important; margin: 0px ! important; padding: 0px ! important; background: transparent none repeat scroll 0% 0% ! important; border: 0px none ! important; position: static ! important; vertical-align: baseline ! important; overflow: visible ! important; width: auto ! important; height: auto ! important; max-width: none ! important; max-height: none ! important; float: none ! important; visibility: visible ! important; text-align: left ! important; text-transform: none ! important; border-collapse: separate ! important; border-spacing: 2px ! important; box-sizing: content-box ! important; box-shadow: none ! important; opacity: 1 ! important; text-shadow: none ! important; }
#lleo_enjoyContentControls { background: rgb(247, 200, 117) none repeat scroll 0% 0% ! important; position: fixed ! important; right: 0px ! important; top: -40px ! important; width: 39px ! important; height: 34px ! important; opacity: 0.85 ! important; border-top-left-radius: 4px ! important; border-bottom-left-radius: 4px ! important; box-shadow: 2px 4px 12px rgba(0, 0, 0, 0.3) ! important; z-index: 2147483647 ! important; overflow: hidden ! important; }
#lleo_enjoyContentControls.lleo_show { top: 130px ! important; transition: top 0.8s ease-out 0s ! important; }
#lleo_enjoyContentControls:hover { opacity: 1 ! important; transition: opacity 0.4s linear 0s, width 0.4s linear 1.2s ! important; }
#lleo_enjoyContentControls #lleo_enjoyContentPanel { white-space: nowrap ! important; margin: 9px 44px 0px 10px ! important; opacity: 0 ! important; transition: all 0.4s linear 1.2s ! important; }
#lleo_enjoyContentControls:hover #lleo_enjoyContentPanel { opacity: 1 ! important; }
#lleo_enjoyContentControls #lleo_enjoyContentPanel input { margin-right: 5px ! important; }
#lleo_enjoyContentButton { background: rgb(255, 255, 255) url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQBAMAAADt3eJSAAAAA3NCSVQICAjb4U/gAAAACXBIWXMAAAk6AAAJOgHwZJJKAAAAGXRFWHRTb2Z0d2FyZQB3d3cuaW5rc2NhcGUub3Jnm+48GgAAADBQTFRF////AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAL92gewAAAA90Uk5TAA0PEhUcLS9CxszS29z1M86yOAAAAFVJREFUCFtjYIADduMCMGZgvmsAxggGUFgELMXAwDq7AaI8MnULmGY9pTYRzIj5vzIALHD+/1Gw4pj/fwJA2kECYHNkgAJgK/yAAmDA8QasBQgcIRQA12YesqxFXfcAAAAASUVORK5CYII8850aa0afe766ff5b5bb3697b8de32f7") no-repeat scroll center center / 16px 16px ! important; width: 36px ! important; height: 34px ! important; cursor: pointer ! important; position: absolute ! important; right: 0px ! important; top: 0px ! important; }
@media only screen and (min\2D\2D moz-device-pixel-ratio: 2), not all, not all {
  #lleo_enjoyContentButton { background-image: url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAMAAABEpIrGAAAAA3NCSVQICAjb4U/gAAAACXBIWXMAABJ0AAASdAHeZh94AAAAGXRFWHRTb2Z0d2FyZQB3d3cuaW5rc2NhcGUub3Jnm+48GgAAAD9QTFRF////AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAxQXeHgAAABR0Uk5TAAQFDRkcNz1PcHSIrK6ws7/b8P5ZkzdFAAAAo0lEQVQ4y62RzQ6DMAyDTSkjpfy04Pd/1h12WAtpKk3zzfosJXGAP2l0AODGloc7BJDDtTwgTInS9kAik+W7gd6I7pLFWRK9cuZXgcyT0VjguliJwG1AYPZNvjsAC2OLXzOAYa07LPfj+YLbGJqcPOfd5OTV4WSAVlTNn1Xf+ONZD357t8KrgMbLERovlxSFV2dGhVfy2ebAlG3+SVgc8FHws96B8RE8rjk+bwAAAABJRU5ErkJggga9a50a0b337b82ef5c0bda0f6d6960d3") ! important; }
}
--></style><style type="text/css"><!--
/* Effective stylesheet produced by snapshot save */
#lleo_enjoyContentControls:hover { width: 225px ! important; }
--></style></head>
<body>
<h1 align="center">Arrays and Lists in SQL Server 2008<br><small>Using Table-Valued Parameters</small></h1>
<p><small>An SQL text by <a href="http://www.sommarskog.se/index.html">Erland Sommarskog</a>, SQL Server MVP. <a href="#Revisions">Latest
revision</a>: 2012-07-01.</small></p>
<h2><a name="introduction">Introduction</a></h2>
<p>In the public forums for <small>SQL</small> Server, you often see people asking <i>How do I use
arrays in <small>SQL</small> Server</i>? Or <i>Why does <code>SELECT * FROM tbl WHERE col IN (@list)</code>
not work</i>? The short answer to the first question is that <small>SQL</small> Server does not have
arrays – <small>SQL</small> Server has tables. Upto <small>SQL</small> Server 2005, there was no way to pass a table
from a client, but you had to pass a comma-separated string or similar to <small>SQL</small> Server, and then you would unpack that list into a table in your stored procedure.</p>
<p>This changed with <small>SQL</small> 2008. The
  advent of table-valued parameters makes it dirt simple to pass a comma-separated list to <small>SQL</small> Server. In this article I will introduce a simple and perfectly reusable class for this task. Table-valued parameters are also great when you want to load data to <small>SQL</small> Server through a stored procedure; you no longer have to build <small>XML</small> documents that you shred in <small>SQL</small> Server. In this article I show you how to load a master-detail file into <small>SQL</small> Server tables in two different ways: read the entire file into memory or stream it directly. What I am not showing – because it's so simple – is that if you already have your data in a <b>DataTable</b> object, you can pass that DataTable as the value for your <small>TVP</small>.</p>
The examples are in C# and VB .<small>NET</small>, using the SqlClient <small>API</small>, and the main body of the article covers this environment. For other environments such as Java or Entity Framework, there is a quick overview of what is possible at the end of the article.
<p>There is an accompanying article: <i><a href="http://www.sommarskog.se/arrays-in-sql-2005.html">Arrays and Lists in <small>SQL</small> Server 2005 and Beyond</a></i>
  (and an even older for <a href="http://www.sommarskog.se/arrays-in-sql-2000.html"><small>SQL</small> 2000</a>) where I in detail describe various methods to pass a list of values in a
string and unpack them into a table in <small>SQL</small> Server. For the hard-core geeks there are two performance indexes, one <a href="http://www.sommarskog.se/arrays-in-sql-perftest-2009.html">labelled <small>SQL</small> 2008</a> and an older <a href="http://www.sommarskog.se/arrays-in-sql-perftest.html">labelled <small>SQL</small> 2005</a>, where I relate data from performance tests of the methods described in
the articles, including table-valued parameters.</p>
<p><b>Contents:</b></p>
<contents>
<p>   <b><a href="#introduction">Introduction</a></b><br>
     <b><a href="#Background">Background</a></b><br>
     <b><a href="#TVP_in_TSQL">Table-Valued Parameters in T‑SQL</a></b><br>
        <a href="#Declarations">Declarations</a><br>
        <a href="#Invoking">Invoking an SP with a TVP</a><br>
        <a href="#Permissions">Permissions</a><br>
        <a href="#CLR">Restrictions</a><br>
     <b><a href="#ADONET">Passing Table-Valued Parameters from ADO .NET</a></b><br>
        <a href="#Demofiles">About the Sample Code</a><br>
     <b><a href="#PassingCSV">Sending  a Comma-Separated List to SQL Server</a></b><br>
        <a href="#SPcall">Using the Class with a Stored Procedure</a><br>
        <a href="#CSV_splitter">Inside the CSV_splitter Class</a><br>
     <b><a href="#Performance_Considerations">What About Performance?</a></b><br>
        <a href="#Server-side">Performance in SQL Server</a><br>
        <a href="#Client-side">Client-side Performance</a><br>
        <a href="#SqlMetaData">Primary Keys and Sorted Data – Looking Closer at the SqlMetaData constructors</a><br>
     <b><a href="#loadingdata">Loading Data through Table-Valued Parameters</a></b><br>
        <a href="#loadsetup">The Setup</a><br>
        <a href="#fileloaddemo1">Take One: Reading the File Into a List</a><br>
        <a href="#filestreaming">Take Two: Streaming the File</a><br>
        <a href="#filereadperf">Performance Considerations</a><br>
     <b><a href="#OtherAPIs">Using Table-Valued Parameters from Other APIs</a></b><br>
     <b><a href="#Acknowledgements">Acknowledgements, Feedback and Further Reading</a></b><br>
     <b><a href="#Revisions">Revision History</a></b></p>
<p>Some of the sample code in this article refers to the <b>Northwind</b> database.
   This database does not ship with <small>SQL</small> Server, but you can download the script to
install it from <a href="http://www.microsoft.com/downloads/info.aspx?na=22&amp;p=1&amp;SrcDisplayLang=en&amp;SrcCategoryId=&amp;SrcFamilyId=&amp;u=/downloads/details.aspx?FamilyID=06616212-0356-46a0-8da2-eebc53a68034&amp;DisplayLang=en"> Microsoft's web site</a>. </p>
</contents>
<h2><a name="Background">Background</a></h2>
<p>You have a number of key values, identifying a couple of rows in a table, and
   you want to retrieve these rows. If you are the sort of person who composes
   your <small>SQL</small> statements in
   client code, you might have something that looks like this:</p>
<pre>cmd.CommandText = "SELECT ProductID, ProductName FROM Northwind.dbo.Products " &amp; _
                  "WHERE ProductID IN (" &amp; List &amp; ")"
reader = cmd.ExecuteReader()</pre>
<p><code>List</code> is here a string variable that you somewhere have assigned a comma-separated list, for instance <b>"9,12,27,39"</b>.
</p>
<p>This sort of code is bad practice, because you should never interpolate
   parameter values into your query string. (Why, is beyond the scope of this
   article, but I discuss this in detail in my article <i>
   <a href="http://www.sommarskog.se/dynamic_sql.html">The Curse and Blessings of Dynamic <small>SQL</small></a></i>,
   particularly in the sections on <i><a href="http://www.sommarskog.se/dynamic_sql.html#SQL_injection">
   <small>SQL</small> Injection</a></i> and <i><a href="http://www.sommarskog.se/dynamic_sql.html#queryplans">Caching
   Query Plans</a></i>.)
</p>
<p>Since this is bad practice, you want to use stored
   procedures. However, at first glance you don't seem to find that any apparent
   way to do this. Many have tried with:</p>
<pre>CREATE PROCEDURE get_product_names @ids varchar(50) AS
   SELECT ProductID, ProductName
   FROM   Northwind.dbo.Products
   WHERE  ProductID IN (@ids)</pre>
<p>But when they test:</p>
<pre>EXEC get_product_names '9,12,27,37'</pre>
<p>The reward is this error message:</p>
<pre class="errmsg">Server: Msg 245, Level 16, State 1, Procedure get_product_names, Line 2
Syntax error converting the varchar value '9,12,27,37' to a column
of data type int.</pre>
<p>This fails, because we are no longer composing an <small>SQL</small> statement dynamically,
   and  <code>@ids</code>
   is just one value in the <small>IN</small> clause. An <small>IN</small> clause that could also read:</p>
<pre>... WHERE col IN (@a, @b, @c)</pre>
<p>Or more to the point, consider this little script:</p>
<pre>CREATE TABLE #csv (a varchar(20) NOT NULL)
go
INSERT #csv (a) VALUES ('9,12,27,37')
INSERT #csv (a) VALUES ('something else')
SELECT a FROM #csv WHERE a IN ('9,12,27,37')  -- Returns one row.</pre>
<p>So now you know why <code>col IN (@list)</code> does not work. Or rather: you know now that it works differently from
your expectations. In the following we will look into how to solve this kind
of problem with table-valued parameters.</p>
<p>Before I go on, I should add that sometimes you may find yourselves in the (very
unfortunate) situation when you have a delimited list in a table
column in your database. To unpack such a list, you would need any of the methods that I discuss in <a href="http://www.sommarskog.se/arrays-in-sql-2005.html">
<i>Arrays
and Lists for <small>SQL</small> 2005 and Beyond</i></a>;
<small>TVP</small>s cannot help you here.</p>
<h2><a name="TVP_in_TSQL">Table-Valued Parameters in <span class="nowrap">T-SQL</span></a></h2>
<h3><a name="Declarations"></a>Declarations</h3>
<p>Let's first look at how to use <small>TVP</small>s in <small>T‑SQL</small> without involving a client. To be able to declare a <small>TVP</small>, you first need
  to create a table type like this:</p>
<pre>CREATE TYPE integer_list_tbltype AS TABLE (n int NOT NULL PRIMARY KEY)</pre>
<p>That is, after <small>CREATE TYPE</small> you specify the type name followed by <small>AS TABLE</small> and then
comes the table definition,
using the same syntax as <small>CREATE TABLE</small>. You cannot use  everything you can use with <small>CREATE
TABLE</small>, but you can define <small>PRIMARY KEY, UNIQUE</small> and <small>CHECK</small> constraints, you can use <small>IDENTITY</small> and <small>DEFAULT</small> definitions,
and you can define computed columns. Once you have this table type, you can use it
to declare table variables:</p>
<pre>DECLARE @mylist integer_list_tbltype</pre>
<p>However, you cannot use the type with <small>CREATE TABLE</small> (it could have been nutty with temp tables!),  nor can you use it for
the declaration of the return table in a multi-step table function. The raison d'être for table types is to make it possible to declare table-valued parameters for stored procedures or user-defined functions. Here is one example:</p>
<pre>CREATE PROCEDURE get_product_names @prodids integer_list_tbltype READONLY AS
   SELECT p.ProductID, p.ProductName
   FROM   Northwind.dbo.Products p
   WHERE  p.ProductID IN (SELECT n FROM @prodids)</pre>
<p>The body of the procedure brings no surprises. The code looks just as it would have if <code>@prodids</code> had been a local
table variable. The parameter declaration on the other hand includes a keyword hitherto not seen in this
context: <small>READONLY</small>. This keyword means what it says: you cannot modify the contents of the table parameter in any
way in the procedure. As an aside, this restriction makes <small>TVP</small>s far less useful than they could have been; 
often you want to pass data between stored procedures as I discuss in my article <i><a href="http://www.sommarskog.se/share_data.html">How to
Share Data Between Stored Procedures</a></i>. However, for the task at hand, passing data from a client, the <small>READONLY</small>
restriction is no major obstacle.</p>
<h3><a name="Invoking"></a>Invoking an SP with a TVP</h3>
<p>Calling this procedure is straightforward:</p>
<pre>DECLARE @mylist integer_list_tbltype
INSERT @mylist(n) VALUES(9),(12),(27),(37)
EXEC get_product_names @mylist</pre>
<p>(Here I use the new syntax for <small>INSERT</small> that permits me to specify values for more than one row in the <small>VALUES</small> clause.)
You can also use <small>TVP</small>s with <b><a href="http://www.sommarskog.se/dynamic_sql.html#sp_executesql">sp_executesql</a></b>:</p>
<pre>DECLARE @mylist integer_list_tbltype,
        @sql nvarchar(MAX)
SELECT  @sql = N'SELECT p.ProductID, p.ProductName
                 FROM    Northwind..Products p
                 WHERE   p.ProductID IN (SELECT n FROM @prodids)'
INSERT @mylist VALUES(9),(12),(27),(37)
EXEC sp_executesql @sql, N'@prodids integer_list_tbltype READONLY', @mylist</pre>
<p>There are a few peculiarities, though. This does not work:</p>
<pre>EXEC get_product_names NULL</pre>
<p>but results in this error message:</p>
<pre class="errmsg">Msg 206, Level 16, State 2, Procedure get_product_names, Line 0
Operand type clash: void type is incompatible with integer_list_tbltype</pre>
<p>It is quite logical when you think of it: <small>NULL</small> is a scalar value,  not a table value. But what do you think about
this:</p>
<pre>EXEC get_product_names</pre>
<p>You may expect this to result in an error due to the missing parameter, but instead this runs and produces an empty
result set! The same happens with:</p>
<pre>EXEC get_product_names DEFAULT</pre>
<p>The scoop is that a table-valued parameter always has the implicit default value of an empty table. Whether this is
good or bad can be disputed, but if there were to be explicit default values, Microsoft would have to invent a lot of
syntax for it. And in most cases, the default value you want is probably the empty table, so it is not entirely unreasonable.</p>
<h3><a name="Permissions"></a>Permissions</h3>
<p>One thing with table types which is not apparent is that you need permission 
  to use a table type. This can be demonstrated in this script:</p>
<pre>CREATE USER testuser WITHOUT LOGIN
go
EXECUTE AS USER = 'testuser'
go
DECLARE @p integer_list_tbltype
go
REVERT
go
DROP USER testuser</pre>
<p>(What we do here is to create a loginless user, and then impersonate that 
user. This is a quick way to test permissions. For more details on impersonation, 
see my article <i><a href="http://www.sommarskog.se/grantperm.html">Giving Permissions through Stored 
Procedures</a></i>.)</p>
<p>The output from this script is puzzling:</p>
<pre class="errmsg">Msg 229, Level 14, State 5, Line 1
The EXECUTE permission was denied on the object 'integer_list_tbltype',
database 'tempdb', schema 'dbo'.</pre>
<p>But the message is to be taken by the letter. To be able to use a table type, 
you need to have <small>EXECUTE</small> permission on the type. (This does not apply to normal 
scalar types, but it does apply to user-defined <small>CLR</small> types.) To grant permission 
on a type, the syntax is:</p>
<pre>GRANT EXECUTE ON TYPE::integer_list_tbltype TO testuser</pre>
<p>The <code>TYPE::</code> prefix is needed to specify the object class.</p>
<h3><a name="CLR"></a>Restrictions</h3>
<p>It is maybe not so surprising that you cannot use table-valued parameters across linked servers, given that there are many restrictions with linked servers. But it does not stop there: you cannot even use table-valued parameters across databases. If you try something like:</p>
<pre>USE tempdb
go
CREATE TYPE tobbe AS TABLE (a int NOT NULL PRIMARY KEY)
go
CREATE PROCEDURE tobbe_sp @t tobbe READONLY AS
   SELECT a FROM @t
go
USE otherdb
go
CREATE TYPE tobbe AS TABLE (a int NOT NULL PRIMARY KEY)
go
DECLARE @t tobbe
EXEC tempdb..tobbe_sp @t</pre>
<p>  The error message is:</p>
<pre class="errmsg">Msg 206, Level 16, State 2, Procedure tobbe_sp, Line 0
Operand type clash: tobbe is incompatible with tobbe</pre>
<p>And if you try</p>
<pre>CREATE PROCEDURE tobbe_sp @t otherdb.dbo.tobbe READONLY AS
   SELECT a FROM @t </pre>
<p>You get the message:</p>
<pre class="errmsg">Msg 117, Level 15, State 2, Procedure tobbe_sp, Line 1
The type name 'otherdb.dbo.tobbe' contains more than the maximum number of prefixes. The maximum is 1.</pre>
<p>This some awkward message informs us that the data type for a parameter cannot be a three-part name with a database component.</p>
<p>You cannot create stored procedures or
  user-defined functions in the <small>CLR</small> that
  take a table-valued parameter. But the
  other way works: you can call a <small>T‑SQL</small> procedure with a <small>TVP</small> from a <small>CLR</small> procedure, using the same mechanisms you use from a client and which is what we will
look at next.</p>
<h2><a name="ADONET">Passing Table-Valued Parameters from ADO .NET</a></h2>
<p>Passing values to <small>TVP</small>s from <small>ADO .NET</small> is very straightforward, and requires very little extra code compared to passing
  data to regular parameters. You need  .<small>NET</small> Framework 3.5 SP1 or higher to have support for TVPs. You can only use TVPs with SqlClient; you cannot use TVPs with the classes in the <b>System.Data.OleDb</b> or <b>System.Data.Odbc</b> namespaces. </p>
<p>The specifics can be summarised as:</p>
<ul>
  <li>For the data type you specify <b>SqlDbType.Structured</b>.</li>
  <li>You specify the name of the table type in the <b>TypeName</b> property of the parameter.</li>
  <li>You set the <b>Value</b> property of the parameter to something suitable.</li>
</ul>
<p>Exactly what is suitable then? There is an <a href="http://msdn.microsoft.com/en-us/library/bb675163.aspx"><b><small>MSDN</small> topic</b></a> that suggest that the three choices are <b>List&lt;SqlDataRecord&gt;</b>, a <b>DataTable</b> or a <b>DbDataReader</b>. It turns out that this is not the full story. I have not been able – and nor have I really tried – to find the exact requirements, but it seems that you can pass anything that implements <b>IEnumerable</b> and <b>IDataRecord</b>, and then <b>DataTable</b> is a special case that goes beyond that. Exactly what you can use and not use is not particularly interesting. I would suggest that in practice you will use one of these four:</p>
<ul>
  <li><b>List&lt;SqlDataRecord&gt;</b>.</li>
  <li>Custom-written classes that implement <b>IEnumerable&lt;SqlDataRecord&gt;</b> and <b>IEnumerator&lt;SqlDataRecord&gt;</b>.</li>
  <li><b>DataTable</b>.</li>
  <li>A <b>DbDataReader</b> of some sort.</li>
</ul>
<p>Of these, you would use the first two for general-purpose programming. The only reason to pass a <b>DataTable</b> is that you already have the data in  such an object. If you have the data somewhere else – in a file, on a wire etc – there is no reason to fill a <b>DataTable</b> when you can use a <b>List</b> which is more lightweight. On the same token, the only reason you would use a <b>DbDataReader</b>, is because you have a <b>DbDataReader</b> anyway. That is, if the data for your <small>TVP</small> comes from an Oracle database, you can pass an <b>OracleDataReader</b> directly – no need to populate a <b>List</b> or a <b>DataTable</b>. </p>
<p>For this reason, in this article I focus on the first two alternatives, and all examples use either a custom-written class or a <b>List&lt;SqlDataRecord&gt;</b>.</p>
<p>One caveat about <b>DataTable</b> and <b>DbDataReader</b>: if your <small>TVP</small> has an <small>IDENTITY</small> column or a computed column, you may not be able to get these columns to work with your objects. In this case, you can always use a <b>List</b> or a custom-written iterator, since this gives you access to some more options to define the metadata for the <small>TVP</small>, and I will briefly cover how to do this <a href="#SqlMetaData">later</a>.</p>
<h3><a name="Demofiles"></a>About the Sample Code</h3>
<p>Before we move on, I like to give a quick introduction to the demo files that accompany this article. They are compiled in two zip files, one with the demos in <a href="http://www.sommarskog.se/arraylist-2008/Demo/Csharp.zip">C#</a> and one with the same demos in <a href="http://www.sommarskog.se/arraylist-2008/Demo/VB.zip">Visual Basic .<small>NET</small></a>. Use the language that is the most convenient to you. In the text itself, I sometimes show the code in C# and sometimes in VB .<small>NET</small>. If you are more comfortable with the language I'm not using for the moment, please refer to the corresponding file in the other language. For instance, if I refer to <b>TVPDemo.DemoHelper.cs</b>, you can rely on that there is also a <b>TVPDemo.DemoHelper.vb</b> file in the zip file with the VB code. </p>
<p>Beside the source code in C# and VB .<small>NET</small>, the zip files also include an <small>SQL</small> script and a file with sample data for one of the demos. There is also a file <b>compile.bat</b> you can use to compile the files. There are however no project/solution files for Visual Studio, as that goes a little above my head. </p>
<p>I will cover the code in the zip files as we arrive to the examples where they belong. There is however one class I like to highlight here and now, and that is the <b>TVPDemo.DemoHelper</b> class. This class includes some utility routines that are of little interest for the article as such. There is one thing I like to highlight, though, to wit the connection string:</p>
<pre>private const string connstr =
        "Application Name=TVPdemo;Integrated Security=SSPI;" +
        "Data Source=.;Initial Catalog=tempdb";</pre>
<p>You may have to  change it to fit your environment. Particularly, if you only have Express Edition installed, you should probably use .\<small>SQLEXPRESS</small>  for <code>Data Source</code> instead of the single dot.</p>
<p>In the article, the code mainly appears without comments, since I explain the code in the text. However, in the source files, the code is thoroughly commented.</p>
<p><b>Disclaimer</b>: My expertise is in <small>SQL</small> Server, and I only write .<small>NET</small> code left-handedly. While I have done my best to adhere what I think is best practice, you may see things which makes you think "I would never do something like that". It is not unlikely that you will be right. Please let me know in such case!</p>
<h2><a name="PassingCSV"></a>Sending a Comma-Separated List to SQL Server</h2>
<p>The <i>Arrays and Lists</i> articles take their base in the problem of using a comma-separated list in <small>SQL</small> Server. Programmers often encounter them, because there are form controls that produce such lists. (Or so the .<small>NET</small> programmers I know keep telling me.) The other articles in this series present solutions to transform these lists into a table in <small>SQL</small> Server. Here I'm showing you a much better solution: transform the list in the client and pass it to a table-valued parameter. <small>SQL</small> Server should spend its resources on reading and writing tabular data, not string processing. Not only are the resources better spent this way, the solution is also much simpler and cleaner with help of  the class <b>CSV_splitter</b> that I will introduce.</p>
<h3><a name="SPcall"></a>Using the CSV_splitter Class</h3>
<p>Using the <b>CSV_splitter</b> class is extremely simple. All your application code sees is a call to the constructor and that's it. Here is an example where we call a stored procedure with a <small>TVP</small>. We use the table type and the stored procedure I used in the <a href="#TVP_in_TSQL"><small>T‑SQL</small> section</a> above.:</p>
<pre>CREATE TYPE integer_list_tbltype AS TABLE (n int NOT NULL PRIMARY KEY)
go
CREATE PROCEDURE get_product_names @prodids integer_list_tbltype READONLY AS
   SELECT p.ProductID, p.ProductName
   FROM   Northwind.dbo.Products p
   WHERE  p.ProductID IN (SELECT n FROM @prodids)</pre>
<p>In the set of demo files you find <b>CSVDemo.vb</b> which includes the 
procedure <code>CSV_to_SP</code> that calls <code>get_product_names</code>, and it is no more complicated 
than this.</p>
<pre>Private Sub CSV_to_SP()

   Using cn As SqlConnection = TVPDemo.DemoHelper.SetupConnection(), _
         cmd As SqlCommand = cn.CreateCommand()

      cmd.CommandType = CommandType.StoredProcedure
      cmd.CommandText = "dbo.get_product_names"

<b>      cmd.Parameters.Add("@prodids", SqlDbType.Structured)
      cmd.Parameters("@prodids").Direction = ParameterDirection.Input
      cmd.Parameters("@prodids").TypeName = "integer_list_tbltype"
      cmd.Parameters("@prodids").Value = _ 
                new TVPDemo.CSV_splitter("9,12,27,37")</b>

      Using da As new SqlDataAdapter(cmd), _
            ds As new DataSet()
         da.Fill(ds)
         TVPDemo.DemoHelper.PrintDataSet(ds)
      End Using
   End Using
End Sub</pre>
<p>To a large extent, very typical code to call a stored procedure. We first set up a connection and create a command object. We move on to state which stored procedure to call, and we define the single parameter that <code>get_product_names</code> accepts. Finally, we invoke the procedure, and in this example I have chosen to  use <b>DataAdapter.Fill</b> together with a method in my <b>DemoHelper</b> class that prints the result set. In a real-world scenario you may prefer to use <b>ExecuteReader</b> or whatever fits you. </p>
<p>(I assume that most readers are acquainted with <code>Using</code>, but in case you are not: this statement permits you to declare a variable
  that is accessible in the enclosed block, and when the block exits, any <b>Dispose</b> method of the class will be invoked. This is
  highly recommendable for <b>SqlConnection</b> and <b>SqlCommand</b> objects. If you just leave it to garbage collection to take care of
them, you may spew <small>SQL</small> Server with a lot of extra connections. <code>Using</code> is available in C# as well, but spelt <code>using</code>.)</p>
<p>The interesting part is the four statements that set up the parameter. The first adds the parameter and defines the type:</p>
<pre>cmd.Parameters.Add("@prodids", SqlDbType.Structured)</pre>
<p>For a table-valued parameter, you always specify <b>SqlDbType.Structured</b> here.</p>
<pre>cmd.Parameters("@prodids").Direction = ParameterDirection.Input</pre>
<p>Specifying the direction of the parameter is somewhat superfluous; since TVPs are read-only, <b>Input</b> is the only choice. Nevertheless, I have included it for clarity. Next we introduce the name of the table type in <small>SQL</small> Server, by setting the special parameter property <b>TypeName</b>:</p>
<pre>cmd.Parameters("@prodids").TypeName = "integer_list_tbltype"</pre>
<p>Strictly speaking, this is not necessary when calling a stored procedure, since <small>SQL</small> Server knows the type anyway.
  However, it is definitely best practice to always specify the type. For
  one thing, this will give you a clearer error message when there is a mismatch between the structure you pass from the client and the table type in <small>SQL</small> Server.</p>
<p>And now – drum roll! – it's time pass an actual value to the <small>TVP</small>:</p>
<pre>cmd.Parameters("@prodids").Value = new TVPDemo.CSV_splitter("9,12,27,37")</pre>
<p>You create a new <b>CSV_splitter</b> object which you pass as the parameter value. And as the parameter to the constructor you pass your list of comma-separated integers. Since this is a sample, the list is a constant; in practical code you would of course have a variable here.</p>
<p>All you need to do to get this to work is to put the <b>CSV_splitter</b> class in place. Which  is very simple, since the code is included in the download files. You only need to change the namespace to fit your local conventions. The joy is that this class is perfectly reusable, and while I will cover the internals of the class in a second, all you really need to know if you are in a hurry is this:</p>
<ul>
  <li>The class assumes a list of integers – more precisely <b>Int64</b>. If you want a list of strings, you will need to clone the class.</li>
  <li>The constructor takes an optional parameter which permits you to specify a different delimiter. (But it has to be a single-character delimiter.)</li>
  <li>Empty elements in the string are ignored.</li>
</ul>
<p>You might ask: what if I don't use stored procedures? Can I still use TVPs and the <b>CSV_splitter</b> class? Sure enough. The file <b>CSVDemo.vb</b> also includes this routine:</p>
<pre>Private Sub CSV_to_SQL()

   Using cn As SqlConnection = TVPDemo.DemoHelper.SetupConnection(), _
         cmd As SqlCommand = cn.CreateCommand()

      cmd.CommandType = CommandType.Text
      cmd.CommandText = " SELECT p.ProductID, p.ProductName " &amp; _
                        " FROM   Northwind.dbo.Products p " &amp; _
                        " WHERE  p.ProductID IN (SELECT n FROM @prodids)"

      cmd.Parameters.Add("@prodids", SqlDbType.Structured)
      cmd.Parameters("@prodids").Direction = ParameterDirection.Input
      cmd.Parameters("@prodids").TypeName = "integer_list_tbltype"
      cmd.Parameters("@prodids").Value = _ 
          New TVPDemo.CSV_splitter("1, 11, 76, 34")

      Using da As new SqlDataAdapter(cmd), _
            ds As new DataSet()
         da.Fill(ds)
         TVPDemo.DemoHelper.PrintDataSet(ds)
      End Using
   End Using
End Sub</pre>
<p>It is very similar to <code>CSV_to_SP</code>. The one thing to observe is this line:</p>
<pre>   cmd.Parameters("@prodids").TypeName = "integer_list_tbltype" </pre>
<p>When you use <b>CommandType.Text</b>, it is compulsory to specify the name of the table type. For stored procedures you can leave it out, but as I noted above, best practice is to always include the type name.</p>
<h3><a name="CSV_splitter"></a>Inside the CSV_splitter Class</h3>
<p>As I discussed above, the object you pass as the value for a <small>TVP</small> must   implement <b>IEnumerable&lt;SqlDataRecord&gt;</b> and <b>IEnumerator&lt;SqlDataRecord&gt;</b>. I guess most .<small>NET</small> programmers understand what this means. In case you don't: an interface consists of a number of members with well-defined signatures, but without any code. To implement an interface you write a class that includes the members of the interface with exactly those signatures – now with code added. To this you can add other members as you like. You don't have to worry too much about inadvertently leaving something out – the compiler will inform you of any small detail you forget. Some interfaces feature over 20 members, but these two interfaces are quite slender with in total four methods and one property.</p>
<p>While the main purpose is to feed a table-valued parameter, it is worth noting that since <b>CSV_splitter</b> implements <b>IEnumerable</b>, you can use the class in this way.</p>
<pre>foreach (SqlDataRecord rec in new TVPDemo.CSV_splitter("1,2,3,4")) {
   Console.WriteLine (rec.GetInt64(0).ToString());
}</pre>
<p>Not that this is particularly useful, but it gives an understanding what this is all about. </p>
<p>I will now walk you through the inside of the <b>CSV_splitter</b> class, which you find in <b>TVPDemo.CSV_splitter.cs</b>. For reference, here is the <code>using</code> section:</p>
<pre>using System;
using System.Collections.Generic;
using Microsoft.SqlServer.Server;</pre>
<p>Nothing startling here. (Most people would probably add a few more namespaces, but I refer to some classes in full for clarity.) The class declaration looks like this:</p>
<pre>public class CSV_splitter : IEnumerable&lt;SqlDataRecord&gt;,
                            IEnumerator&lt;SqlDataRecord&gt;
</pre>
<p>That is, this class implements both <b>IEnumerable</b> and <b>IEnumerator</b>. This is possibly disputable; some people may prefer to have one class per interface, but I could not really see the point in this. (I did say that I'm normally not a .<small>NET</small> programmer, didn't I?)</p>
<p>The class has a few private member variables:</p>
<pre>string        input;         // The input string.
char          delim;         // The delimiter.
int           start_ix;      // Start position for current list element.
int           end_ix;        // Position for the next list delimiter.
SqlDataRecord outrec;        // The record we use to return data. </pre>
<p><code>input</code> is the comma-separated list itself and <code>delim</code> is the delimiter. <code>start_ix</code> and <code>end_ix</code> keep track of where in the string we are. The most interesting member is <code>outrec</code>. As the snippet above shows, each iteration produces an instance of <b>SqlDataRecord</b> and as we shall see, it comes from this <code>outrec</code> variable.</p>
<p>To permit for an alternate delimiter, the class has two constructors which in  the C# version fork off to a common private method. </p>
<pre>public CSV_splitter (string  str,
                     char    delimiter) {
   constructor(str, delimiter);
}

public CSV_splitter (string str) {
   constructor(str, ',');
}</pre>
<pre>private void constructor(string  str,
                         char    delimiter) {
   this.input  = str;
   this.delim  = delimiter;

   this.outrec = new SqlDataRecord(
          new SqlMetaData("nnnn", System.Data.SqlDbType.BigInt));

   this.Reset();
}</pre>
<p>First the constructor saves the input parameters into the private members. Next comes the key part: the constructor creates an instance of <b>SqlDataRecord</b> that matches the table type for the table-valued parameter. The constructor for <b>SqlDataRecord</b> accepts an array of <b>SqlMetaData</b> objects. (Both these classes are in the <b>Microsoft.SqlServer.Server</b> namespace.) These classes are closely related: the raison d'être for <b>SqlMetaData</b> is exactly to describe a single column in an <b>SqlDataRecord</b> and ultimately a column in <small>SQL</small> Server. </p>
<p><b>SqlMetaData</b> has a whole slew of constructors to accommodate the various data types in <small>SQL</small> Server and I cover some of the variations as we encounter them<a href="#SqlMetaData"></a>. For the <small>CSV</small> splitter we use the simplest constructor of them all and pass only the column name and the data type. You may note that the column we define in <b>SqlMetaData</b> differs from the column in <b>integer_list_tbltype</b> on two accounts: </p>
<ol>
  <li>The column names are not the same. I have made them different on purpose to show that what names you put in the column definition with <b>SqlMetaData</b> has no importance in the context of passing data to TVPs. </li>
  <li>The data type  is <b>BigInt</b>, while in the table type the column is  <b>int</b>. I chose to use <b>BigInt</b> to make the class as general as possible. That is, you can use the class with any integer data type. (Of course, I could have used <b>bigint</b> in the table type as well, but since product IDs in Northwind are <b>int</b>, I used that type.) As we shall see later, this generalism comes with a price.</li>
</ol>
<p>The last line in the constructor is a call to <b>Reset</b> which is one of the methods required by the <b>IEnumerator</b> interface. Its task is to initiate <code>start_ix</code> and <code>end_ix</code>, and we set them to values that indicate that we have not starting scanning the string yet:</p>
<pre>public void Reset() {
   this.start_ix = -1;
   this.end_ix   = -1;
}</pre>
<p>Next comes the part of the class that implements <b>IEnumerable</b>. This interface requires the implementation of a single method: <b>GetEnumerator</b>, which should return an object that implements <b>IEnumerator</b>. Since the class implements both interfaces, it returns itself:</p>
<pre>System.Collections.IEnumerator
     System.Collections.IEnumerable.GetEnumerator() {
   return this;
}

public System.Collections.Generic.IEnumerator&lt;SqlDataRecord&gt;
     GetEnumerator() {
   return this;
}</pre>
<p>What is a little tricky is that the interface <b>IEnumerable&lt;T&gt;</b> requires that you also implement  the non-generic version (and for some reason, the latter cannot be <code>public</code>). </p>
<p>The interface <b>IEnumerator</b> requires you to implement three methods <b>MoveNext</b>, <b>Reset</b> and <b>Dispose</b> and one read-only property, <b>Current</b>. We have already looked at <b>Reset</b>, and <b>Dispose</b> is only there to permit you to explicitly close files or <small>SQL</small> connections without waiting for garbage collection. That leaves <b>MoveNext</b> and <b>Current</b> as the two interesting members.</p>
<p>The purpose of <b>MoveNext</b> is to permit the caller to move to the next value in the iteration which the caller can retrieve with <b>Current</b>. <b>MoveNext</b> is a boolean function and should return <code>true</code> as long as there is a new item to retrieve with <b>Current</b>. If the caller moves past the last item, the method should return <code>false</code>.</p>
<p>Here is how <b>CSV_Splitter.MoveNext</b> looks like:</p>
<pre>public bool MoveNext() {
   this.start_ix = this.end_ix + 1;

   while (this.start_ix &lt; this.input.Length &amp;&amp;
          this.input[this.start_ix] == this.delim) {
      this.start_ix++;
   }

   if (this.start_ix &gt;= this.input.Length) {
      return false;
   }

   this.end_ix = this.input.IndexOf(this.delim, this.start_ix);
   if (this.end_ix == -1) {
      this.end_ix = this.input.Length;
   }

   return true;
}</pre>
<p>The first action is to set <code>start_ix</code> to be one step ahead of <code>end_ix</code>. This is followed by a <code>while</code> loop of which the purpose is to skip adjacent delimiters. (Imagine that you have a string like <code>"1,2,,4"</code>.) If we at this point find that <code>start_ix</code> is equal to the length of the string, we are past the last character in  the string, and we return <code>false</code> to the caller to indicate that the iteration is over.</p>
<p>Else <code>start_ix</code> is now at the first character in the next value, and we set <code>end_ix</code> to be at the delimiter following this value. If there is no delimiter after the last value, we pretend that there is one any way. Since there is at least one more value in this case, we return <code>true</code>.</p>
<p>That is, all we do here is to position <code>start_ix</code> and <code>end_ix</code>. In the <b>Current</b> property we make use of these values. This property should return the same type as <b>IEnumerable&lt;T&gt;</b> was instantiated with, that is, <b>SqlDataRecord</b>. Here is how our <b>Current</b> property looks like:</p>
<pre>public SqlDataRecord Current {
   get {
      string str = this.input.Substring(this.start_ix,
                                        this.end_ix - this.start_ix);
      this.outrec.SetInt64(0, Convert.ToInt64(str));
      return this.outrec;
   }
}</pre>
<p>We first extract the 
  <nomeddle>substring</nomeddle>
between <code>start_ix</code> and <code>end_ix - 1</code>, and then we convert that value to <b>Int64</b> to set the only column in the <code>outrec</code> which we then return. From a logical point of view, the code could just as well have read:</p>
<pre>public SqlDataRecord Current {
   get {
      string str = this.input.Substring(this.start_ix,
                                        this.end_ix - this.start_ix);
      SqlDataRecord outrec = new SqlDataRecord(
            new SqlMetaData("nnnn", System.Data.SqlDbType.BigInt)); 
      outrec.SetInt64(0, Convert.ToInt64(str));
      return outrec;
   }
}</pre>
<p>And there would have been no need to have <code>outrec</code> as a variable on class level, but it seemed to me slightly more efficient to create the record once and reuse it.</p>
<p>When you implement <b>IEnumerable&lt;T&gt;</b> you must also implement a non-generic version, and we just let it invoke the generic version.</p>
<pre>Object System.Collections.IEnumerator.Current {
   get {
       return this.Current;
   }
}</pre>
What you have seen in <b>MoveNext</b> and <b>Current</b> is fairly normal string-parsing code. There is certainly room for all sorts of improvements: multi-character delimiters, alternate delimiters, trim blanks. (A string like <code>"1,2, ,3"</code> will cause a run-time error in <b>Convert.ToInt64</b>.) If you want to handle comma-separated lists of strings, you can easily clone the class – or make the type a parameter or make the class generic. I leave all these ideas as exercises to the reader.
<p>To conclude, you can see that implementing a custom-iterator to feed a <small>TVP</small> is by no means any advanced matter, and we will leverage on this later in this article.</p>
<h2><a name="Performance_Considerations">What About Performance?</a></h2>
<p>Before we move on to the next demo, we will learn some more theory, mainly from the perspective of performance. The other<i> Arrays and Lists</i> articles discuss performance, so why not this one? In the first two subsections we will look at how TVPs performs compared to methods where we send a comma-separated list or similar to <small>SQL</small> Server. In the last subsection, we will discuss whether the <small>TVP</small> should have a primary key, and how we can improve performance when we know have data that is sorted. In passing, we will also learn how to work with <small>IDENTITY</small> columns and computed columns in the table type.</p>
<h3><a name="Server-side">Performance in SQL Server</a></h3>
<p>As you have understood from the fact that I devoted an article solely to table-valued parameters, this is the
  preferred method for passing a list of values to <small>SQL</small> Server. One important reason is simplicity: writing a stored procedure that
  accepts a table-valued parameter is straightforward. Not that using a list-to-table function is a big deal, but relational databases are centred around tables. And as you have seen, passing a value to
  a <small>TVP</small> from <small>ADO .NET</small> is
  a very simple affair.  <small>TVP</small>s also have the advantage that you can add constraints to the table type to enforce uniqueness or some other type of contract. Nor do you have to worry in your database code about format errors in a comma-separated list.</p>
<p>Does this also mean that this method gives you the best performance? In general, yes. In each and
  every case? No. When running the tests for the <a href="http://www.sommarskog.se/arrays-in-sql-perftest-2009.html">performance appendix</a>, I did find situations where other methods
  outperformed <small>TVP</small>s. However, I believe that in the long run <small>TVP</small>s will you give you better performance than any other
  method. There are two reasons for this:</p>
<p><b>Data is already in table format.</b> With all other methods, cycles need to be spent on parsing a character string
  to get the data into table format. In my tests, the <a href="http://www.sommarskog.se/arrays-in-sql-2005.html#fixed-length">fixed-length method</a> performed better in some tests with integer data. Indeed, this method just chops up a fixed-length string reading from a
  table of numbers, so it is very similar to reading from a table variable. However, <small>TVP</small>s have one more ace up the sleeve:</p>
<p><b>The optimizer gets a clue</b>. For all  other methods, the optimizer has no understanding of what is going on.
  In many situations you get a useful plan nevertheless, but with methods based on inline <small>T‑SQL</small> functions the optimizer often lose grip
  entirely and produce a plan that is nothing but a disaster. And even if the plan is useful, it may not be the most optimal because the optimizer
  has no idea how many rows your list of values will produce, which means that if you use the list in a query with other
  tables, row estimates are likely to be way off.</p>
<p>This is different for table-valued parameters. Just like table variables, table-valued parameters do not have
  distribution statistics, but there is nevertheless one piece of information: cardinality. That is, the first time you call a procedure that
  takes a <small>TVP</small>, the optimizer sniffs the <small>TVP</small> – as sniffs all other parameters – and the optimizer sees that
  the <small>TVP</small> has so and so many rows. This gives the optimizer better odds for good estimates for the number of rows in
  the rest of the query.</p>
<p>Not that it is perfect: There is the general problem that the sniffed value may be atypical. (For a closer discussion on parameter sniffing, see my article <a href="http://www.sommarskog.se/query-plan-mysteries.html"><i>Slow in the Application, Fast in <small>SSMS</small>?</i></a><i>.</i>) And it
  is not always correct information leads to the best plan; in the <a href="http://www.sommarskog.se/arrays-in-sql-perftest-2009.html">performance appendix for <small>SQL</small> 2008</a> you can read about a case where <small>SQL</small> Server chooses an incorrect plan, when it has more accurate cardinality information. But as I discuss in the appendix this concerns only a window of the input  size. Furthermore, cardinality is far from sufficient in all cases. Consider the query:</p>
<pre>SELECT * FROM Orders WHERE CustomerID IN (SELECT custid FROM @custids)</pre>
<p>Say that there are four values in <code>@custids</code>. If they are just four plain customers,
  seeking the non-clustered index on <b>CustomerID</b> is good. But if they are the top four customers accounting for 40 % of the volume, you want a table scan. But
  since a <small>TVP</small> does not have distribution statistics, the optimizer
  cannot distinguish the cases. The workaround is
  simple: bounce the data over a temp table and take benefit of that temp tables have distribution statistics. Since that workaround
  is the same as for all list-to-table functions, you may argue that when you need to do this, there is no special
  performance advantage of <small>TVP</small>s.</p>
<h3><a name="Client-side">Client-side Performance</a></h3>
<p>A reasonable question is: does <small>TVP</small> incur more calling overhead than regular parameters? The answer is yes. In my
  tests I found that passing 50 000 integer values to an unindexed <small>TVP</small> from <small>ADO .NET</small> took
  40-50 ms compared to
  20-35 ms for a comma-separated list. (Note that these numbers apply to the specific hardware that I used for the tests.)  For a <small>TVP</small> with a primary key, the overhead was around 150 ms. </p>
<p>While this overhead may seem considerable, you need to put it in perspective
  and look at the total execution time, and in most cases, the server-side
  execution time exceeds the  numbers in the previous paragraph with a wide
  margin. As just one
  data point: in my test, the server-side execution time for my join test over
  50 000 list elements was 213 ms for a non-indexed <small>TVP</small>, and the best non-<small>TVP</small> method (<a href="http://www.sommarskog.se/arrays-in-sql-2005.html#fixbinary_single">fixed-length
  binary</a> input) needed 420 ms. The <a href="http://www.sommarskog.se/arrays-in-sql-perftest-2009.html"> performance appendix for <small>SQL</small> 2008</a> has more details.</p>
<p>As for the extra overhead when there is a primary key, we will discuss this more closely in the next section.</p>
<h3><a name="SqlMetaData"></a>Primary Keys and Sorted Data – Looking Closer at the SqlMetaData constructors</h3>
<p>The <b>SqlMetaData</b> class has no less than 15 constructors. They control in total 17 read-only properties – i.e., once set you can't change them. To a great extent which constructor to use depends on the data type. For a string or a binary column you use a constructor that includes the <code>maxLength</code> parameter, for a decimal column you need one that exposes scale and precision etc. I am not covering all constructors and properties here, but I refer you to the <a href="http://msdn.microsoft.com/en-us/library/microsoft.sqlserver.server.sqlmetadata.aspx">.<small>NET</small> documentation</a>. </p>
<p>Here I will  discuss four parameters to control special properties for table-valued parameters. They appear in several constructors, and a constructor either has all four or none of these parameters. Here is the C# declaration for the simplest of these constructors:</p>
<pre>public SqlMetaData(       
    string    name,        
    SqlDbType dbType,        
    bool      useServerDefault,        
    bool      isUniqueKey,        
    SortOrder columnSortOrder,        
    int       sortOrdinal) </pre>
<p>The first of these parameters, <code>useServerDefault</code>, serves a different purpose than the other three. You may guess from the name what it is all about, but your guess may not be exactly right. When you specify this parameter as <code>true</code>, <small>SQL</small> Server will ignore any value you set for the column but always set the column to its default value. Sounds corny? Here is the scoop: the <b>SqlDataRecord</b> must have exactly as many columns as your table type has. But what if your table type includes an <small>IDENTITY</small> column or a computed column which you cannot assign values to? It is for that sort of columns you specify <code>useServerDefault</code> as <code>true</code>. It's also useful for columns with a default of <b><span class="nowrap">newid()</span></b> or <small>NEXT VALUE FOR</small>. (The latter is for sequences, a  feature added in <small>SQL</small> 2012.)</p>
<p>The other three parameters, <code>isUniqueKey</code>, <code>columnSortOrder</code> and <code>sortOrdinal</code> are related and they exist in order to permit a performance enhancement. But before we can discuss what purpose they serve and how they work, we need to take one step back and look at the declaration for the table type we used with <code>get_product_names</code>.</p>
<pre>CREATE TYPE integer_list_tbltype AS TABLE (n int NOT NULL <b><u>PRIMARY KEY</u></b>)</pre>
<p>The table type has a primary key, and thus it assumes that the values in the <small>TVP</small> are unique. Is this a good thing? To start with, when you design your tables, you should always look for a natural primary key, and this includes table variables and temp tables. One reason is that if you  write your code under the assumption that a certain column or a set of columns is unique, you should also state this in the table declaration as an assertion. If your assumption is incorrect, your code will die early and not produce incorrect results.</p>
<p>But there is also a performance aspect. Let's look at the code for <code>get_product_names</code> again:</p>
<pre>CREATE PROCEDURE get_product_names @prodids integer_list_tbltype READONLY AS
   SELECT p.ProductID, p.ProductName
   FROM   Northwind.dbo.Products p
   WHERE  p.ProductID IN (SELECT n FROM @prodids)</pre>
<p>For <small>SQL</small> Server, the query is equivalent to:</p>
<pre>SELECT p.ProductID, p.ProductName
FROM   Northwind.dbo.Products p
JOIN   @prodids ps ON ps.n = p.ProductID</pre>
<p>If the table type would not have a primary key, this would not be true. Instead the equivalent query would be:</p>
<pre>SELECT DISTINCT p.ProductID, p.ProductName
FROM   Northwind.dbo.Products p<br>JOIN   @prodids ps ON ps.n = p.ProductID</pre>
<p>That is, <small>SQL</small> Server would have to add an operator somewhere to remove  duplicate values. This comes with an extra cost. Of course, for four values in the <small>TVP</small> this is entirely negligible, but assume that there are has 50 000 values. Now the difference is starting to be measurable, and you can see this in the <a href="http://www.sommarskog.se/arrays-in-sql-perftest-2009.html">performance appendix</a>.</p>
<p>However, as I noted above, I found in my performance tests that there is considerable difference in overhead when passing data to a <small>TVP</small> with a primary key and one without. And indeed, from what I have said this far, this is a zero-sum game. If the <small>TVP</small> has a primary key, there is no need for a Sort or Hash operator in the query above to remove duplicates. But when the data arrives, <small>SQL</small> Server must sort it so that it can be stored according to the index. Only if the <small>TVP</small> is used in more than one query, there is a performance gain with the primary key.</p>
<p>If we don't know anything about the data we are passing to <small>SQL</small> Server, we can't do any better. But what if we know that the data already is sorted according to the index? This is where the three parameters <code>isUniqueKey</code>, <code>columnSortOrder</code> and <code>sortOrdinal</code> come into play. They permit you to specify that the data is sorted and how. <code>isUniqueKey</code> should be <code>true</code> in this case. <code>columnSortOrder</code> can take any of the values <b>SortOrder.Unspecified</b>, <b>SortOrder.Ascending</b> and <b>SortOrder.Descending</b>. (The <b>SortOrder</b> enum is in the <b>System.Data.SqlClient</b> namespace.) For sorted data you would use any of the latter two; <b>Unspecified</b> is the value you use when you use a constructor with these parameters to be able to specify <code>true</code> for <code>useServerDefault</code>. Finally, <code>SortOrdinal</code> specifies where in the unique key the column appears. Use 0 for the first column in the key, 1 for the second etc. Use ‑1 for <b>SortOrder.Unspecified</b>. (If you want to see an example on this, stay tuned. They will be coming.)</p>
<p>You need to use these parameters with care. It goes without saying that you need to ensure that the data you have really is sorted. If you sort the data or create the sort keys yourself, you have control, but it may be precarious to rely on data coming from an outside source to be sorted. If you are mistaken, <small>SQL</small> Server will not let you get away with it, but produce an error message like this one.</p>
<pre>Msg 4819, Severity 16, State: 1, Procedure , Line no: 0
Cannot bulk load. The bulk data stream was incorrectly specified as sorted or the 
data violates a uniqueness constraint imposed by the target table. Sort order
incorrect for the following two rows: primary key of first row: (gamma), primary
key of second row: (delta).
Msg 3621, Severity 0, State: 0, Procedure , Line no: 1
The statement has been terminated.</pre>
<p>There is another thing to watch out for, and in this case <small>SQL</small> Server will stay silent. Inspired by what we have read, we may get the idea to change the constructor for the <b>CSV_splitter</b> class, so that <code>outrec</code> is created in this way:</p>
<pre> this.outrec = new SqlDataRecord(
      new SqlMetaData("nnnn", SqlDbType.BigInt, 
                      false, true, SortOrder.Ascending, 0));</pre>
<p>If you make this change and then run the <b>CSVdemo</b> program, you will find that it runs just fine. But wait! In the procedure <code>CSV_to_SQL</code> there is this line:</p>
<pre> cmd.Parameters("@prodids").Value = New TVPDemo.CSV_splitter("1, 11, 76, 34")</pre>
<p>Data is out of order,  so an error message is to be expected. Still we did not get any. Why? Recall that <b>CSV_splitter</b> uses <b>BigInt</b> to be as reusable as possible, while the table type has an integer column. Because of the data-type mismatch, <small>SQL</small> Server decides to ignore the information that the data is sorted and sorts it anyway. If you change the type to <b>SqlDbType.Int</b> and try again, you will get the error message above.</p>
<p>Thus, to be sure that <small>SQL</small> Server does not decide to sort behind your back, you should make sure that you create the <b>SqlDataRecord</b> object so that it matches your table type exactly. To be precise, you can have a mismatch as long as <small>SQL</small> Server feels that it can trust the conversion to not affect the sort order. If you want to be sure, the simplest way to test is to send data out of order. If you get error message 4819, the plot worked, else it did not. You can also use Profiler, and include the event <b>Performance:Show Statistics <small>XML</small> Profile</b> and run the application. If you also add <b>SP:StmtCompleted</b> you will see the insertion into the <small>TVP</small> as <i>encrypted text</i>. This helps you to locate the query plan, and it should not include a Sort or Hash operator. </p>
<p>Character data is particularly difficult in this context. The first thing to note is that the length  must match. That is, if you define the column in .<small>NET</small> as </p>
<pre>new SqlMetaData("charcol", SqlDbType.NVarChar, 120, 
                false, true, SortOrder.Ascending, 0);</pre>
<p>but the target column is <span class="nowrap"><b>nvarchar(20)</b></span>, <small>SQL</small> Server will ignore your sorting parameters and sort the data. Another complication is that character data can be sorted in many ways, that is, according to different collations. If you look through  the constructors for <b>SqlMetaData</b> you will find two parameters <code>localeID</code> and <code>compareOptions</code> which seem like they could be used to specify the collation. I tested this, but I found that they had no effect. From what I can tell, <small>SQL</small> Server assumes that character data is always sorted according to the database collation. If the data you send with the <small>TVP</small> is sorted according to a different collation, you will get an error once there is a deviation. You can of course specify an explicit collation for the column in the table type, and it may save you from error messages about data being non-unique. However, my testing indicates that if a key column has a different collation from the database collation, <small>SQL</small> Server will ignore the sorting parameters and always sort the incoming data stream.</p>
<h2><a name="loadingdata"></a>Loading Data through Table-Valued Parameters</h2>
<p>We will look at one more example. This time we will see how we can use table-valued parameters to easily load lots of data to <small>SQL</small> Server. There are  several other options for this task: <small>BCP</small>, <small>BULK INSERT</small>, <small>SQL</small> Server Integration Services and the <b>SqlBulkCopy</b> class. But none these options permit you to send data directly to a stored procedure. We will learn two ways to do this. The plain way where we read the file into memory and a more efficient way where we stream the file to the <small>TVP</small>. I've taken the opportunity to cover some ground beyond the topic of TVPs, so you may learn some other tricks in this chapter as well.</p>
<h3><a name="loadsetup"></a>The Setup</h3>
<p>For this example we will look at loading data into these two tables:</p>
<pre>CREATE TABLE Albums (AlbumID     int           IDENTITY,
                     Artist      nvarchar(200) NOT NULL,
                     Title       nvarchar(200) NOT NULL,
                     ReleaseDate date          NULL,
                     Length      time(0)       NULL,
                     CONSTRAINT pk_Albums PRIMARY KEY (AlbumID)
)

CREATE TABLE Tracks (AlbumID     int           NOT NULL,
                     TrackNo     tinyint       NOT NULL,
                     Title       nvarchar(200) NOT NULL,
                     Length      time(0)       NULL,
                     CONSTRAINT pk_Tracks PRIMARY KEY (AlbumID, TrackNo),
                     CONSTRAINT fk_Tracks_Albums FOREIGN KEY(AlbumID)
                     REFERENCES Albums(AlbumID)
)</pre>
<p>We have a music collection, and <b>Albums</b> includes information about an album, and <b>Tracks</b> details the tracks for the albums. All and all, a fairly typical master-detail scenario. These table definitions, as well as other <small>SQL</small> code in this chapter, are inclued in the file <b>fileloaddemo.sql</b> which you find among the <a href="#Demofiles">demo files</a>.</p>
<p>Our task is to load new albums with  their tracks into the database, from the file <b>Albums.csv</b> which is also included in the <a href="#Demofiles">demo files</a>. Here are some sample lines from this file:</p>
<pre>A,Adrian Belew,Desire Caught By the Tail,,33:25
T,1,Tango Zebra,458553,
T,2,Laughing Man,332460,
T,3,The Gypsy Zurna,187141,
T,4,Portrait of Margaret,240718,
T,5,Beach Creatures Dancing Like Cranes,197564,
T,6,At the Seaside Cafe,113319,
T,7,Guernica,127216,
T,8,"""Z""",338416,
A,"Al di Meola, John McLaughlin, Paco de Lucia",Friday Night in San Francisco,8/10/1981,42:09
T,1,A. Mediterranean Sundance-B. Rio Ancho,708780,
T,2,Short Tales of the Black Forest,535484,
T,3,Frevo Rasgado,486608,
T,4,Fantasia Suite,541492,
T,5,Guardian Angel (McLaughin),247066,
A,David Bowie,"""Heroes""",14/10/1977,40:56
T,1,Beauty and the Beast,217182,</pre>
<p>The first field defines whether the line contains an album (A) or a track (T). On an Album line, the fields are <i>Artist</i>. <i>Album title</i>, <i>Release date</i> and <i>Length</i> in minutes and seconds. On a Track line, the fields are <i>Track number, Track title</i> and <i>Length</i> in milliseconds. That is, the fields are the same as in the tables, except for one thing: there is no <b>AlbumID</b>. It is part of our loading task to assign this id.</p>
<p>As for the format, you can note that some fields are quoted in double quotes, but this happens only when the field includes a comma or a double quote. Some fields include a plethora of double quotes; this happens when the double quotes are part of the value. (You may recall that the name of David Bowie's classic album from 1977 really is <i>"Heroes" </i>with quotes and all.)</p>
<p>An aside: you cannot load this file with <small>BCP</small> or <small>BULK INSERT</small> in a simple way. To start with, they cannot really cope with master-detail formats at all, but you would have to load the data into a staging table to be able to separate albums and tracks. And this is only possible it there is an equal number of fields on each line. As it happens, Excel – which I used to create this file – was kind to add an extra comma at the end of the Tracks lines, so this is not an issue here. Instead, the real killer is the inconsistent quoting. As long as a field is consistently quoted through a file, you can load quoted fields with <small>BCP</small> or <small>BULK INSERT</small>, if you use a format file that specifies delimiters that include the quotes. But in this file where only some values are in quotes and where these values include the field delimiter, <small>BCP</small> and <small>BULK INSERT</small> are completely lost. These tools are designed to read a binary stream, and do they not do string parsing.</p>
<p>We need two table types and a stored procedure. The table types mirror the file with one addition:</p>
<pre>CREATE TYPE Albums_tbltype AS TABLE
     (TempID      int           NOT NULL,
      Artist      nvarchar(200) NOT NULL,
      Title       nvarchar(200) NOT NULL,
      ReleaseDate date          NULL,
      Length      time(0)       NULL,
      PRIMARY KEY (TempID)
)

CREATE TYPE Tracks_tbltype AS TABLE
     (TempID      int           NOT NULL,
      TrackNo     tinyint       NOT NULL,
      Title       nvarchar(200) NOT NULL,
      Length      time(0)       NULL,
      PRIMARY KEY (TempID, TrackNo)
)
go</pre>
<p>Since there is no album ID in the file, the loading process must assign new ids. As long as we have the file, we know which tracks that go with which albums, since the file is ordered. But when we load the data into different tables that order is lost, since tables are unordered objects by definition. For this reason, both table types include a column <b>TempID</b>, which is a temporary ID that uniquely identifies an album during the loading process.</p>
<p>The stored procedure is worth dwelling on for an extra second:</p>
<pre>CREATE PROCEDURE LoadAlbums @Albums Albums_tbltype READONLY,
                            @Tracks Tracks_tbltype READONLY AS

DECLARE @idmap TABLE (TempID  int NOT NULL PRIMARY KEY,
                      AlbumID int NOT NULL UNIQUE)

SET XACT_ABORT ON
BEGIN TRANSACTION

MERGE Albums A
USING @Albums T ON 1 = 0
WHEN NOT MATCHED BY TARGET THEN
   INSERT(Artist, Title, ReleaseDate, Length)
      VALUES(T.Artist, T.Title, T.ReleaseDate, T.Length)
OUTPUT T.TempID, inserted.AlbumID INTO @idmap(TempID, AlbumID)
;

INSERT Tracks(AlbumID, TrackNo, Title, Length)
   SELECT i.AlbumID, T.TrackNo, T.Title, T.Length
   FROM   @Tracks T
   JOIN   @idmap i ON i.TempID = T.TempID

COMMIT TRANSACTION
go</pre>
<p>To start with, the procedure sets up a user-defined transaction so that we don't end up loading only the albums. While I am a strong advocate of error handling, I don't use <small>TRY-CATCH</small> here. In the interest of brevity, I let it suffice with <small>SET XACT_ABORT ON</small> to make sure that any error aborts and rolls back the transaction. (If you want directions for error handling, please see my article <a href="http://www.sommarskog.se/error_handling_2005.html"><i>Error Handling in <small>SQL</small> Server 2005 and Later</i></a><i>.</i>)</p>
<p>What may surprise the reader is the <small>MERGE</small> statement. This is a pure insert operation (for the sake of the example, I am completely ignoring that the album may already be in the database), so why use <small>MERGE</small>? And with that weird condition 1 = 0? By using this condition we make sure that no rows in the source match the target. That is, all rows in <code>@Albums</code> will match the condition <small>WHEN NOT MATCHED BY TARGET</small>, and thus all rows in <code>@Albums</code> will be inserted into <b>Albums</b>. Or in another words, this is a complicated way of saying: </p>
<pre>INSERT Albums(Artist, Title, ReleaseDate, Length)
   SELECT Artist, Title, ReleaseDate, Length
   FROM   @Albums </pre>
<p>Why all this? The answer lies in the <small>OUTPUT</small> clause. We need to map the <b>TempID</b> in <code>@Albums</code> to the <small>IDENTITY</small> values generated for <b>AlbumID</b> in <b>Albums</b>, so that we can insert the correct <b>AlbumID</b> values into <b>Tracks</b>, and this is the purpose of the table variable <code>@idmap</code>. If you try to make the mapping with <small>INSERT</small>, you will find that this does not work, because in the <small>OUTPUT</small> clause for <small>INSERT</small> you only have access to the columns in the target table. This is different with <small>MERGE</small>; with <small>MERGE</small> you have access to both target and source columns in the <small>OUTPUT</small> clause.</p>
<p>When inserting into <b>Tracks</b> there is no need for extra fireworks, and we can use plain <small>INSERT</small> where we pick up the album IDs from the<b> <code>@idmap</code></b> table. </p>
<h3><a name="fileloaddemo1"></a>Take One: Reading the File Into a List</h3>
<p>There are two example programs to load the file, and we will first look at <b>fileloaddemo1.cs</b> which reads the file into two <b>List&lt;SqlDataRecord&gt;</b>, one for albums and one for tracks. This program starts of with a number <code>using</code> clauses, of which one may be surprising:</p>
<pre>using System;
using System.Data;
using System.Data.SqlClient;
using System.Collections.Generic;
using Microsoft.SqlServer.Server;
using Microsoft.VisualBasic.FileIO; </pre>
<p><b>System</b> is needed as always of course, and <b>System.Data</b> includes <b>SqlDBType</b> and more. <b>SqlClient</b> is what this text is all about. We need <b>System.Collections.Generic</b> for the class <b>List&lt;T&gt;</b>, and as noted previously we get <b>SqlDataRecord</b> and <b>SqlMetaData</b> from <b>Microsoft.SqlServer.Server</b>. But staunch fans of C# may be appalled by the appearance of Visual Basic here. As I pointed out above, the format of this file is somewhat complex. While I could have written the code to parse the lines on my own, I said to myself "this file has been generated by Excel; there must be code out there that performs this task". So I did a search on Google, and I was quickly pointed to the class <b>TextFieldParser</b>  that exists in the namespace <b>Microsoft.VisualBasic.FileIO</b>.</p>
<p>If you want to use this class from C#, you need to add a reference to <b>Microsoft.VisualBasic.dll</b>. VB programmers get this <small>DLL</small> automatically.</p>
<p><b>Fileloaddemo1</b> includes two routines of interest, <code>read_file</code> and <code>load_albums</code>. The latter  first calls <code>read_file</code> and then calls the stored procedure <code>LoadAlbums</code>. We will look at <code>read_file</code> first. Here is the declaration:</p>
<pre>private static void read_file (string                     filename,
                               out    List&lt;SqlDataRecord&gt; albums,
                               out    List&lt;SqlDataRecord&gt; tracks) {</pre>
<p>It accepts a file name and return album and track data in the two <b>List</b> parameters. The first few lines in <code>read_file</code> are pretty dull:</p>
<pre> System.Globalization.DateTimeFormatInfo no_culture =
     new System.Globalization.DateTimeFormatInfo();
 System.Globalization.DateTimeStyles no_datetime_style =
     System.Globalization.DateTimeStyles.None;</pre>
<pre> int album_no = 0; </pre>
<p>The two items from <b>System.Globalization</b> are some jazz needed when we parse the date and time fields, I'll return to them later. The variable <code>album_no</code> is more interesting: this variable will feed the <b>TempID</b> columns in the table parameters.</p>
<p>The next two statements are significantly hotter, because this is where we set up the <b>SqlMetaData</b> definitions that  map to our table types:</p>
<pre>SqlMetaData[] albums_tbltype =
    { new SqlMetaData("id", SqlDbType.Int, false,
                      true, SortOrder.Ascending, 0),
      new SqlMetaData("artist", SqlDbType.NVarChar, 200),
      new SqlMetaData("album", SqlDbType.NVarChar, 200),
      new SqlMetaData("released", SqlDbType.Date),
      new SqlMetaData("length", SqlDbType.Time, 0, 0)};

SqlMetaData[] tracks_tbltype =
    { new SqlMetaData("id", SqlDbType.Int, false,
                      true, SortOrder.Ascending, 0),
      new SqlMetaData("trackno", SqlDbType.TinyInt,  false,
                      true, SortOrder.Ascending, 1),
      new SqlMetaData("title", SqlDbType.NVarChar, 200),
      new SqlMetaData("length", SqlDbType.Time, 0, 0)};</pre>
<p>In difference to the <b>CSV_splitter</b> class, we don't create any <b>SqlDataRecord</b> at this point; since we are adding to a <b>List</b>, we will need a new <b>SqlDataRecord</b> object each time. Whence, we only create the <b>SqlMetaData</b> arrays in advance.</p>
<p>Here we see some more examples of using the special parameters for the <b>SqlMetaData</b> constructor to specify that the data is sorted. For <code>albums_tbltype</code> there is a single column in the sort key, while for <code>tracks_tbltype</code> there is a composite key and as you see  we specify that both columns are unique. We set the parameter <code>sortOrdinal</code> to 0 and 1 respectively. Admittedly, to some extent this contradicts what I said previously about ensuring that the data is sorted. The id columns are no problem; we are generating the id values in our code and we have  full control over them. But the track numbers comes from the file and in a real-world scenario, we may not be able to rely on that the track numbers come in numeric order.</p>
<p>Here are also examples of <b>SqlMetaData</b> constructors where we specify the length for the string columns. For the <b>time</b> columns we need to use a constructor that exposes scale and precision, even if <b>time</b> only has one of them. </p>
<p>In the C# version we create the lists at this point:</p>
<pre>albums = new List&lt;SqlDataRecord&gt;();
tracks = new List&lt;SqlDataRecord&gt;(); </pre>
<p>(In the VB code this happens in <code>load_albums</code> since VB did not seem to like it when I passed uninitialised variables.)</p>
<p>Next we open the file by creating a <b>TextFieldParser</b> object:</p>
<pre>TextFieldParser fp = new TextFieldParser(filename,
                                         System.Text.Encoding.Default);</pre>
<p>In total, this class offers eight different constructors. For this demo, we use one where we pass the name of the file (there are also constructors accept a <b>Stream</b> object instead) and the encoding. The default for the <b>TextFieldParser</b> is <small>UTF</small>-8, but <small>CSV</small> files from Excel appears to always be <small>ANSI</small> files. (And since one of the tracks from <i>"Heroes"</i> is called <i>Neuköln</i> it matters for the sample file.) </p>
<p>We need to configure our newly created object:</p>
<pre>fp.TextFieldType = FieldType.Delimited;
fp.Delimiters = new String[] {","};
fp.HasFieldsEnclosedInQuotes = true; </pre>
<p>The <b>TextFieldParser</b> can handle both delimited files and fixed-length formats. Here we set up the file to be comma-delimited. We also specify that there are fields enclosed in quotes. Yet an option, that we don't make use of, is to specify comment tokens.</p>
<p>Once this is done, we have completed the preparations and can read the file.</p>
<pre>while (! fp.EndOfData) {
   String[] fields = fp.ReadFields(); </pre>
<p>The <b>ReadFields</b> method consumes the next set of fields and returns them in a string array. (If the file does not comply with the expected format, the method will throw an exception, but I have not included any error handling to keep the example down in length.) Depending on <code>fields[0]</code> we take different paths:</p>
<pre>if (fields[0] == "A") {
   SqlDataRecord album_rec = new SqlDataRecord(albums_tbltype);</pre>
<pre>   album_rec.SetInt32(0, ++album_no);
   album_rec.SetString(1, fields[1]);
   album_rec.SetString(2, fields[2]);</pre>
<p>If we have an A in the first field, we create an <b>SqlDataRecord</b> that aligns with the table type for albums, and then we go on and populate the fields, using various <b>Set</b> methods of the <b>SqlDataRecord</b> class. Above, we save the temporary id (which we first increment), the artist name and the album title. As you see, we refer to the columns by number, starting on 0. If you prefer to access the columns by name, you need to use the <b>GetOrdinal</b> method:</p>
<pre>album_rec.SetInt32(album_rec.GetOrdinal("id"), ++album_no);
album_rec.SetString(album_rec.GetOrdinal("artist"), fields[1]);
album_rec.SetString(album_rec.GetOrdinal("album"), fields[2]); </pre>
<p>Note here that you need to use the name you specified in the <b>SqlMetaData</b> constructor; you cannot use the names in the table type.</p>
<p>The <b>ReleaseDate</b> column is  a little more complex for two reasons: it is permitted to be <small>NULL</small>, and date formats are always problematic. Here is the code:</p>
<pre> DateTime releasedate;
 bool date_ok = DateTime.TryParseExact (
                  fields[3], "d/MM/yyyy", no_culture, no_datetime_style,
                  out releasedate);
 if (date_ok) {
    album_rec.SetDateTime(3, releasedate);
 }
 else {
    album_rec.SetDBNull(3);
 } </pre>
<p>We use <b>TryParseExact</b> to see if there is a legit date in the field. It just so happens that the dates in the file are on the format <small class="nowrap">DD/MM/YYYY</small>, because I generated the <small>CSV</small> file with my regional settings set to English (Australia). (Had I used my regular Swedish settings, the <small>CSV</small> file would have had semicolon as delimiter, which would have been less interesting with regards to the double quotes.) When reading dates from text input – be that a file or text box – you should never assume that all dates are well-formed. There may be a mix of different date formats, and there may be completely bogus dates like 1992-02-30. If the parsing succeeds, we set the date column in <code>album_rec</code>, else we set it to <small>NULL</small>. </p>
<p>When I composed this demo program, the release date proved to be the most difficult to get right. It turned out that it is not sufficient to specify an exact date format. When I tested the program, I had switched back to Swedish settings where the date format is <small class="nowrap">YYYY-MM-DD</small>. Eventually I found that I could not leave the third parameter <code>null</code>, but I had to use an explicit value to state that I wanted to ignore regional settings, whence this <code>no_culture</code>. <code>no_datetime_style</code> is the value for an enum parameter which is mandatory with <b>TryParseExact</b>. </p>
<p>The last album field is the  length, which is handled similarly:</p>
<pre>DateTime length;
bool length_ok = DateTime.TryParseExact(
                   fields[4], new string [] {"h:m:ss", "m:ss"},
                   no_culture, no_datetime_style,
                   out length);
if (length_ok) {
   album_rec.SetTimeSpan(4, length.TimeOfDay);
}
else {
   album_rec.SetDBNull(4);
}</pre>
<p> The only thing that is different is that I permit for time formats both with and without hours, since an album may exceed one hour in length.</p>
<p>When all this is done, we add the record to the <code>albums</code> list:</p>
<pre>albums.Add(album_rec);
</pre>
<p>The code for dealing with the tracks data is similar with a sanity check 
  added.</p>
<pre>else if (fields[0] == "T") {
   if (album_no == 0) {
      throw new Exception("Bad file format: track rows before the first album row!");
   }
   SqlDataRecord track_rec = new SqlDataRecord(tracks_tbltype);

   track_rec.SetInt32(0, album_no);
   track_rec.SetByte(1, Convert.ToByte(fields[1]));
   track_rec.SetString(2, fields[2]);

   if (fields[3] != "") {
      TimeSpan length = TimeSpan.FromMilliseconds(Convert.ToInt32(fields[3]));
      track_rec.SetTimeSpan(3, length);
   }
   else {
      track_rec.SetDBNull(3);
   }

   tracks.Add(track_rec);
}</pre>
<p>Here is the code for <code>load_albums</code>:</p>
<pre>private static void load_albums() {

   List&lt;SqlDataRecord&gt; albums;
   List&lt;SqlDataRecord&gt; tracks;

   read_file("albums.csv", out albums, out tracks);

   using (SqlConnection cn = TVPDemo.DemoHelper.setup_connection())
   using (SqlCommand cmd = cn.CreateCommand()) {

      cmd.CommandType = CommandType.StoredProcedure;
      cmd.CommandText = "dbo.LoadAlbums";

      cmd.Parameters.Add("@Albums", SqlDbType.Structured);
      cmd.Parameters["@Albums"].Direction = ParameterDirection.Input;
      cmd.Parameters["@Albums"].TypeName = "Albums_tbltype";
      cmd.Parameters["@Albums"].Value = albums;

      cmd.Parameters.Add("@Tracks", SqlDbType.Structured);
      cmd.Parameters["@Tracks"].Direction = ParameterDirection.Input;
      cmd.Parameters["@Tracks"].TypeName = "Tracks_tbltype";
      cmd.Parameters["@Tracks"].Value = tracks;

      cmd.ExecuteNonQuery();
   }
}</pre>
<p>It first calls <code>read_file</code> to fill <code>albums</code> and <code>tracks</code>, and then it calls the stored procedure <code>LoadAlbums</code>. The only difference to the code we saw for comma-separated list is this line:</p>
<pre>cmd.Parameters["@Albums"].Value = albums;</pre>
<p>That is, we pass a <b>List</b> object and not an custom-written iterator. </p>
<p>While this code may seem trivial, it is worth emphasising the flexibility. Here we pass a <b>List</b> object, but if we would change our mind and want to pass something else, we can do that very easily. In fact, this is exactly what we will do in a second.</p>
<h3><a name="filestreaming"></a>Take Two: Streaming the File</h3>
<p>The sample file that comes with this article is short; there are only five albums. But imagine that you have a very large file, tens of megabytes in size. With the solution above, you would have to read the entire file into memory before you start sending the data to <small>SQL</small> Server. Is that really necessary? No, and you might already have guessed how we can approach this. If we can write a custom-iterator for a comma-separated list, we should be able to write an iterator that reads the file, so that SqlClient can send a row to <small>SQL</small> Server as soon as we have read it.</p>
<p>Now, the fact that this is a mater-detail file makes this a little more complicated. If we have two TVPs, we would need two iterator classes, one for albums and one for tracks. And these classes would both have to read the file, which thus would be read twice. To avoid this, I decided to use a single table type that can accommodate both row types. Depending on how different headers and details are from each other, this can be quite messy. Thankfully, our albums-and-tracks example is quite forgiving in this sense. (At this point I can sense objection from some readers who think that it is possible to have two table types and still only read the file once. Permit me to come back to this idea after I have gone through the streaming example.)</p>
<p>Here is the table type (which you also find in <b>fileloaddemo.sql</b>):</p>
<pre>CREATE TYPE AlbumTracks_tbltype AS TABLE 
    (TempID      int           NOT NULL,
     TrackNo     tinyint       NOT NULL,
     Artist      nvarchar(200) NULL,
     Title       nvarchar(200) NOT NULL,
     ReleaseDate date          NULL,
     Length      time(0)       NULL,
     PRIMARY KEY (TempID, TrackNo),
     CHECK (TrackNo = 0 AND Artist IS NOT NULL 
         OR TrackNo &gt; 0 AND Artist IS NULL 
                        AND ReleaseDate IS NULL)
)</pre>
<p>I did not add the <small>A/T</small> field to the table type; instead I use <b>TrackNo</b> as the distinguishing column; <b>TrackNo</b> = 0  indicates that this is a header row. I've also added constraints to state rules that are unique for album rows (<b>Artist</b> must be present) and track rows (must not have <b>Artist</b> and <b>ReleaseDate</b>). Such <small>CHECK</small> constraints help to detect errors in the client program.</p>
<p>To use this table type, there is a second stored procedure, similar to the one we looked at previously:</p>
<pre>CREATE PROCEDURE LoadAlbums_2 @AlbumTracks AlbumTracks_tbltype READONLY AS

DECLARE @idmap TABLE (TempID  int NOT NULL PRIMARY KEY,
                      AlbumID int NOT NULL UNIQUE)

SET XACT_ABORT ON
BEGIN TRANSACTION

MERGE Albums A
USING (SELECT TempID, Artist, Title, ReleaseDate, Length
       FROM   @AlbumTracks
       WHERE  TrackNo = 0) AT ON 1 = 0
WHEN NOT MATCHED BY TARGET THEN
   INSERT(Artist, Title, ReleaseDate, Length)
      VALUES(AT.Artist, AT.Title, AT.ReleaseDate, AT.Length)
OUTPUT AT.TempID, inserted.AlbumID INTO @idmap(TempID, AlbumID)
;

INSERT Tracks(AlbumID, TrackNo, Title, Length)
   SELECT i.AlbumID, AT.TrackNo, AT.Title, AT.Length
   FROM   @AlbumTracks AT
   JOIN   @idmap i ON i.TempID = AT.TempID
   WHERE  AT.TrackNo &gt; 0

COMMIT TRANSACTION</pre>
<p>In <a href="#Demofiles">demo files</a>, there is a sample program <b>fileloaddemo2.vb</b> that calls this procedure. Here is the code that calls <code>LoadAlbums_2</code>:</p>
<pre>Private Sub LoadAlbums

   Using cn As SqlConnection = TVPDemo.DemoHelper.SetupConnection(), _
         cmd As SqlCommand = cn.CreateCommand()

      cmd.CommandType = CommandType.StoredProcedure
      cmd.CommandText = "dbo.LoadAlbums_2"

      cmd.Parameters.Add("@AlbumTracks", SqlDbType.Structured)
      cmd.Parameters("@AlbumTracks").Direction = ParameterDirection.Input
      cmd.Parameters("@AlbumTracks").TypeName = "AlbumTracks_tbltype"

      cmd.Parameters("@AlbumTracks").Value = _
             new TVPDemo.AlbumReader("Albums.csv")

      cmd.ExecuteNonQuery()
   End Using
End Sub</pre>
<p>You have seen this pattern a couple of times now. What is different from <b>fileloaddemo1</b> is that there is no call to <code>read_file</code>, but instead there is an instantiation of the class <b>TVPDemo.AlbumReader</b>. This is analogous to when we worked with comma-separated strings of integers, and when we look inside <b>TVPDemo.AlbumReader.vb</b> there is a mix of what we saw in <b>CSV_splitter.cs</b> and <b>fileloaddemo1.cs</b>. The most startling difference may be that this time I show the code is in Visual Basic... So I will rash through the code fairly quickly. The important takeaway is that writing a class that streams a file to a <small>TVP</small> is by no means complicated.</p>
<p>Here is the <code>Imports</code> section:</p>
<pre>Imports System
Imports System.Data
Imports System.Collections.Generic
Imports Microsoft.SqlServer.Server
Imports Microsoft.VisualBasic.FileIO</pre>
<p>Again <b>Microsoft.VisualBasic.FileIO</b> is featured, but I like to remind you that the choice of using the <b>TextFieldParser</b> class is due to the specific file format. While it is likely to be useful for <small>CSV</small> files in general, you may have a file format for which it is less suitable. Particularly, it is not that you need to use this class only because you are streaming to a <small>TVP</small>.</p>
<p>The class declaration:</p>
<pre> Public Class AlbumReader 
    Implements IEnumerable(Of SqlDataRecord), _
               IEnumerator(Of SqlDataRecord)</pre>
<p>is no different from the <b>CSV_splitter</b>. I implement both interfaces in the same class. There are some global members:</p>
<pre>Dim AlbumNo As Integer        ' Current album.
Dim fp As TextFieldParser     ' Our file-reading class.
Dim Outrec As SqlDataRecord   ' The record we use to return data.

Dim NoCulture As New System.Globalization.DateTimeFormatInfo 
Dim NoDateTimeStyle As System.Globalization.DateTimeStyles = _
    System.Globalization.DateTimeStyles.None</pre>
<p><code>AlbumNo</code> is the <b>TempID</b> for the current album and <code>fp</code> is the object for the file we are reading. <code>Outrec</code> is a single output record that I reuse just like in the <b>CSV_splitter</b> class. Then follows the <b>System.Globalization</b> jazz.</p>
<p>The constructor:</p>
<pre>Public Sub New (FileName As String) 
   Me.AlbumNo = 0

   Me.Outrec = new SqlDataRecord( _
        New SqlMetaData("id", SqlDbType.Int, false, _
                       true, System.Data.SqlClient.SortOrder.Ascending, 0), _
        New SqlMetaData("trackno", SqlDbType.TinyInt,  false, _
                        true, System.Data.SqlClient.SortOrder.Ascending, 1), _
        New SqlMetaData("artist", SqlDbType.NVarChar, 200), _
        New SqlMetaData("title", SqlDbType.NVarChar, 200), _
        New SqlMetaData("released", SqlDbType.Date), _
        New SqlMetaData("length", SqlDbType.Time, 0, 0))

   Me.fp = New TextFieldParser(FileName, System.Text.Encoding.Default)
   fp.TextFieldType = FieldType.Delimited
   fp.Delimiters = New String() {","}
   fp.HasFieldsEnclosedInQuotes = True
End Sub</pre>
<p>Again, we take the occasion to state that our <small>TVP</small> is sorted to save <small>SQL</small> Server from sorting when the data arrives. The last few lines set up the <b>TextFieldParser</b> class for reading a <small>CSV</small> file. </p>
<p>Next comes <b>GetEnumerator</b> and in Visual Basic, the two functions must have different names: </p>
<pre>Function GetEnumerator_nongeneric As System.Collections.IEnumerator _
   Implements System.Collections.IEnumerable.GetEnumerator
   Return Me
End Function</pre>
<pre>Public Function GetEnumerator_generic As IEnumerator (Of SqlDataRecord) _
   Implements IEnumerable (Of SqlDataRecord).GetEnumerator
   Return Me
End Function</pre>
<p>Observe here that this is identical to <b>CSV_splitter.vb</b>, and trust me: the implementation in <b>AlbumReader.cs</b> looks exactly to what I showed you for the <b>CSV_splitter</b> class. That is, as long as you follow the pattern with implementing <b>IEnumerable</b> and <b>IEnumerator</b> in the same class, <b>GetEnumerator</b> will always look the same.</p>
<p>The <b>Reset</b> method is somewhat brutal:</p>
<pre>Public Sub Reset Implements IEnumerator(Of SqlDataRecord).Reset 
   Throw New NotImplementedException("AlbumReader.Reset")
End Sub </pre>
<p>I could not think of anything to put here. Well, I guess a proper <b>Reset</b> method could restart the file, but I'm not sure I want that to happen. I chanced to see a blog post that used this pattern, which I decided to copy. Since there is no reason why SqlClient would have to call <b>Reset</b> when you pass a <small>TVP</small>, you could always implement <b>Reset</b> this way. (But try to remember to change the class name in the argument to the exception constructor.)</p>
<p>Next we look at the <b>Current</b> property, which is very straightforward here, even if there is some level of noise due to the requirement to have both a generic and a non-generic implementation:</p>
<pre>ReadOnly Public Property Current_generic As SqlDataRecord _
   Implements IEnumerator (Of SqlDataRecord).Current
   Get 
      Return Me.Outrec
   End Get
End Property

ReadOnly Property Current_nongeneric As Object _
   Implements System.Collections.IEnumerator.Current
   Get 
      Return Me.Outrec
   End Get
End Property</pre>
<p>In the <b>CSV_splitter</b> class, I put the final extraction in <b>Current</b>, but in this class I have put all work to fill <code>Outrec</code> in <b>MoveNext</b>, and that is probably you will do most of the time. </p>
<p>Now, if you think of what we have seen so far, there are really only two things you have craft from scratch when you implement a new custom-iterator for a <small>TVP</small>: the constructor and <b>MoveNext</b>. As for <b>GetEnumerator</b>, <b>Reset</b> and <b>Current</b>, you simply clone from your previous effort. Oh, I forgot: you need to implement <b>Dispose</b> as well:</p>
<pre>Public Sub Dispose Implements IDisposable.Dispose
   Me.fp.Close()
   Me.fp.Dispose()
End Sub</pre>
<p>This time, there is something real to dispose of.</p>
<p>Left to show is the implementation of <b>MoveNext</b>, which is very much a rehash of the loop in <code>read_file</code> above, why I only include an outline to highlight the one thing that is different:  since this is <b>MoveNext</b> we should return <code>False</code> if we are at end of file, else <code>True</code>.</p>
<pre>Public Function MoveNext As Boolean _
   Implements IEnumerator (Of SqlDataRecord).MoveNext

   If Me.fp.EndOfData Then _
      Return False

   Dim Fields() As String = fp.ReadFields()

   If Fields(0) = "A" Then
      ' ... 
   Else If Fields(0) = "T" Then 
      ' ...
   Else 
      Throw New Exception("Illegal record type '" &amp; fields(0) &amp; "'.")
   End If

   Return True
End Function</pre>
<p>We have now looked at two classes that both feed a <small>TVP</small>. While they have lot of common when we look at the code, there is nevertheless one important distinction. The <b>CSV_splitter</b> class is intended to be a general class that you can reuse in many places. <b>AlbumReader</b>, on the other hand, is specific to a certain problem. You would have to write a new class for every new file or data source you read. And as you have seen, this is no big deal at all. Just remember that if there is a <b>DbDataReader</b> class for your data source, you should pass a data-reader object to your <small>TVP</small> directly; no need to write your own class in this case.</p>
<h3><a name="filereadperf"></a>Performance Considerations</h3>
<p>Before you start to stream files all over town, I like to add some words of caution. While the pattern I have shown here is very practical and neat, it is not the most optimal. It will serve you well for large files – but not for very large files. I wanted to prove that a streamed file is not buffered in the client, why I wrote a very stupid file reader which just chopped up the file into chunks of 1024 bytes and passed it to a <small>TVP</small> with a <b><span class="nowrap">binary(1024)</span></b> column. I was able to load an 80 MB file this way, although it took some time. (But the memory consumption in the client stayed flat, proving that data was indeed streamed.) When I tried a 500 MB file, my reward was a timeout message and a <small>TDS</small> error. I never investigated very closely what the underlying reason was, but I assume that I hit a resource limit. Maybe I triggered an auto-grow of the log file which took too long.</p>
<p>An advantage with TVPs is that they make it simple to implement a polished well-packaged solution using stored procedures. But keep in mind that  the table parameter is an intermediate storage. This intermediate storage may be in memory or on disk, depending on how <small>SQL</small> Server decides to handle it, but it is intermediate storage. For this reason, it will always be more efficient if you can load the data directly into the target table through <small>BCP</small>, <small>BULK INSERT</small> or the <b>SqlBulkCopy</b> class. As I noted previously, <small>BCP</small> and <small>BULK INSERT</small> are not able to handle files with formats that require stateful parsing. Since <b>SqlBulkCopy</b> is an <small>API</small>, you have more control and you could use a class like the <b>TextFieldParser</b> to feed an <b>SqlBulkCopy</b> session to load data into the target table directly.</p>
<p> (In case you are thinking that <small>XML</small> or delimited strings could be an alternative here, permit me to point out that they, too, represent intermediate storage. If you pass a 50 <small>MB XML</small> document, it is very likely that <small>SQL</small> Server will spill it to disk.)</p>
<p>When you insert or update large amounts of data, there is always reason to consider chopping up the operation in batches. This applies no matter you are loading data from an outside source like a file, or if you copy data from one table to another. If for no other reason, it helps to keep the transaction-log size in check. In the context of loading a file through a <small>TVP</small>, this means that you need to call your procedure for every batch. There are two challenges here:</p>
<ul>
  <li>Implement the batching as such.</li>
  <li>Make the process restartable in case of a crash half-way through the file.</li>
</ul>
<p>I will not go into details here, but let if suffice with a brief discussion. The first point is not too difficult. You could pass the custom-iterator a <b>Stream</b> object and a batch size, and the custom-iterator would read that many of number of lines from the file. Or you could keep it simple: use a moderate batch size and fill a <b>List</b> with one batch at a time, and don't stream at all. </p>
<p>Making the process restartable may be more difficult. For a simple <small>MERGE</small> scenario (that is, if-not-exists-insert-else-update) you may accept to run part of the file twice. But there are scenarios where re-running part of a file would alter the outcome, for instance when columns are updated incrementally. Or <small>INSERT</small>-only scenarios like the one we have looked at in this article, where a re-run would result in primary-key violations or even worse: load of duplicate data. You can add <small>WHERE NOT EXISTS</small> in the stored procedure as a simple way out, but it may prove to have an undesirable performance impact for all loads, not only restarted ones. The best solution is likely to depend on the exact situation.</p>
<p>Finally, let's discuss the specific problem with master-detail files a little more closely. I said previously that with two table types and two iterators, both iterators would have to read the file from start to end. You may object to this statement and suggest that there could be a single class that reads the file and which puts the rows into two queues, one for albums and one for tracks. The custom-iterators would read from these queues. But, no, this will not fly. Well, it would fly in the sense that you would be able to load the file. However, you not would achieve the aim of preserving memory in the client process. Why?</p>
<p>Keep in mind that SqlClient sends the data to <small>SQL</small> Server over single a communication line where it has to respect the <small>TDS</small> protocol. And if you look in the <a href="http://msdn.microsoft.com/en-us/library/dd304523%28v=prot.13%29.aspx"><small>TDS</small> specification</a>, you will find that the data for one <small>TVP</small> has to be sent in a single sequence. That is, SqlClient cannot interleave data for the two TVPs, but it will have to read all data for one <small>TVP</small> first. Which means that the data for the other <small>TVP</small> will be buffered into in this queue and take up memory which was exactly what we wanted to avoid. There is simply a law of nature working against us here: the data in the file comes in a different order than we want to process it, and there is no way around it. The best you can do is to stream the detail rows and buffer the header rows (of which there are likely to be fewer). But this appears to be messy to implement – it may be simpler use a batchwise implementation with a <b>List&lt;SqlDataRecord&gt;</b>.</p>
<p>It is worth noticing that neither my solution with a single table type overcomes problem with having to reorder the data. As long as the procedure has not started executing, no reordering has occurred, but all data has been buffered in <small>SQL</small> Server – in memory or in tempdb. However, when the procedure runs, it scans the table variable twice. Depending on the situation and hardware configuration this may be a better – or worse – solution than having the client to read the file twice. It goes without saying that if you are facing this scenario, and performance is critical for you, you should benchmark several solutions.</p>
<h2><a name="OtherAPIs">Using Table-Valued Parameters from Other APIs</a></h2>
<p>This article has focused on using table-valued parameters with <small>ADO</small> .<small>NET</small> and SqlClient for two reasons. 1) It's a very common environment. 2) It's very simple to use TVPs from SqlClient. Before I conclude this article, I will give a brief exposé over other <small>API</small>s and whether they support table-valued
  parameters. I also discuss what options you have if your <small>API</small> does not support
<small>TVP</small>s.</p>
<h4><a name="ODBC">ODBC</a></h4>
<p>You can use table-valued parameters with <small>ODBC</small>. You need to specify <span codelanguage="other"><i><small>SQL</small> Server Native
Client 10.0</i> or later as your <small>ODBC</small> driver. <small>SQL</small> Server Native Client is a <small>DLL</small> that implements both an <small>ODBC</small> driver and an <small>OLE DB </small></span>provider for <small>SQL</small> Server. It comes with <small>SQL</small> Server and <span codelanguage="other">is freely redistributable.</span></p>
<p><span codelanguage="other">As I have not worked with <small>ODBC</small> myself, I cannot assess how smooth or difficult it is to use <small>TVP</small>s
  with <small>ODBC</small>. I believe that as with <small>ADO</small> .<small>NET</small> there are two ways to pass a
<small>TVP</small> through <small>ODBC</small>: streaming and non-streaming. Just like <small>ADO</small> .<small>NET</small>, </span><small>ODBC</small> exposes properties to specify that your data is sorted, to avoid sorting in <small>SQL</small> Server when the <small>TVP</small> has a primary key.</p>
<p><span codelanguage="other"> Books Online have two examples on using table-valued parameter in the section <span id="nsrTitle" class="style1">Table-Valued Parameters (<small>ODBC</small>)</span> . There is also a sample on <a href="http://msftdpprodsamples.codeplex.com/wikipage?title=SS2008%21README%20ODBC%20Table-Valued%20Parameters&amp;referringTitle=Home">Codeplex</a>.</span></p>

<h4><a name="OLE_DB">OLE DB</a></h4>
<p>You can use table-valued parameters with <small>OLE DB</small>, if you use the <small>SQLNCLI10</small> provider or later, that is the <small>OLE DB</small> half of <small>SQL</small>
Server Native Client. <small>OLE DB</small> offers two models for passing <small>TVP</small>s. One is the push model, where you create a rowset
with the metadata, fill the rowset with your data, and in the regular parameter area, you pass the rowset pointer. This is the same basic idea as passing a <b>List</b> with <small>ADO .NET</small>, but you need to write more code. (As always with
<small>OLE DB, I</small>'m tempted to say.)</p>
<p>The alternative is the pull model, which essentially is a role reversal where the consumer needs to implement
<b>IRowset</b>
on its own whereupon the provider will read from the rowset as a consumer. This model is intended for streaming scenarios, where you get data from an external source, and you don't want
any intermediate storage in the client. </p>
<p>I can't find anything in <small>SQL</small> Server Books Online that discusses how to specify that your data source is already sorted, so I don't know if this is possible. I have a suspicion, though, that they rely on general <small>OLE DB</small> functionality. To define a table parameter, you need to use the interface <b>ITableDefinitionWithConstraints</b>, and this interface has a method <b>AddConstraint</b> that permits you to specify a primary key. It is a little embarrassing that I don't know, since I have actually implemented TVPs with <small>OLE DB</small> (see below under Perl).</p>
<p>Whether you can use <small>TVP</small>s if you use the <small>OLE DB</small> Consumer Templates, I don't know. I've only worked with "naked" <small>OLE DB</small>
myself, never the consumer templates.</p>
<p>You can find a sample for the pull model on <a href="http://msftdpprodsamples.codeplex.com/wikipage?title=SS2008%21Readme_Table-Valued%20Parameters&amp;referringTitle=Home">CodePlex</a>. I have not found any sample for
the push model, but if you are desperate you can download the source code for my Perl module (see below), but you will find it difficult to find the forest among the all the trees there.</p>
<h4><a name="ADO">ADO</a></h4>
<p>No, you cannot use table-valued parameters with old <small>ADO</small>. Yes, <small>ADO</small> sits on top of <small>OLE DB</small>, and you can use <small>SQLNCLI</small>10 as the
<small>OLE DB</small> provider with <small>ADO</small>. But <small>ADO</small> itself has not been updated for the new data types added in <small>SQL</small> 2005 and <small>later </small>cannot
work with them.</p>
<h4><a name="LINQ_EF">LINQ to SQL and Entity Framework</a></h4>
<p>None of them have support for table-valued parameters. Ironic isn't it? Microsoft touts them as the hottest and
best way to access <small>SQL</small> Server, and then you find you don't have access to all features. The obvious workaround, besides
using one of the older list-to-table methods, is to make a direct call from <small>ADO .NET</small> and bypass that language-integrated
thing. Arguable, this causes your code to have a mix of paradigms. But it could be a first step from moving away from
<small>LINQ/EF</small> entirely. (Wait, did I just say that? OK, let it be said: I am not a fan of neither of these technologies, as I
feel that they serve to increase the object/relational impedance between client-side developers and
<small>SQL</small> Server people.)</p>
<p>See also the section <a href="#Acknowledgements"><i>Further Reading</i></a> for some useful links in this area.</p>
<h4><a name="JDBC">JDBC</a></h4>
<p>The version of the <a href="http://msdn.microsoft.com/en-us/data/aa937724.aspx">Microsoft <small>SQL</small> Server <small>JDBC</small> Driver</a>
that is current of this writing (4.0) does not seem to support table-valued parameters. But please check Microsoft's site for
updates. I don't know whether   <small>JDBC</small> drivers for <small>SQL</small> Server from other vendors support
<small>TVP</small>s, but it
could definitely be worth investigating.</p>
<h4><a name="PHP">PHP</a></h4>
<p>Microsoft has a <small>PHP</small> driver for <small>SQL</small> Server. The current version of this writing is 3.0. What I can understand, it does not support table-valued parameters. The driver is available with source code on <a href="http://sqlsrvphp.codeplex.com/">Codeplex</a>.</p>
<h4><a name="Perl">Perl</a></h4>
<p>If you use the standard <small>DBI/DBD</small> modules, I doubt that you will find any support for table-valued parameters. However,
the best option for connecting to <small>SQL</small> Server – as long you do not need to support other data sources
– is
<a href="http://www.sommarskog.se/MSSQL/index.html">Win32::SqlServer</a>, of which I am the author myself. And, yes, it supports table-valued
parameters. However, I found in my performance tests that the performance for passing TVPs is very poor. It took two seconds to pass a <small>TVP</small> with 50 000 values. Compare this with 50-150 ms for <small>ADO</small> .<small>NET</small>. I cannot say whether this is due to <small>OLE DB</small> or my own miserable programming.</p>

<h4><a name="Workarounds">What If Your API Does Not Support TVPs</a></h4>
<p>As you have realised when you've read this small summary is that if you are using <small>VB6, VBA</small>, Access, Java, <small>PHP</small> – and
probably a few more environments which I did not list here – you cannot use <small>TVP</small>s directly in your <small>API</small>. If you need to
pass a list of values, you should use any of the methods that I discuss in my article <a href="http://www.sommarskog.se/arrays-in-sql-2005.html">
<i>Arrays
and Lists for <small>SQL</small> 2005 and Beyond</i></a>.</p>
<p>If you need to call a stored procedure that takes a table-valued parameter, you can always do this by writing a
wrapper procedure that takes a comma-separated list (or an <small>XML</small> document for multi-column <small>TVP</small>s) as a parameter
and inserts the data
to a table variable and then calls the inner procedure. Say for instance that you need to call <code>get_product_names</code> from VB6:</p>
<pre>CREATE PROCEDURE get_product_names_wrapper @prodids nvarchar(MAX)
DECLARE @prodid_table integer_list_tbltype
INSERT @prodid_table(n)
    SELECT number FROM <a href="http://www.sommarskog.se/arrays-in-sql-2005.html#iter_intlist_to_tbl">iter_intlist_to_tbl</a>(@prodids)
EXEC get_product_names @prodid_table</pre>
<p>If you think creating a procedure is too much, you can  submit a parameterised command batch:</p>
<pre>DECLARE @prodid_table integer_list_tbltype
INSERT @prodid_table(n)
    SELECT number FROM iter_intlist_to_tbl(?)
EXEC get_product_names @prodid_table</pre>
<p>If you've never seen a parameterised command before, see the section on
<a href="http://www.sommarskog.se/dynamic_sql.html#SQL_injection"><small>SQL</small> Injection</a> in my article on dynamic <small>SQL</small> for a
brief introduction.</p>
<h2><a name="Acknowledgements">Acknowledgements, Feedback</a> and Further Reading</h2>
<p>I like to thank my <small>MVP</small> colleagues who helped me by reviewing my demo program and
  with other research: Bob Beauchemin,
Alejandro Mesa, Greg Low, Daniel Joskovski, Lenni Lobel and Adam Machanic.</p>
<p>If you have opinions, additions or just have spotted a language/grammar error, please mail me at
<a href="mailto:esquel@sommarskog.se">esquel@sommarskog.se</a>. If you have questions about using <small>TVP</small>s or arrays and
lists in <small>SQL</small> Server in general, I advice you to post your questions to the appropriate public forum. Which forum you should use depends on the exact nature of your question. If you have questions related to C# and VB .<small>NET</small>, you should use a .<small>NET</small> forum. For questions on <small>ADO</small> .<small>NET</small> the <a href="http://social.msdn.microsoft.com/Forums/en-US/sqldataaccess/threads"><small>SQL</small> Server Data Access forum</a> may be the best place, while <small>T‑SQL</small> questions goes into the <a href="http://social.msdn.microsoft.com/Forums/en-US/transactsql/threads"><small>T‑SQL</small> forum</a>.</p>
<p>Here are some more blog posts about <small>TVP</small>:</p>
<p><i><a href="http://lennilobel.wordpress.com/2009/07/29/sql-server-2008-table-valued-parameters-and-c-custom-iterators-a-match-made-in-heaven/"><small>SQL</small> Server 2008 Table-Valued Parameters and C# Custom Iterators: A Match Made In Heaven!</a></i> by <small>SQL</small> Server <small>MVP</small> Lenni Lobel, which includes some approaches that I did not include (because they go beyond my C# abilities).</p>
<p><a href="http://archive.msdn.microsoft.com/LinqEntityDataReader"><span id="ctl00_ctl00_WideContent_ProjectTitleControl1_TitleLabel"><i><small>LINQ</small> Entity Data Reader</i></span></a>, a <small>MSDN</small> article how that shows how to use TVPs together with <small>LINQ</small>.</p>
<p id="ctl00_TitleArea_ArticleTitle"><i><a href="http://www.codeproject.com/Articles/179481/Code-First-Stored-Procedures">Code First Stored Procedures</a></i>, an article that discusses how to use TVPs with Entity Framework 4.1 and CodeFirst.</p>
<h2><a name="Revisions">Revision History</a></h2>
<p><b>2012-07-01</b> – More or less a total rewrite of the sections that cover .<small>NET</small> because of two reasons: I realised how simple it is to write a reusable class for parsing a comma-separated list and pass it to a table-valued parameter. The original version of the article incorrectly said you could not stream data to a <small>TVP</small> through <small>ADO</small> .<small>NET</small>, but that SqlClient would always buffer. This unfortunate error was due to a misunderstanding between me and a Program Manager at Microsoft. To the latter end, I have added examples how to load data from a file through a table-valued parameter, both streaming and non-streaming. For the other sections there mainly some language polishing, but I've added a caveat that you cannot use TVPs between stored procedures in different databases.</p>
<p><b>2011-02-25</b> – Added text that you need <a href="#TVP_in_TSQL"> <small>EXECUTE</small> 
  permission</a> to use a table type.</p>
<p><b>2010-01-06</b> – First version.</p>
<p align="right"><a href="http://www.sommarskog.se/index.html">Back to my home page</a>.</p>

<div class=" lleo_show" id="lleo_enjoyContentControls">
    <div id="lleo_enjoyContentPanel">
        <label id="lleo_enjoyContentLabel"><input id="lleo_enjoyContentCheckbox" checked="checked" type="checkbox">Show this icon if possible</label>
    </div>
    <div id="lleo_enjoyContentButton" title="Enjoy Content!"></div>
</div></body></html>